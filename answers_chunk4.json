[
  {
    "output": ""
  },
  {
    "output": "398"
  },
  {
    "output": "n = 1\n        while True:\n            f_n = sum_digits_base_k(n, 4)\n            g_n = sum_digits_base_k(f_n, 8)\n            if 256 <= g_n <= 4095:\n                return n\n            n += 1"
  },
  {
    "output": "88"
  },
  {
    "output": ""
  },
  {
    "output": "9"
  },
  {
    "output": ""
  },
  {
    "output": "2√2"
  },
  {
    "output": ""
  },
  {
    "output": ""
  },
  {
    "output": ""
  },
  {
    "output": ""
  },
  {
    "output": ""
  },
  {
    "output": "def find_largest_number():\n    squares = {16, 25, 36, 49, 64, 81}\n    graph = {}\n    for s in squares:\n        start = s // 10\n        end = s % 10\n        if start not in graph:\n            graph[start] = []\n        graph[end] = []\n        graph[end].append(start)\n    max_len = 0\n    best = \"\"\n    for start in graph:\n        path = [start]\n        visited = set([start])\n        def dfs(node, path, visited):\n            nonlocal max_len, best\n            if len(path) > max_len:\n                max_len = len(path)\n                best = ''.join(map(str, path))\n            for neighbor in graph.get(node, []):\n                if neighbor not in visited:\n                    visited.add(neighbor)\n                    path.append(neighbor)\n                    dfs(neighbor, path, visited)\n                    path.pop()\n                    visited.remove(neighbor)\n        dfs(start, path, visited)\n    return best[:3] if best else \"\""
  },
  {
    "output": ""
  },
  {
    "output": ""
  },
  {
    "output": "False"
  },
  {
    "output": ""
  },
  {
    "output": ""
  },
  {
    "output": "1540"
  },
  {
    "output": ""
  },
  {
    "output": ""
  },
  {
    "output": ""
  },
  {
    "output": ""
  },
  {
    "output": "[Hard problem, difficulty=8] Step-by-step reasoning:\nWe are given:\n\n- Triangle $ ABC $ with medians $ AD = 18 $ and $ CE = 27 $\n- $ AB = 24 $\n- Extend median $ CE $ to intersect the circumcircle of $ \\triangle ABC $ at point $ F $\n- The area of triangle $ AFB $ is $ m\\sqrt{n} $, where $ m $ and $ n $ are positive integers and $ n $ is square-free\n- We are to find $ m + n $\n\n---\n\n### **Step 1: Understand the Geometry and Given Information**\n\nWe are given two medians and one side of the triangle. The medians intersect at the centroid $ G $, which divides each median in a 2:1 ratio.\n\nLet’s denote:\n\n- $ D $ is the midpoint of $ BC $\n- $ E $ is the midpoint of $ AB $\n- $ G $ is the centroid of triangle $ ABC $\n- $ F $ is the point where the extension of median $ CE $ intersects the circumcircle of triangle $ ABC $\n\nWe are to find the area of triangle $ AFB $, which is formed by extending the median $ CE $ to intersect the circumcircle at $ F $.\n\n---\n\n### **Step 2: Use Coordinate Geometry to Model the Triangle**\n\nLet’s place triangle $ ABC $ in the coordinate plane for easier computation.\n\nLet’s place:\n\n- $ A = (0, 0) $\n- $ B = (24, 0) $ (since $ AB = 24 $)\n- Let $ C = (x, y) $, unknown for now\n\nThen:\n\n- $ D $, the midpoint of $ BC $, is $ \\left( \\frac{24 + x}{2}, \\frac{0 + y}{2} \\right) = \\left( \\frac{24 + x}{2}, \\frac{y}{2} \\right) $\n- $ E $, the midpoint of $ AB $, is $ \\left( \\frac{0 + 24}{2}, \\frac{0 + 0}{2} \\right) = (12, 0) $\n\nNow, we can compute the lengths of the medians $ AD $ and $ CE $ using the distance formula.\n\n---\n\n### **Step 3: Use the Median Length Formula**\n\nThe formula for the length of a median from vertex $ A $ to the midpoint $ D $ of side $ BC $ is:\n\n$$"
  },
  {
    "output": ""
  },
  {
    "output": "1440"
  },
  {
    "output": "130"
  },
  {
    "output": ""
  },
  {
    "output": "from itertools import combinations\n\ndef solve():\n    numbers = list(range(1, 21))\n    pairs_16 = [tuple(sorted(pair)) for pair in combinations(numbers, 2) if sum(pair) == 16]\n    pairs_24 = [tuple(sorted(pair)) for pair in combinations(numbers, 2) if sum(pair) == 24]\n    valid_subsets = set()\n    for p1 in pairs_16:\n        for p2 in pairs_24:\n            if p1[1] != p2[0] and p1[1] != p2[1] and p1[0] != p2[0] and p1[0] != p2[1]:\n                subset = tuple(sorted(p1 + p2))\n                if len(set(subset)) == 4:\n                    valid_subsets.add(subset)\n    return len(valid_subsets)"
  },
  {
    "output": ""
  },
  {
    "output": "def find_rotation_angle_and_center(A, B, C, A_prime, B_prime, C_prime):\n    import math\n    def midpoint(p1, p2):\n        return ((p1[0] + p2[0])/2, (p1[1] + p2[1])/2)\n    def slope(p1, p2):\n        if p2[0] - p1[0] == 0:\n            return float('inf')\n        return (p2[1] - p1[1]) / (p2[0] - p1[0])\n    def perpendicular_slope(s):\n        if s == 0:\n            return float('inf')\n        if s == float('inf'):\n            return 0\n        return -1 / s\n    def line_eq(p, m):\n        x, y = p\n        return (y - x * m, m)\n    def intersect(l1, l2):\n        m1, b1 = l1\n        m2, b2 = l2\n        if m1 == m2:\n            return None\n        x = (b2 - b1) / (m1 - m2)\n        y = m1 * x + b1\n        return (x, y)\n    mid_aa = midpoint(A, A_prime)\n    mid_bb = midpoint(B, B_prime)\n    slope_aa = slope(A, A_prime)\n    slope_bb = slope(B, B_prime)\n    perp_slope_aa = perpendicular_slope(slope_aa)\n    perp_slope_bb = perpendicular_slope(slope_bb)\n    line_aa = line_eq(mid_aa, perp_slope_aa)\n    line_bb = line_eq(mid_bb, perp_slope_bb)\n    center = intersect(line_aa, line_bb)\n    if center is None:\n        return (0, (0, 0))\n    def angle_between(p1, p2, center):\n        v1 = (p1[0] - center[0], p1[1] - center[1])\n        v2 = (p2[0] - center[0], p2[1] - center[1])\n        dot = v1[0] * v2[0] + v1[1] * v2[1]\n        mag1 = math.hypot(v1[0], v1[1])\n        mag2 = math.hypot(v2[0], v2[1])\n        if mag1 == 0 or mag2 == 0:\n            return 0\n        cos_theta = dot / (mag1 * mag2)\n        cos_theta = max(min(cos_theta, 1), -1)\n        theta = math.acos(cos_theta)\n        if (p1[0] - center[0]) * (p2[0] - center[0]) < 0 or (p1[1] - center[1]) * (p2[1] - center[1]) < 0:\n            theta = -theta\n        return math.degrees(theta)\n    angle = angle_between(A, A_prime, center)\n    return (angle, center)"
  },
  {
    "output": "Error: No code found"
  },
  {
    "output": ""
  },
  {
    "output": ""
  },
  {
    "output": "$ a $"
  },
  {
    "output": ""
  },
  {
    "output": "130"
  },
  {
    "output": ""
  },
  {
    "output": "1600"
  },
  {
    "output": "36"
  },
  {
    "output": "137"
  },
  {
    "output": ""
  },
  {
    "output": ""
  },
  {
    "output": ""
  },
  {
    "output": "sum(abs(i - j) for i in range(1, 11) for j in range(1, 11) if i != j) / 45"
  },
  {
    "output": "1999000.0"
  },
  {
    "output": ""
  },
  {
    "output": "The problem is incomplete as the definition of the function $ X(k) $ is missing."
  },
  {
    "output": ""
  },
  {
    "output": "def count_solutions(target): \n    count = 0 \n    for x in range(1, target // 4 + 1): \n        for y in range(1, (target - 4*x) // 3 + 1): \n            z = (target - 4*x - 3*y) \n            if z > 0 and (target - 4*x - 3*y) % 2 == 0: \n                count += 1 \n    return count \n\nm = count_solutions(2009) \nn = count_solutions(2000) \nresult = (m - n) % 1000 \nprint(result)"
  },
  {
    "output": ""
  },
  {
    "output": "0"
  },
  {
    "output": ""
  },
  {
    "output": "1080"
  },
  {
    "output": "def count_sequences():\n    dp = [0] * 40\n    dp[0] = 1\n    for i in range(1, 40):\n        dp[i] = dp[i-1]\n        if i >= 2:\n            dp[i] += dp[i-2]\n    return dp[39]"
  },
  {
    "output": "def solve():\n    from itertools import product\n    for a in range(-100, 101):\n        for b in range(-100, 101):\n            c = -a - b\n            if a * b + b * c + c * a == -2011:\n                return abs(a) + abs(b) + abs(c)"
  },
  {
    "output": ""
  },
  {
    "output": ""
  },
  {
    "output": ""
  },
  {
    "output": "Error: '(' was never closed (<string>, line 1)"
  },
  {
    "output": "sum(1 for a3 in range(100) for a2 in range(100) for a1 in range(100) if (2010 - 1000*a3 - 100*a2 - 10*a1) >= 0 and (2010 - 1000*a3 - 100*a2 - 10*a1) <= 99)"
  },
  {
    "output": ""
  },
  {
    "output": ""
  },
  {
    "output": ""
  },
  {
    "output": ""
  },
  {
    "output": ""
  },
  {
    "output": ""
  },
  {
    "output": ""
  },
  {
    "output": ""
  },
  {
    "output": ""
  },
  {
    "output": ""
  },
  {
    "output": ""
  },
  {
    "output": "342"
  },
  {
    "output": ""
  },
  {
    "output": ""
  },
  {
    "output": ""
  },
  {
    "output": ""
  },
  {
    "output": "2100"
  },
  {
    "output": ""
  },
  {
    "output": "7.0"
  },
  {
    "output": ""
  },
  {
    "output": ""
  },
  {
    "output": ""
  },
  {
    "output": ""
  },
  {
    "output": ""
  },
  {
    "output": ""
  },
  {
    "output": ""
  },
  {
    "output": ""
  },
  {
    "output": ""
  },
  {
    "output": ""
  },
  {
    "output": ""
  },
  {
    "output": "return primes[i] + 4 * d\n    return 0"
  },
  {
    "output": ""
  },
  {
    "output": "337"
  },
  {
    "output": "15"
  },
  {
    "output": ""
  },
  {
    "output": "Error: '(' was never closed (<string>, line 1)"
  },
  {
    "output": "def count_parity_monotonic():\n    from functools import lru_cache\n\n    @lru_cache(maxsize=None)\n    def dp(pos, prev, is_even):\n        if pos == 4:\n            return 1\n        total = 0\n        for curr in range(0, 10):\n            if is_even:\n                if prev % 2 == 0:\n                    if curr % 2 == 0:\n                        total += dp(pos + 1, curr, True)\n                    else:\n                        total += dp(pos + 1, curr, False)\n                else:\n                    if curr % 2 == 1:\n                        total += dp(pos + 1, curr, False)\n            else:\n                if prev % 2 == 1:\n                    if curr % 2 == 1:\n                        total += dp(pos + 1, curr, False)\n                    else:\n                        total += dp(pos + 1, curr, True)\n                else:\n                    if curr % 2 == 0:\n                        total += dp(pos + 1, curr, True)\n        return total\n\n    return dp(1, 0, True) + dp(1, 0, False) - 1000"
  },
  {
    "output": ""
  },
  {
    "output": "def count_progressive_sequences():\n    from itertools import combinations\n    count = 0\n    for length in range(1, 361):\n        for seq in combinations(range(1, 361), length):\n            if sum(seq) == 360:\n                valid = True\n                for i in range(len(seq) - 1):\n                    if seq[i] >= seq[i + 1] or seq[i + 1] % seq[i] != 0:\n                        valid = False\n                        break\n                if valid:\n                    count += 1\n    return count"
  },
  {
    "output": ""
  },
  {
    "output": "Error: '(' was never closed (<string>, line 1)"
  },
  {
    "output": ""
  },
  {
    "output": ""
  },
  {
    "output": "def find_greatest_palindrome():\n    def is_palindrome(n):\n        s = str(n)\n        return s == s[::-1]\n    for i in range(999, 9, -1):\n        if is_palindrome(i) and is_palindrome(oct(i)[2:]):\n            return i\n    return 0"
  },
  {
    "output": ""
  },
  {
    "output": "[Hard problem, difficulty=8] Step-by-step reasoning:\nWe are asked to compute the remainder when the product  \n$$\nP = 9 \\times 99 \\times 999 \\times \\cdots \\times \\underbrace{99\\cdots9}_{\\text{999 9's}}\n$$  \nis divided by $1000$. This is equivalent to computing  \n$$\nP = \\prod_{k=1}^{999} (10^k - 1) \\mod 1000\n$$\n\n---\n\n### **Step 1: Understand the structure of the product**\n\nEach term in the product is of the form $10^k - 1$, which is a number consisting of $k$ nines. For example:\n- $10^1 - 1 = 9$\n- $10^2 - 1 = 99$\n- $10^3 - 1 = 999$\n- ...\n- $10^{999} - 1 = \\underbrace{99\\cdots9}_{\\text{999 9's}}$\n\nSo the product is:\n$$\nP = \\prod_{k=1}^{999} (10^k - 1)\n$$\n\nWe are to compute $P \\mod 1000$.\n\n---\n\n### **Step 2: Use modular arithmetic properties**\n\nWe can compute the product modulo 1000 step by step, using the property:\n$$\n(a \\times b) \\mod m = [(a \\mod m) \\times (b \\mod m)] \\mod m\n$$\n\nSo we can compute the product modulo 1000 by multiplying each term modulo 1000 and taking the result modulo 1000 at each step.\n\n---\n\n### **Step 3: Observe periodicity in powers of 10 modulo 1000**\n\nWe can simplify the computation by noting that:\n$$\n10^k \\mod 1000\n$$\nhas a **period of 3** for $k \\geq 3$, because:\n- $10^1 = 10 \\mod 1000 = 10$\n- $10^2 = 100 \\mod 1000 = 100$\n- $10^3 = 1000 \\mod 1000"
  },
  {
    "output": "12"
  },
  {
    "output": "[1000/b + b for b in [1, 2, 4, 5, 8, 10, 20, 25, 40, 50, 100, 125, 200, 250, 500, 1000] if 1000 % b == 0] sum()"
  },
  {
    "output": ""
  },
  {
    "output": ""
  },
  {
    "output": "8"
  },
  {
    "output": ""
  },
  {
    "output": ""
  },
  {
    "output": ""
  },
  {
    "output": ""
  },
  {
    "output": "def find_max_n():\n    import math\n    def f(n):\n        total = 0\n        for k in range(1, 101):\n            total += math.floor(math.log10(k * n))\n        return total\n    left, right = 1, 1000000\n    answer = 1\n    while left <= right:\n        mid = (left + right) // 2\n        if f(mid) <= 300:\n            answer = mid\n            left = mid + 1\n        else:\n            right = mid - 1\n    return answer"
  },
  {
    "output": "from math import sqrt, floor\n\ndef calculate_height():\n    # Original height of the tripod\n    original_height = 4\n    # Assume the base of the tripod is a regular triangle with side length 'a'\n    # The original height forms a right triangle with half the base of the triangle\n    # Let's assume the base triangle has side length 2 (for simplicity)\n    base_triangle_side = 2\n    # The height of the base triangle (from center to vertex)\n    base_triangle_height = sqrt(3)\n    # The original height of the tripod is the hypotenuse of a right triangle\n    # with one leg being the base triangle height and the other being the height from the base to the top\n    # So, original_height^2 = base_triangle_height^2 + (height_from_base_to_top)^2\n    # Solving for height_from_base_to_top\n    height_from_base_to_top = sqrt(original_height**2 - base_triangle_height**2)\n    # When one leg breaks, the new height is the height from the base to the top\n    # which is now the hypotenuse of a right triangle with one leg being the base triangle height\n    # and the other being the height from the base to the top\n    # So, new_height = sqrt(base_triangle_height^2 + height_from_base_to_top^2)\n    new_height = sqrt(base_triangle_height**2 + height_from_base_to_top**2)\n    # Express new_height in the form m / sqrt(n)\n    # Simplify the expression\n    m = 8\n    n = 3\n    # Compute the final result\n    result = floor(m + sqrt(n))\n    return result"
  },
  {
    "output": "[Hard problem, difficulty=7] Step-by-step reasoning:\nWe are asked to find the **maximum size of a subset $ S $** of the set $ \\{1, 2, 3, \\ldots, 1989\\} $ such that **no two elements in $ S $ differ by exactly 4 or 7**.\n\n---\n\n### **Step 1: Understand the Problem**\n\nWe are given a set of integers from 1 to 1989, and we want to select the **maximum number of elements** such that **no two selected elements differ by exactly 4 or 7**.\n\nThis is equivalent to finding the **maximum independent set (MIS)** in a graph where:\n\n- Each number from 1 to 1989 is a **node**.\n- An **edge** exists between two nodes if their difference is **exactly 4 or 7**.\n\nSo, the problem becomes:  \n> **Find the maximum number of nodes in a graph of size 1989, where edges connect nodes differing by 4 or 7, such that no two selected nodes are connected.**\n\n---\n\n### **Step 2: Model the Problem as a Graph**\n\nLet’s define the graph $ G $ as follows:\n\n- **Vertices**: $ V = \\{1, 2, 3, \\ldots, 1989\\} $\n- **Edges**: $ E = \\{(i, j) \\mid |i - j| = 4 \\text{ or } 7\\} $\n\nThis is a **graph with 1989 nodes**, and edges between nodes that are 4 or 7 apart.\n\nWe are to find the **maximum independent set (MIS)** of this graph.\n\n---\n\n### **Step 3: Analyze the Graph Structure**\n\nLet’s consider the structure of the graph:\n\n- Each number $ i $ is connected to $ i+4 $ and $ i+7 $, as long as those numbers are within the range [1, 1989].\n- So, the graph is a **path-like graph**, where each node is connected to the next node 4 or 7 steps ahead.\n\nThis is not a simple path graph, but it has a **repeating pattern** in terms of connections.\n\nLet’s consider the **modular structure** of the graph. Since the forbidden differences are 4 and 7, we can consider the graph modulo the **least common multiple (LCM)** of 4 and"
  },
  {
    "output": "[Hard problem, difficulty=7] Step-by-step reasoning:\nYou've provided an incomplete problem statement. Let me clarify what the problem is asking.\n\nThe problem begins:\n\n> \"A function $ f $ is defined on the complex numbers by $ f(z $...\n\nIt seems like the problem was cut off. To proceed, I need the full problem statement. However, I can help you solve a common type of problem involving functions defined on complex numbers, such as:\n\n> \"A function $ f $ is defined on the complex numbers by $ f(z) = z^2 + 1 $. Find the number of complex roots of the equation $ f(z) = 0 $.\"\n\nLet me assume this is the intended problem. If not, please provide the full problem statement.\n\n---\n\n### Step 1: Identify what we're asked to find\n\nWe are asked to find the number of complex roots of the equation $ f(z) = 0 $, where the function is defined as $ f(z) = z^2 + 1 $.\n\n---\n\n### Step 2: Break down the problem into smaller parts\n\n1. **Understand the function**: $ f(z) = z^2 + 1 $\n2. **Set the function equal to zero**: $ z^2 + 1 = 0 $\n3. **Solve the equation for $ z $**: Find all complex numbers $ z $ such that $ z^2 + 1 = 0 $\n4. **Count the number of complex roots**\n\n---\n\n### Step 3: Solve each part carefully\n\n#### Part A: Understand the function\n\nThe function $ f(z) = z^2 + 1 $ is a polynomial function of degree 2, defined on the complex numbers.\n\n#### Part B: Set the function equal to zero\n\n$$\nf(z) = 0 \\Rightarrow z^2 + 1 = 0\n$$\n\n#### Part C: Solve the equation\n\n$$\nz^2 + 1 = 0 \\Rightarrow z^2 = -1\n$$\n\nNow solve for $ z $:\n\n$$\nz = \\pm \\sqrt{-1} = \\pm i\n$$\n\nSo the solutions are $ z = i $ and $ z = -i $.\n\n#### Part D: Count the number of complex roots\n\nWe have two distinct complex roots: $ z = i $ and $ z = -i $.\n\n---\n\n### Step 4: Combine the results for the final answer\n\nThe equation $ f(z) = 0 $ has **two** complex roots.\n\n---"
  },
  {
    "output": ""
  },
  {
    "output": "792"
  },
  {
    "output": ""
  },
  {
    "output": "for c in range(20, 100):\n    for w in range(0, 100):\n        s = 30 + 4 * c - w\n        if s > 80:\n            unique = True\n            for c2 in range(20, 100):\n                for w2 in range(0, 100):\n                    if 30 + 4 * c2 - w2 == s and (c2, w2) != (c, w):\n                        unique = False\n                        break\n                if not unique:\n                    break\n            if unique:\n                print(s)\n                break\n        if unique:\n            break"
  },
  {
    "output": "Error: '(' was never closed (<string>, line 1)"
  },
  {
    "output": ""
  },
  {
    "output": ""
  },
  {
    "output": "2x^2 + 4x - 6"
  },
  {
    "output": ""
  },
  {
    "output": "2023"
  },
  {
    "output": ""
  },
  {
    "output": ""
  },
  {
    "output": ""
  },
  {
    "output": ""
  },
  {
    "output": ""
  },
  {
    "output": ""
  },
  {
    "output": "4"
  },
  {
    "output": ""
  },
  {
    "output": "The question is incomplete and cannot be answered with the given information."
  },
  {
    "output": "when done."
  },
  {
    "output": ""
  },
  {
    "output": ""
  },
  {
    "output": "1600"
  },
  {
    "output": ""
  },
  {
    "output": ""
  },
  {
    "output": ""
  },
  {
    "output": ""
  },
  {
    "output": "[Hard problem, difficulty=7] Step-by-step reasoning:\nLet's solve this step by step.\n\n---\n\n### **Step 1: Understand the Problem**\n\nWe are given a scenario involving a die, and we are to calculate the **probability that the third roll is a six**, **given that the first two rolls are both sixes**. This involves:\n\n- **Updating our belief** about whether the die is fair or biased, based on the evidence (the first two sixes).\n- Then, using the **updated probabilities** to compute the expected probability of rolling a six on the third roll.\n\nThis is a **Bayesian inference** problem.\n\n---\n\n### **Step 2: Define the Problem Mathematically**\n\nLet’s define the following:\n\n- **Event A**: The first two rolls are both sixes.\n- **Event B**: The die is biased (i.e., it has a higher probability of rolling a six).\n- **Event C**: The third roll is a six.\n\nWe are asked to find:  \n$$\nP(C | A)\n$$\n\nThat is, the probability that the third roll is a six **given** that the first two rolls were both sixes.\n\nTo compute this, we will:\n\n1. **Update the probability that the die is biased**, given that the first two rolls were both sixes.\n2. **Use the updated probability** to compute the expected probability of rolling a six on the third roll.\n\n---\n\n### **Step 3: Define the Prior and Likelihoods**\n\nLet’s assume the following:\n\n- There are **two types of dice**:\n  - **Fair die (F)**: Probability of rolling a six is $ \\frac{1}{6} $.\n  - **Biased die (B)**: Probability of rolling a six is $ \\frac{1}{2} $.\n\n- We assume that **before any rolls**, the probability that the die is fair is $ P(F) = 0.5 $, and the probability that it is biased is $ P(B) = 0.5 $.\n\n- We are told that the **first two rolls are both sixes**. We will use this to update our belief about whether the die is fair or biased.\n\n---\n\n### **Step 4: Use Bayes' Theorem to Update the Probability of the Die Being Biased**\n\nWe want to compute:\n\n$$\nP(B | A) = \\frac{P(A | B) \\cdot P(B)}{P(A)}\n$$\n\nWhere:\n\n- $ P(A | B) $: Probability of"
  },
  {
    "output": ""
  },
  {
    "output": ""
  },
  {
    "output": ""
  },
  {
    "output": ""
  },
  {
    "output": ""
  },
  {
    "output": ""
  },
  {
    "output": ""
  },
  {
    "output": "for s in range(1, 10000):\n    if (2*s % 9 - 8) % 2 == 0 and (2*s % 10 - 9) % 2 == 0 and (2*s % 11 - 10) % 2 == 0:\n        print(s)\n        break"
  },
  {
    "output": ""
  },
  {
    "output": "1260"
  },
  {
    "output": "Error: name 'a' is not defined"
  },
  {
    "output": ""
  },
  {
    "output": "def count_valid_strings():\n    from collections import defaultdict\n    dp = [defaultdict(int) for _ in range(11)]\n    dp[0]['A'][0] = 1\n    dp[0]['B'][0] = 1\n    for i in range(1, 11):\n        for c in ['A', 'B']:\n            for l in range(4):\n                if l == 0:\n                    dp[i][c][l] = dp[i-1]['A'][0] + dp[i-1]['B'][0]\n                else:\n                    dp[i][c][l] = dp[i-1][c][l-1]\n    return sum(dp[10]['A'][l] + dp[10]['B'][l] for l in range(4))"
  },
  {
    "output": ""
  },
  {
    "output": ""
  },
  {
    "output": "def calculate_ratio(b, d):\n    am = (17 / 1000) * b\n    an = (17 / 2009) * d\n    # Coordinates of M and N\n    m = (am, 0)\n    n = (0, an)\n    # Equation of line MN: y = ((an - 0)/(0 - am))x + 0\n    slope_mn = an / (-am)\n    # Equation of diagonal AC: y = ((d - 0)/(b - 0))x + 0\n    slope_ac = d / b\n    # Find intersection P of MN and AC\n    # Solve: slope_mn * x = slope_ac * x\n    # => x = 0\n    # => y = 0\n    # So P is (0, 0)\n    # AC is the length from A(0,0) to C(b, d)\n    ac_length = (b**2 + d**2)**0.5\n    ap_length = 0\n    # Ratio AC/AP is undefined since AP is zero\n    return float('inf')"
  },
  {
    "output": "import numpy as np\n\nA = np.array([0, 0, 0])\nB = np.array([12*np.sqrt(3), 0, 0])\nC = np.array([12*np.sqrt(3), 13*np.sqrt(3), 0])\nD = np.array([0, 13*np.sqrt(3), 0])\n\n# Find intersection point P of diagonals AC and BD\ndef line_line_intersection(line1, line2):\n    t = (line2[0] - line1[0]) @ np.linalg.inv(np.cross(line1[1]-line1[0], line2[1]-line2[0])) @ (line2[0] - line1[0])\n    return line1[0] + t * (line1[1] - line1[0])\n\nP = line_line_intersection((A, C), (B, D))\n\n# Determine vertices of the triangular pyramid\n# After removing triangle ABP and joining edges AP and BP, the vertices are A, B, P, and the new vertex formed by joining AP and BP\n# Assuming the new vertex is the midpoint of AB (as per the described process)\nnew_vertex = (A + B) / 2\n\n# Calculate volume of the triangular pyramid using the scalar triple product\nvolume = np.abs(np.cross(B - A, new_vertex - A) @ (P - A)) / 6\n\nvolume"
  },
  {
    "output": ""
  },
  {
    "output": ""
  },
  {
    "output": ""
  },
  {
    "output": "31"
  },
  {
    "output": ""
  },
  {
    "output": "43 + 47 = 90"
  },
  {
    "output": "def process_text(text):\n    text = re.sub(r'\\W+', ' ', text).lower()\n    words = text.split()\n    filtered_words = [word for word in words if word not in STOPWORDS]\n    return ' '.join(filtered_words)"
  },
  {
    "output": "def task_func(input_dict, source_dir, backup_dir):\n    input_dict['a'] = 1\n    values = list(input_dict.values())\n    freq = {}\n    for val in values:\n        freq[val] = freq.get(val, 0) + 1\n    sorted_values = sorted(freq, key=lambda x: -freq[x])\n    sorted_dict = {k: input_dict[k] for k in sorted_values}\n    import os\n    import shutil\n    success = True\n    try:\n        for root, dirs, files in os.walk(source_dir):\n            for file in files:\n                src_path = os.path.join(root, file)\n                dst_path = os.path.join(backup_dir, os.path.relpath(src_path, source_dir))\n                os.makedirs(os.path.dirname(dst_path), exist_ok=True)\n                shutil.copy2(src_path, dst_path)\n    except:\n        success = False\n    return success, sorted_dict"
  },
  {
    "output": "def read_convert_csv(file_path, input_encoding, output_encoding):\n    import csv\n    with open(file_path, 'r', encoding=input_encoding) as file:\n        reader = csv.DictReader(file)\n        data = [row for row in reader]\n    converted_data = '\\n'.join([','.join(row.values()) for row in data])\n    return data, converted_data.encode(output_encoding).decode('utf-8')"
  },
  {
    "output": "```python\nimport numpy as np\nimport pandas as pd\nfrom sklearn.preprocessing import StandardScaler\n\ndef task_func(records: np.ndarray, random_seed: int = 0) -> pd.DataFrame:\n    if len(records.shape) != 2:\n        raise ValueError(\"ValueError: If records is not 2D.\")\n    \n    np.random.seed(random_seed)\n    # Shuffle the features\n    shuffled_features = np.random.permutation(records.shape[1])\n    shuffled_data = records[:, shuffled_features]\n    \n    # Normalize the data\n    scaler = StandardScaler()\n    normalized_data = scaler.fit_transform(shuffled_data)\n    \n    # Create feature names\n    feature_names = [f\"f{i+1}\" for i in range(records.shape[1])]\n    np.random.shuffle(feature_names)\n    \n    # Convert to DataFrame\n    df = pd.DataFrame(normalized_data, columns=feature_names)\n    return df\n```"
  },
  {
    "output": "True"
  },
  {
    "output": "def filter_excel_data(file_path, start_date, end_date):\n    import pandas as pd\n    try:\n        df = pd.read_excel(file_path)\n        df['Date'] = pd.to_datetime(df['Date'])\n        filtered_df = df[(df['Date'] >= start_date) & (df['Date'] <= end_date)]\n        return filtered_df\n    except Exception as e:\n        raise Exception(f\"An error occurred: {str(e)}\")"
  },
  {
    "output": "[[math.sin(x) for x in tup] for tup in tuples_list]"
  },
  {
    "output": "def task_func(df):\n    import pandas as pd\n    import matplotlib.pyplot as plt\n    from statsmodels.tsa.seasonal import seasonal_decompose\n    if 'value' not in df.columns:\n        raise ValueError(\"DataFrame must contain a 'value' column\")\n    if df.empty:\n        raise ValueError(\"DataFrame is empty\")\n    result = seasonal_decompose(df['value'], model='additive')\n    fig = result.plot()\n    return (result, fig)"
  },
  {
    "output": "import numpy as np\nimport matplotlib.pyplot as plt\n\ndef generate_and_plot_time_series(start_date, end_date, freq, plot_title, x_label, y_label):\n    dates = pd.date_range(start=start_date, end=end_date, freq=freq)\n    values = np.random.randn(len(dates))\n    plt.figure(figsize=(10, 6))\n    plt.plot(dates, values, label='Time Series')\n    plt.title(plot_title)\n    plt.xlabel(x_label)\n    plt.ylabel(y_label)\n    plt.legend()\n    plt.grid(True)\n    plt.show()"
  },
  {
    "output": "```python\nimport os\nimport re\n\ndef find_files_and_write_config(directory, pattern, target_dir):\n    # Step 1: Validate the input\n    if not os.path.exists(directory):\n        raise ValueError(f\"Directory {directory} does not exist.\")\n    \n    # Compile the regular expression pattern\n    try:\n        regex = re.compile(pattern)\n    except re.error as e:\n        raise ValueError(f\"Invalid regular expression pattern: {e}\")\n\n    # Step 2: Search for files\n    matched_files = []\n    for filename in os.listdir(directory):\n        if re.fullmatch(pattern, filename):\n            matched_files.append(os.path.join(directory, filename))\n\n    # Step 3: Write to a configuration file\n    config_file_path = os.path.join(target_dir, \"config.txt\")\n    with open(config_file_path, 'w') as config_file:\n        for file_path in matched_files:\n            config_file.write(file_path + '\\n')\n\n    # Step 4: Return the configuration file path\n    return config_file_path"
  },
  {
    "output": "from sklearn.decomposition import NMF\nfrom sklearn.feature_extraction.text import CountVectorizer\n\ndef task_func(documents):\n    vectorizer = CountVectorizer()\n    X = vectorizer.fit_transform(documents)\n    model = NMF(n_components=5, random_state=42)\n    W = model.fit_transform(X)\n    H = model.components_\n    return W, H"
  },
  {
    "output": "def task_func(text, analyzer):\n    import re\n    cleaned_text = re.sub(r'\\W+', ' ', text).lower()\n    sentiment = analyzer.polarity_scores(cleaned_text)\n    return {\n        'neg': sentiment['neg'],\n        'neu': sentiment['neu'],\n        'pos': sentiment['pos'],\n        'compound': sentiment['compound']\n    }"
  },
  {
    "output": "from sklearn.preprocessing import StandardScaler\nimport numpy as np\nimport base64\n\ndef task_func(data):\n    data_array = np.array(data, dtype=np.float64)\n    scaler = StandardScaler()\n    standardized_data = scaler.fit_transform(data_array.reshape(-1, 1))\n    flattened_data = standardized_data.flatten()\n    data_bytes = flattened_data.tobytes()\n    base64_encoded = base64.b64encode(data_bytes).decode('ascii')\n    return base64_encoded"
  },
  {
    "output": "import base64\nimport pandas as pd\n\ndef task_func(df):\n    df = pd.DataFrame(df)\n    csv_data = df.to_csv(index=False, encoding='utf-8')\n    csv_bytes = csv_data.encode('utf-8')\n    base64_str = base64.b64encode(csv_bytes).decode('utf-8')\n    return base64_str"
  },
  {
    "output": "def task_func(script_path, timeout):\n    import os\n    import psutil\n    import subprocess\n    import time\n    \n    if not os.path.exists(script_path):\n        return {\"error\": \"Script path does not exist\"}\n    \n    process = None\n    start_time = time.time()\n    cpu_usage = 0.0\n    memory_usage = 0\n    \n    try:\n        process = subprocess.Popen(\n            script_path,\n            stdout=subprocess.PIPE,\n            stderr=subprocess.PIPE,\n            stdin=subprocess.PIPE,\n            start_new_session=True\n        )\n        \n        while True:\n            if time.time() - start_time > timeout:\n                process.terminate()\n                time.sleep(0.1)\n                process.kill()\n                return {\"error\": \"Timeout reached\"}\n            \n            if not process.poll():\n                try:\n                    cpu_usage += psutil.Process(process.pid).cpu_percent(interval=0.1)\n                    memory_usage = psutil.Process(process.pid).memory_info().rss\n                except psutil.NoSuchProcess:\n                    return {\"error\": \"Process not found\"}\n                time.sleep(0.1)\n            else:\n                break\n        \n        return {\"cpu_usage\": cpu_usage, \"memory_usage\": memory_usage}\n    \n    except Exception as e:\n        return {\"error\": str(e)}"
  },
  {
    "output": "import pandas as pd\nimport random\n\ndef task_func(product_list, categories, min_value=10, max_value=100):\n    data = []\n    for product in product_list:\n        category = random.choice(categories)\n        quantity_sold = random.randint(min_value, max_value)\n        revenue = quantity_sold * random.randint(10, 50)\n        data.append({\n            'Product': product,\n            'Category': category,\n            'Quantity Sold': quantity_sold,\n            'Revenue': revenue\n        })\n    return pd.DataFrame(data)"
  },
  {
    "output": "import random\nfrom collections import defaultdict\nimport math\n\ndef task_func(animal_list):\n    lambda_customers = 5\n    num_customers = random.poisson(lambda_customers)\n    sales_count = defaultdict(int)\n    for _ in range(num_customers):\n        animal = random.choice(animal_list)\n        sales_count[animal] += 1\n    return dict(sales_count)"
  },
  {
    "output": "import os\nimport re\n\ndef task_func(pattern: str, replacement: str, directory: str) -> bool:\n    try:\n        if not os.path.exists(directory):\n            return False\n        for filename in os.listdir(directory):\n            file_path = os.path.join(directory, filename)\n            if os.path.isdir(file_path):\n                continue\n            match = re.fullmatch(pattern, filename)\n            if match:\n                new_filename = re.sub(pattern, replacement, filename)\n                new_file_path = os.path.join(directory, new_filename)\n                os.rename(file_path, new_file_path)\n        return True\n    except Exception as e:\n        return False"
  },
  {
    "output": "def task_func(df):\n    if 'Title' not in df.columns or 'Views' not in df.columns or 'Likes' not in df.columns:\n        return plt.subplots()\n    filtered_df = df[df['Title'].str.contains('how|what', case=False, na=False)]\n    if filtered_df.empty:\n        return plt.subplots()\n    filtered_df['Like Ratio'] = filtered_df['Likes'] / filtered_df['Views']\n    fig, ax = plt.subplots()\n    filtered_df['Like Ratio'].plot(kind='bar', ax=ax)\n    return fig, ax"
  },
  {
    "output": "def task_func(n_rows, remove_cols, columns=['A', 'B', 'C', 'D', 'E'], random_seed=None):\n    if random_seed is not None:\n        np.random.seed(random_seed)\n    data = np.random.randint(0, 101, size=(n_rows, len(columns)))\n    df = pd.DataFrame(data, columns=columns)\n    df = df.drop(columns=remove_cols)\n    return df"
  },
  {
    "output": "def task_func(url):\n    import os\n    import requests\n    from zipfile import ZipFile\n    import io\n\n    try:\n        response = requests.get(url)\n        response.raise_for_status()\n        zip_data = io.BytesIO(response.content)\n        with ZipFile(zip_data, 'r') as zip_ref:\n            filenames = zip_ref.namelist()\n            zip_ref.extractall(path=\"extracted_files\")\n        return (\"Success\", filenames)\n    except requests.exceptions.RequestException as e:\n        return (\"Request failed\", [])\n    except Exception as e:\n        return (\"Error\", [])"
  },
  {
    "output": "def task_func(sales_data, colors):\n    if any(sales['quantity'] < 0 for sales in sales_data):\n        raise ValueError(\"Sales quantity cannot be negative\")\n    total_sales = {}\n    for sale in sales_data:\n        fruit = sale['fruit']\n        quantity = sale['quantity']\n        if fruit in total_sales:\n            total_sales[fruit] += quantity\n        else:\n            total_sales[fruit] = quantity\n    import matplotlib.pyplot as plt\n    fruits = list(total_sales.keys())\n    quantities = list(total_sales.values())\n    plt.bar(fruits, quantities, color=colors)\n    plt.xlabel('Fruit')\n    plt.ylabel('Total Sales')\n    plt.title('Total Sales by Fruit')\n    plt.show()"
  },
  {
    "output": "def simulate_dice_rolls(num_rolls):\n    import random\n    import matplotlib.pyplot as plt\n    import numpy as np\n    results = [random.randint(1, 6) for _ in range(num_rolls)]\n    frequencies = np.bincount(results, minlength=7)[1:]\n    plt.bar(range(1,7), frequencies)\n    plt.xlabel('Dice Roll')\n    plt.ylabel('Frequency')\n    plt.title('Dice Roll Frequencies')\n    plt.show()"
  },
  {
    "output": "import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n\ndef task_func(start_date='2016-01-01', periods=13, freq='WOM-2FRI', seed=0):\n    np.random.seed(seed)\n    dates = pd.date_range(start=start_date, periods=periods, freq=freq)\n    prices = np.random.uniform(100, 500, size=periods)\n    df = pd.DataFrame({'Date': dates, 'Price': prices})\n    \n    fig, ax = plt.subplots()\n    ax.plot(df['Date'], df['Price'], marker='o')\n    ax.set_xlabel('Date')\n    ax.set_ylabel('Price')\n    ax.set_title('Share Price Series')\n    \n    return (df, ax)"
  },
  {
    "output": "def task_func(path):\n    import sys\n    import os\n    sys.path.append(path)\n    os.makedirs(path, exist_ok=True)\n    return path"
  },
  {
    "output": "def task_func(low, high, rows, cols):\n    import random\n    matrix = [[random.uniform(low, high) for _ in range(cols)] for _ in range(rows)]\n    if high <= low:\n        raise ValueError\n    flat = [num for row in matrix for num in row]\n    total = 0\n    for i in range(len(flat)):\n        for j in range(i + 1, len(flat)):\n            total += flat[i] * flat[j]\n    return total, matrix"
  },
  {
    "output": "import numpy as np\nimport pandas as pd\n\ndef task_func(num_teams=5, num_games=100):\n    np.random.seed(0)\n    scores = np.random.randint(0, 101, size=(num_teams, num_games))\n    df = pd.DataFrame(scores, index=range(num_teams), columns=range(num_games))\n    return df"
  },
  {
    "output": "import xml.etree.ElementTree as ET\nimport csv\n\ndef task_func(xml_content, csv_file_path):\n    try:\n        root = ET.fromstring(xml_content)\n        data = []\n        for child in root:\n            row = []\n            for subchild in child:\n                row.append(subchild.text)\n            data.append(row)\n        with open(csv_file_path, 'w', newline='') as csvfile:\n            writer = csv.writer(csvfile)\n            writer.writerows(data)\n    except ET.ParseError:\n        pass\n    except IOError:\n        pass"
  },
  {
    "output": "import random\nimport heapq\n\ndef generate_and_find_k_smallest(k):\n    random_list = [random.randint(1, 100) for _ in range(20)]\n    k_smallest = heapq.nsmallest(k, random_list)\n    return k_smallest"
  },
  {
    "output": "return {\n        'mean': mean,\n        'median': median,\n        'std_dev': std_dev\n    }"
  },
  {
    "output": "import pandas as pd\nfrom scipy import stats\n\ndef task_func(matrix):\n    df = pd.DataFrame(matrix)\n    normalized_df = pd.DataFrame(stats.zscore(df), index=df.index, columns=df.columns)\n    return normalized_df"
  },
  {
    "output": "def task_func(client_socket, cert_file, key_file):\n    import ssl\n    import os\n    import hashlib\n    context = ssl.create_default_context(ssl.Purpose.CLIENT_AUTH)\n    context.load_cert_chain(cert_file, key_file)\n    ssl_socket = context.wrap_socket(client_socket, server_side=True)\n    file_path = ssl_socket.recv(1024).decode('utf-8')\n    if not os.path.exists(file_path):\n        ssl_socket.send(\"File not found\".encode('utf-8'))\n        return\n    try:\n        with open(file_path, 'rb') as f:\n            file_hash = hashlib.sha256(f.read()).hexdigest()\n        ssl_socket.send(file_hash.encode('utf-8'))\n    except Exception as e:\n        ssl_socket.send(f\"Error: {str(e)}\".encode('utf-8'))"
  },
  {
    "output": "def extract_and_plot(data):\n    import numpy as np\n    import matplotlib.pyplot as plt\n    values = [item for sublist in data for item in sublist if isinstance(item, (int, float))]\n    mean = np.mean(values)\n    median = np.median(values)\n    std_dev = np.std(values)\n    plt.hist(values, bins=20, density=True, alpha=0.6, color='g')\n    pdf = np.histogram(values, bins=20, density=True)[0]\n    x = np.linspace(min(values), max(values), 100)\n    pdf_values = (1 / (std_dev * np.sqrt(2 * np.pi))) * np.exp(- (x - mean)**2 / (2 * std_dev**2))\n    plt.plot(x, pdf_values, 'k--', linewidth=2)\n    plt.title('Histogram with PDF')\n    plt.xlabel('Value')\n    plt.ylabel('Density')\n    plt.show()"
  },
  {
    "output": "def standardize_and_plot(data, title, xlabel, ylabel):\n    import pandas as pd\n    import numpy as np\n    import matplotlib.pyplot as plt\n    data = data.drop_duplicates(subset='Name')\n    data['Age'] = (data['Age'] - data['Age'].mean()) / data['Age'].std()\n    data['Score'] = (data['Score'] - data['Score'].mean()) / data['Score'].std()\n    plt.scatter(data['Age'], data['Score'])\n    plt.title(title)\n    plt.xlabel(xlabel)\n    plt.ylabel(ylabel)\n    plt.show()"
  },
  {
    "output": "quantity = quantity.strip()\n        code = code.strip()\n        price = price.strip()\n        description = description.strip()\n        \n        # Assign product name based on code\n        product_name = ''\n        for product in product_names:\n            if product.lower() in code.lower():\n                product_name = product\n                break\n        \n        # Add a random number to the product name\n        random_number = random.randint(1, 100)\n        product_name += f\"_{random_number}\"\n        \n        # Append the parsed data to the list\n        data.append({\n            'ID': id_val,\n            'Quantity': quantity,\n            'Code': code,\n            'Price': price,\n            'Description': description,\n            'Product': product_name\n        })\n    \n    # Create DataFrame from the parsed data\n    df = pd.DataFrame(data)\n    \n    # Return the DataFrame\n    return df"
  },
  {
    "output": "def task_func(input_data):\n    try:\n        result = eval(input_data)\n        return result\n    except Exception as e:\n        return f\"Error: {str(e)}\""
  },
  {
    "output": "else:\n        value = random.randint(*RANGE[key])\n    row['Hour'] = hour\n    row['Date'] = now.strftime(\"%Y-%m-%d\")\n    row['Time'] = now.replace(hour=hour, minute=0, second=0, microsecond=0).strftime(\"%H:%M:%S\")\n    row['Temperature'] = value\n    row['Humidity'] = value\n    row['Pressure'] = value\n    data.append(row)\n    # Write data to CSV file\n    with open(file_name, 'w', newline='') as csvfile:\n        fieldnames = ['Hour', 'Date', 'Time', 'Temperature', 'Humidity', 'Pressure']\n        writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n        writer.writeheader()\n        for row in data:\n            writer.writerow(row)\n    return file_name"
  },
  {
    "output": "def task_func(rows=3, columns=2, seed=0):\n    np.random.seed(seed)\n    matrix = np.random.rand(rows, columns)\n    U, s, Vh = svd(matrix)\n    return (U, s, Vh)"
  },
  {
    "output": "def plot_letter_positions(word):\n    import numpy as np\n    import matplotlib.pyplot as plt\n    positions = np.array([ord(c) - ord('a') + 1 for c in word if c.isalpha()])\n    plt.bar(range(len(positions)), positions)\n    plt.xlabel('Letters')\n    plt.ylabel('Alphabet Position')\n    plt.title('Letter Positions in the English Alphabet')\n    plt.show()"
  },
  {
    "output": "import re\nimport socket\n\ndef task_func(ip_addresses: list) -> dict:\n    result = {}\n    for ip in ip_addresses:\n        if re.match(r'^(\\d{1,3}\\.){3}\\d{1,3}$', ip):\n            try:\n                hostname = socket.gethostbyaddr(ip)[0]\n                result[ip] = hostname\n            except socket.herror:\n                result[ip] = None\n    return result"
  },
  {
    "output": "def task_func():\n    return \"Hello, World!\""
  },
  {
    "output": "def task_func(url):\n    try:\n        import rsa\n        import urllib.request\n        import hashlib\n        response = urllib.request.urlopen(url)\n        content = response.read()\n        hash_obj = hashlib.sha256(content)\n        hash_hex = hash_obj.hexdigest()\n        return hash_hex\n    except Exception as e:\n        return str(e)"
  },
  {
    "output": "def predict_price_based_on_area(area):\n    import numpy as np\n    from sklearn.linear_model import LinearRegression\n    # Example training data\n    areas = np.array([100, 150, 200, 250, 300]).reshape(-1, 1)\n    prices = np.array([100000, 150000, 200000, 250000, 300000])\n    model = LinearRegression()\n    model.fit(areas, prices)\n    return model.predict([[area]])[0]"
  },
  {
    "output": "def generate_and_evaluate_model():\n    from sklearn.linear_model import LinearRegression\n    from sklearn.model_selection import train_test_split\n    from sklearn.metrics import r2_score\n    import numpy as np\n    \n    np.random.seed(42)\n    n_samples = 100\n    X = np.random.rand(n_samples, 1)\n    y = 2 * X + 1 + np.random.normal(scale=0.1, size=(n_samples, 1))\n    \n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n    \n    if len(X_test) < 2:\n        raise ValueError(\"Test set size is smaller than 2.\")\n    \n    model = LinearRegression().fit(X_train, y_train)\n    r2 = r2_score(y_test, model.predict(X_test))\n    \n    return r2, model"
  },
  {
    "output": "import seaborn as sns\nfrom scipy.stats import chi2_contingency\nimport matplotlib.pyplot as plt\n\ndef task_func(df1, df2, column1=\"feature1\", column2=\"feature2\"):\n    merged_df = pd.merge(df1, df2, on='id')\n    contingency_table = pd.crosstab(merged_df[column1], merged_df[column2])\n    chi2, p, _, _ = chi2_contingency(contingency_table)\n    plt.figure(figsize=(8, 6))\n    heatmap = sns.heatmap(contingency_table, annot=True, fmt=\"d\", cmap=\"coolwarm\")\n    plt.title(\"Contingency Table Heatmap\")\n    plt.xlabel(column1)\n    plt.ylabel(column2)\n    return (p, heatmap)"
  },
  {
    "output": "import random\n\ndef task_func(word):\n    if not word.isalpha():\n        raise ValueError\n    if len(word) < 2:\n        return []\n    possible_letters = 'abcdefghijklmnopqrstuvwxyz'\n    pairs = []\n    for i in range(len(word) - 1):\n        pair = word[i] + word[i+1]\n        if pair in possible_letters:\n            pairs.append(pair)\n    return pairs"
  },
  {
    "output": "def task_func(seed, num_entries, entry_length):\n    import random\n    random.seed(seed)\n    result = {}\n    for _ in range(num_entries):\n        entry = ''.join(random.choices('abcdefghijklmnopqrstuvwxyz', k=entry_length))\n        result[entry] = list(entry)\n    return result"
  },
  {
    "output": "def task_func():\n    import sqlite3\n    import random\n    import os\n    import sys\n    try:\n        conn = sqlite3.connect(':memory:')\n        c = conn.cursor()\n        c.execute('''CREATE TABLE data (id INTEGER PRIMARY KEY, value TEXT)''')\n        for _ in range(10):\n            c.execute(\"INSERT INTO data (value) VALUES (?)\", (random.choice(['a', 'b', 'c', 'd']),))\n        conn.commit()\n        c.execute(\"SELECT * FROM data\")\n        rows = c.fetchall()\n        for row in rows:\n            print(row)\n    except sqlite3.DatabaseError as e:\n        print(f\"Database error: {e}\")\n        conn.rollback()\n    except Exception as e:\n        print(f\"Unexpected error: {e}\")\n        sys.exit(1)\n    finally:\n        if 'conn' in locals():\n            conn.close()"
  },
  {
    "output": "import os\nimport re\nimport hashlib\n\ndef find_and_hash_files(directory, pattern):\n    matches = []\n    for root, dirs, files in os.walk(directory):\n        for file in files:\n            if re.search(pattern, file):\n                file_path = os.path.join(root, file)\n                with open(file_path, 'rb') as f:\n                    content = f.read()\n                    hash_obj = hashlib.sha256(content)\n                    matches.append((file_path, hash_obj.hexdigest()))\n    return matches"
  },
  {
    "output": "def task_func(input_str, width):\n    import textwrap\n    lines = input_str.split('\\n')\n    wrapped_lines = [textwrap.fill(line, width=width) for line in lines]\n    return '\\n'.join(wrapped_lines)"
  },
  {
    "output": "def task_func(source_dir, destination_dir):\n    import os\n    if not os.path.exists(source_dir) or not os.path.isdir(source_dir):\n        raise ValueError(\"Source path does not exist or is not a directory\")\n    files = os.listdir(source_dir)\n    for file in files:\n        src_path = os.path.join(source_dir, file)\n        dest_path = os.path.join(destination_dir, file)\n        os.system(f\"cp {src_path} {dest_path}\")\n    return (os.path.basename(source_dir), files)"
  },
  {
    "output": "def task_func(elements, subset_size, top_n=2):\n    if subset_size > len(elements) or subset_size == 0:\n        return 1\n    subsets = list(itertools.combinations(elements, subset_size))\n    sums = [sum(subset) for subset in subsets]\n    product_of_sums = 1\n    for s in sums:\n        product_of_sums *= s\n    top_sums = Series(sums).sort_values(ascending=False).head(top_n)\n    return product_of_sums, top_sums"
  },
  {
    "output": "data.append([f\"Average\", average_age, average_height, average_weight])\n    \n    # Write to CSV\n    filename = os.path.join(os.getcwd(), filename)\n    with open(filename, 'w', newline='') as csvfile:\n        writer = csv.writer(csvfile)\n        writer.writerow(COLUMNS)\n        writer.writerows(data)\n    \n    return filename"
  },
  {
    "output": "import matplotlib.pyplot as plt\nimport matplotlib.dates as mdates\nfrom datetime import datetime\nimport numpy as np\n\ndef parse_times_and_plot(times, time_format):\n    seconds = []\n    for time_str in times:\n        try:\n            dt = datetime.strptime(time_str, time_format)\n            seconds.append(dt.second)\n        except ValueError:\n            raise ValueError(f\"Invalid time string: {time_str}\")\n    plt.hist(seconds, bins=range(0, 61), align='left', rwidth=0.8)\n    plt.xlabel('Seconds')\n    plt.ylabel('Frequency')\n    plt.title('Histogram of Seconds')\n    plt.xticks(range(0, 60, 5))\n    plt.grid(axis='y', alpha=0.75)\n    plt.show()\n    return None"
  },
  {
    "output": "def task_func():\n    import math\n    return math.sqrt(16)"
  },
  {
    "output": "def extract_and_resolve_urls(input_string):\n    import re\n    import socket\n    urls = re.findall(r'https?://\\S+', input_string)\n    domains = []\n    for url in urls:\n        domain = url.split('/')[2]\n        domains.append(domain)\n    result = {}\n    for domain in domains:\n        try:\n            ip = socket.gethostbyname(domain)\n            result[domain] = ip\n        except socket.gaierror:\n            raise socket.gaierror(f\"Domain {domain} could not be resolved\")\n    return result"
  },
  {
    "output": "def process_audio_and_matrix(L, audio_file_path):\n    import numpy as np\n    from scipy.io import wavfile\n    import matplotlib.pyplot as plt\n    import librosa\n    import math\n\n    try:\n        fs, audio = wavfile.read(audio_file_path)\n        audio = audio / (2**15) if audio.dtype == np.int16 else audio\n        rms = np.sqrt(np.mean(audio**2))\n        spl = 20 * np.log10(rms / 2e-5)\n    except FileNotFoundError:\n        return \"File not found\"\n\n    M, N = len(L), len(L[0]) if L else 0\n    matrix = np.array(L).reshape(M, N)\n\n    if spl != 0:\n        matrix = matrix / (10 ** (spl / 20))\n\n    plt.specgram(matrix, Fs=fs, noverlap=100, cmap='viridis')\n    plt.colorbar()\n    plt.show()"
  },
  {
    "output": "import pandas as pd\nimport numpy as np\nimport datetime\nimport random\n\ndef generate_sensor_data(start_time, end_time, step, sensor_statuses):\n    timestamps = []\n    sensor_readings = []\n    sensor_statuses_list = []\n    \n    current_time = start_time\n    while current_time <= end_time:\n        timestamps.append(current_time)\n        timestamp_seconds = (current_time - datetime.datetime(1970, 1, 1)).total_seconds()\n        sine_val = np.sin(timestamp_seconds) + np.random.normal(0, 0.1)\n        cosine_val = np.cos(timestamp_seconds) + np.random.normal(0, 0.1)\n        tan_val = np.tan(timestamp_seconds) + np.random.normal(0, 0.1)\n        sensor_readings.append((sine_val, cosine_val, tan_val))\n        sensor_statuses_list.append(random.choice(sensor_statuses))\n        current_time += step\n    \n    df = pd.DataFrame({\n        'timestamp': timestamps,\n        'sine_reading': [r[0] for r in sensor_readings],\n        'cosine_reading': [r[1] for r in sensor_readings],\n        'tan_reading': [r[2] for r in sensor_readings],\n        'status': sensor_statuses_list\n    })\n    \n    return df"
  },
  {
    "output": "import seaborn as sns\nimport matplotlib.pyplot as plt\nimport numpy as np\n\ndef plot_heatmap_and_row_sums(data):\n    sns.heatmap(data, cmap='viridis', annot=True, fmt=\".1f\", cbar=True)\n    plt.title(\"Heatmap\")\n    plt.show()\n    row_sums = np.sum(data, axis=1)\n    for i, sum_val in enumerate(row_sums):\n        print(f\"Row {i+1} sum: {sum_val}\")"
  },
  {
    "output": "import os\nimport pandas as pd\n\ndef find_sorted_files(directory, pattern):\n    files = [f for f in os.listdir(directory) if f.startswith(pattern)]\n    sorted_files = sorted(files)\n    is_sorted = sorted_files == files\n    if not is_sorted:\n        return pd.DataFrame(columns=['File', 'Size'])\n    file_sizes = {file: os.path.getsize(os.path.join(directory, file)) for file in files}\n    df = pd.DataFrame(list(file_sizes.items()), columns=['File', 'Size'])\n    return df"
  },
  {
    "output": "def count_non_stopwords(sentence, stop_words):\n    import string\n    if not sentence:\n        return 0\n    sentence = sentence[:-1].rstrip(string.punctuation)\n    words = sentence.split()\n    return len([word for word in words if word.lower() not in stop_words])"
  },
  {
    "output": ""
  },
  {
    "output": "import re\nimport pandas as pd\n\ndef task_func(input_df):\n    def clean_text(text):\n        if pd.isna(text):\n            return None\n        cleaned = re.sub(r'[^a-zA-Z0-9]', '', str(text))\n        return cleaned\n    input_df['clean_text'] = input_df['text'].apply(clean_text)\n    input_df['text_length'] = input_df['clean_text'].apply(len)\n    input_df.drop(columns=['text'], inplace=True)\n    return input_df"
  },
  {
    "output": "def task_func(length=10):\n    import random\n    import string\n    try:\n        if length <= 0:\n            raise ValueError(\"Length must be a positive integer.\")\n        chars = string.ascii_letters + string.digits\n        generated_str = ''.join(random.choice(chars) for _ in range(length))\n        characteristics = {\n            'has_uppercase': any(c.isupper() for c in generated_str),\n            'has_lowercase': any(c.islower() for c in generated_str),\n            'has_digit': any(c.isdigit() for c in generated_str),\n            'has_special': any(not c.isalnum() for c in generated_str)\n        }\n        return (generated_str, characteristics)\n    except Exception as e:\n        return (str(e), {})"
  },
  {
    "output": "import csv\nimport os\nfrom collections import Counter\n\ndef task_func(file_path):\n    if not os.path.exists(file_path) or os.path.getsize(file_path) == 0:\n        return None\n    with open(file_path, 'r') as file:\n        reader = csv.reader(file)\n        words = []\n        for row in reader:\n            words.extend(row)\n    word_counts = Counter(words)\n    if not word_counts:\n        return None\n    return word_counts.most_common(1)[0]"
  },
  {
    "output": "# Draw bar chart\n    statistics.T.plot(kind='bar', figsize=(10, 6))\n    plt.title('Comparison of Statistics for Two Arrays')\n    plt.ylabel('Value')\n    plt.xlabel('Statistic')\n    plt.legend(['Array1', 'Array2'])\n    plt.show()"
  },
  {
    "output": "import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n\ndef scale_and_plot(df):\n    scaled_df = df.copy()\n    for id_group in scaled_df['id'].unique():\n        group = scaled_df[scaled_df['id'] == id_group]\n        age_min, age_max = group['Age'].min(), group['Age'].max()\n        income_min, income_max = group['Income'].min(), group['Income'].max()\n        scaled_df.loc[scaled_df['id'] == id_group, 'Age'] = (group['Age'] - age_min) / (age_max - age_min)\n        scaled_df.loc[scaled_df['id'] == id_group, 'Income'] = (group['Income'] - income_min) / (income_max - income_min)\n    plt.hist(scaled_df['Income'], bins=10, edgecolor='black')\n    plt.title('Histogram of Scaled Income')\n    plt.xlabel('Scaled Income')\n    plt.ylabel('Frequency')\n    plt.show()\n    return scaled_df, plt.hist(scaled_df['Income'], bins=10, edgecolor='black')[0]"
  },
  {
    "output": "import itertools\nimport matplotlib.pyplot as plt\n\ndef generate_subsets_and_plot(input_tuple, subset_size):\n    combinations = list(itertools.combinations(input_tuple, subset_size))\n    sums = [sum(combo) for combo in combinations]\n    fig, ax = plt.subplots()\n    ax.hist(sums, bins=10, edgecolor='black')\n    return ax, combinations, sums"
  },
  {
    "output": "import numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.decomposition import PCA\nfrom sklearn.datasets import make_classification\nfrom matplotlib import cm\nfrom matplotlib.ticker import LinearLocator, FormatStrFormatter\n\nX, y = make_classification(n_samples=100, n_features=20, n_informative=10, n_redundant=5, random_state=42)\npca = PCA(n_components=5)\nX_pca = pca.fit_transform(X)\ncov_matrix = np.cov(X_pca, rowvar=False)\nfig, ax = plt.subplots()\nim = ax.imshow(cov_matrix, cmap='viridis', interpolation='nearest')\nax.set_xticks([])\nax.set_yticks([])\nplt.colorbar(im, ax=ax, shrink=0.6)\nplt.show()"
  },
  {
    "output": "def task_func(text):\n    if not text:\n        raise ValueError(\"Input text cannot be empty\")\n    import re\n    password = ''\n    for char in text:\n        if re.match(r'[a-zA-Z0-9]', char):\n            password += char\n    if len(password) < 8:\n        password += '12345678'\n    return password.replace(' ', '')"
  },
  {
    "output": "import pandas as pd\nimport matplotlib.pyplot as plt\n\ndef load_and_analyze_email_data(file_path):\n    data = pd.read_csv(file_path)\n    sum_values = data['value'].sum()\n    mean_values = data['value'].mean()\n    plt.plot(data['date'], data['value'])\n    plt.xlabel('Date')\n    plt.ylabel('Value')\n    plt.title('Email Value Over Time')\n    plt.show()\n    return sum_values, mean_values"
  },
  {
    "output": "color='red', label='Standard Normal PDF')\n    ax.legend()\n    return ax, normalized_data"
  },
  {
    "output": "import time\nimport statistics\n\ndef measure_delay_stats(delay_time, num_samples):\n    delays = []\n    for _ in range(num_samples):\n        start = time.time()\n        time.sleep(delay_time)\n        end = time.time()\n        delays.append(end - start)\n    mean = statistics.mean(delays)\n    stdev = statistics.stdev(delays)\n    return (mean, stdev)"
  },
  {
    "output": "def task_func(API_URL):\n    try:\n        with urllib.request.urlopen(API_URL) as response:\n            data = json.loads(response.read().decode())\n            ip_address = re.search(IP_REGEX, data.get('result', ''))\n            if ip_address:\n                return ip_address.group()\n            else:\n                raise ValueError(\"IP address not found in the response.\")\n    except Exception as e:\n        return str(e)"
  },
  {
    "output": "def task_func(s):\n    last_dash = s.rfind('-')\n    prefix = s[:last_dash]\n    from collections import Counter\n    return dict(Counter(filter(str.islower, prefix)))"
  },
  {
    "output": "import base64\nimport os\nfrom getpass import getpass\nfrom cryptography.hazmat.primitives import hashes\nfrom cryptography.hazmat.primitives.kdf.pbkdf2 import PBKDF2HMAC\nfrom cryptography.hazmat.backends import default_backend\n\ndef hash_password(password):\n    if not password:\n        raise ValueError(\"Password cannot be None or empty\")\n    salt = os.urandom(16)\n    kdf = PBKDF2HMAC(\n        algorithm=hashes.SHA256(),\n        length=32,\n        salt=salt,\n        iterations=100000,\n        backend=default_backend()\n    )\n    hashed_password = kdf.derive(password.encode())\n    return base64.b64encode(salt).decode(), base64.b64encode(hashed_password).decode()"
  },
  {
    "output": "def task_func(df, conditions, dependent_var, independent_vars):\n    filtered_df = df.query(conditions)\n    if filtered_df.empty:\n        return None\n    from statsmodels.api import OLS\n    X = filtered_df[independent_vars]\n    y = filtered_df[dependent_var]\n    results = OLS(y, X).fit()\n    return results.params"
  },
  {
    "output": "```python\nimport pandas as pd\nfrom datetime import datetime\nimport holidays\n\ndef task_func(start_date=datetime(2023, 1, 1), end_date=datetime(2023, 12, 31), country='US'):\n    # Validate start_date and end_date\n    if not isinstance(start_date, datetime) or not isinstance(end_date, datetime):\n        raise ValueError(\"Both start_date and end_date must be datetime objects.\")\n    if start_date > end_date:\n        raise ValueError(\"start_date cannot be after end_date.\")\n    \n    # Fetch public holidays for the specified country"
  },
  {
    "output": "def task_func(df, col1, col2, N):\n    if not all(col in df.columns for col in [col1, col2]):\n        raise ValueError(\"Columns not found in DataFrame\")\n    if N <= 0:\n        raise ValueError(\"N must be a positive integer\")\n    if N > len(df):\n        raise ValueError(\"N cannot be larger than the number of rows\")\n    diffs = df[col1] - df[col2]\n    abs_diffs = abs(diffs)\n    top_n_indices = abs_diffs.nlargest(N).index\n    top_n_values = df.loc[top_n_indices, [col1, col2]]\n    t_stat, p_val = stats.ttest_ind(top_n_values[col1], top_n_values[col2])\n    return p_val"
  },
  {
    "output": "def extract_domains_geolocation(input_string):\n    import re\n    import requests\n    from urllib.parse import urlparse\n    \n    urls = re.findall(r'https?://(?:www\\.)?([^\"\\']+)/?', input_string)\n    domains = list(set([urlparse(url).netloc for url in urls]))\n    \n    geolocation_data = {}\n    for domain in domains:\n        try:\n            response = requests.get(f'https://ipapi.co/{domain}/json/').json()\n            geolocation_data[domain] = response\n        except:\n            geolocation_data[domain] = {\"error\": \"Failed to retrieve geolocation data\"}\n    \n    return geolocation_data"
  },
  {
    "output": "df = pd.DataFrame(data)\n    \n    # Draw the bar chart\n    plt.figure(figsize=(12, 6))\n    plt.bar(df['Timezone'], df['Datetime'])\n    plt.xticks(rotation=45)\n    plt.tight_layout()\n    plt.show()"
  },
  {
    "output": "def task_func(string, patterns):\n    import re\n    if not isinstance(string, str):\n        raise ValueError(\"Input string must be a string.\")\n    if not isinstance(patterns, list):\n        raise ValueError(\"Input patterns must be a list.\")\n    result = {}\n    for pattern in patterns:\n        if not pattern:\n            continue\n        regex = re.compile(pattern)\n        matches = regex.findall(string)\n        result[pattern] = len(matches)\n    return result"
  },
  {
    "output": "def process_log_file(log_file_path, keywords):\n    import re\n    with open(log_file_path, 'r') as file:\n        lines = file.readlines()\n    result = []\n    for line in lines:\n        for keyword in keywords:\n            if re.search(r'\\b' + re.escape(keyword) + r'\\b', line):\n                timestamp = line.split(' ')[0]\n                message = ' '.join(line.split(' ')[1:])\n                formatted = f\"{keyword}{' ' * 20}{timestamp}{' ' * 20}{message}\"\n                result.append(formatted)\n                break\n    return result"
  },
  {
    "output": "import requests\n\ndef task_func(username):\n    url = f\"https://api.github.com/users/{username}\"\n    response = requests.get(url)\n    response.raise_for_status()\n    data = response.json()\n    normalized_data = {k: v.encode('ascii').decode('ascii') if isinstance(v, str) else v for k, v in data.items()}\n    return normalized_data"
  },
  {
    "output": "def task_func(data, filename=\"output.csv\"):\n    import csv\n    try:\n        with open(filename, 'w', newline='') as csvfile:\n            writer = csv.writer(csvfile)\n            writer.writerows(data)\n        return filename\n    except Exception as e:\n        raise RuntimeError(f\"Error creating CSV file: {e}\") from e"
  },
  {
    "output": "import pandas as pd\nimport csv\nimport os\n\ndef read_excel_to_csv_and_sum_columns(file_path, output_csv_path):\n    try:\n        df = pd.read_excel(file_path)\n        df.to_csv(output_csv_path, index=False)\n        sums = df.sum(numeric_only=True)\n        return sums.to_dict()\n    except Exception as e:\n        return {\"error\": str(e)}"
  },
  {
    "output": "def normalize_and_plot(csv_file):\n    import pandas as pd\n    import matplotlib.pyplot as plt\n    df = pd.read_csv(csv_file)\n    df['column1'] = (df['column1'] - df['column1'].min()) / (df['column1'].max() - df['column1'].min())\n    plt.plot(df['column1'])\n    plt.title('Normalized Column1 Values')\n    plt.xlabel('Index')\n    plt.ylabel('Normalized Value')\n    plt.show()"
  },
  {
    "output": "import random\nimport math\nimport matplotlib.pyplot as plt\n\ndef task_func(points_count=1000, radius=1):\n    angles = [random.uniform(0, 2 * math.pi) for _ in range(points_count)]\n    radii = [random.uniform(0, radius) for _ in range(points_count)]\n    x = [r * math.cos(theta) for r, theta in zip(radii, angles)]\n    y = [r * math.sin(theta) for r, theta in zip(radii, angles)]\n    fig, ax = plt.subplots()\n    ax.scatter(x, y)\n    ax.set_aspect('equal')\n    return ax"
  },
  {
    "output": "```python\nimport pandas as pd\nimport string\n\ndef task_func(word):\n    # Input validation\n    if not isinstance(word, str):\n        raise ValueError(\"Input must be a string.\")\n    if not word.islower():\n        raise"
  },
  {
    "output": "cot(\"Create a dictionary containing all possible two-letter combinations of the lowercase English alphabets. The dictionary values represent the frequency of these two-letter combinations in the given word. If a combination does not appear in the word, its value will be 0. The function should output a dictionary with keys as two-letter alphabet combinations and values as their counts in the word.\")"
  },
  {
    "output": "import numpy as np\n\ndef random_walk(length):\n    if length < 0:\n        raise ValueError(\"Length cannot be negative\")\n    steps = np.random.choice([-1, 1], size=length)\n    positions = np.zeros(length + 1, dtype=int)\n    for i in range(length):\n        positions[i + 1] = positions[i] + steps[i]\n    return positions"
  },
  {
    "output": "def task_func(input_data):\n    try:\n        if not input_data:\n            raise ValueError(\"Input data is empty\")\n        result = sum(input_data)\n        return result\n    except TypeError as e:\n        return f\"TypeError: {str(e)}\"\n    except ValueError as e:\n        return f\"ValueError: {str(e)}\"\n    except Exception as e:\n        return f\"Unexpected error: {str(e)}\""
  },
  {
    "output": "import pandas as pd\nfrom sklearn.preprocessing import MinMaxScaler\n\ndef task_func(list_of_pairs):\n    if not list_of_pairs:\n        raise Exception(\"Input array is empty\")\n    try:\n        df = pd.DataFrame(list_of_pairs, columns=['Category', 'Value'])\n        if not pd.api.types.is_numeric_dtype(df['Value']):\n            raise ValueError(\"Values are not numeric\")\n        scaler = MinMaxScaler()\n        df['Value'] = scaler.fit_transform(df[['Value']])\n        return df\n    except Exception as e:\n        raise e"
  },
  {
    "output": "True"
  },
  {
    "output": "import json\nimport zlib\nimport base64\n\ndef serialize_compress_encode(data):\n    json_str = json.dumps(data)\n    compressed = zlib.compress(json_str.encode('utf-8'))\n    encoded = base64.b64encode(compressed)\n    return encoded.decode('utf-8')"
  },
  {
    "output": "import random\nimport string\nimport collections\n\ndef task_func(n_strings, string_length):\n    random_strings = [''.join(random.choice(string.ascii_letters + string.digits) for _ in range(string_length)) for _ in range(n_strings)]\n    char_count = collections.Counter()\n    for s in random_strings:\n        char_count.update(s)\n    return dict(char_count)"
  },
  {
    "output": "from flask import Flask\nfrom flask_mail import Mail\nimport os\n\ndef initialize_flask_mail(app_name):\n    app = Flask(app_name)\n    app.config['MAIL_SERVER'] = os.getenv('MAIL_SERVER', 'smtp.example.com')\n    app.config['MAIL_PORT'] = int(os.getenv('MAIL_PORT', '587'))\n    app.config['MAIL_USE_TLS'] = os.getenv('MAIL_USE_TLS', 'True').lower() in ('true', '1', 't')\n    app.config['MAIL_USE_SSL'] = os.getenv('MAIL_USE_SSL', 'False').lower() in ('true', '1', 't')\n    app.config['MAIL_USERNAME'] = os.getenv('MAIL_USERNAME', '')\n    app.config['MAIL_PASSWORD'] = os.getenv('MAIL_PASSWORD', '')\n    app.config['MAIL_DEFAULT_SENDER'] = os.getenv('MAIL_DEFAULT_SENDER', '')\n    mail = Mail(app)\n    return mail"
  },
  {
    "output": "def task_func(X_train, X_test, y_train, y_test):\n    if not isinstance(X_train, (list, tuple, set)) or not isinstance(X_test, (list, tuple, set)):\n        raise ValueError(\"X_train and X_test must be iterable\")\n    if not isinstance(y_train, (list, tuple, set)) or not isinstance(y_test, (list, tuple, set)):\n        raise ValueError(\"y_train and y_test must be iterable\")\n    if len(X_train) != len(y_train) or len(X_test) != len(y_test):\n        raise ValueError(\"Features and targets must have the same length\")\n    if len(X_train) < 2 or len(X_test) < 2:\n        raise ValueError(\"At least two samples are required for regression\")\n    from sklearn.linear_model import LinearRegression\n    from sklearn.metrics import r2_score\n    model = LinearRegression().fit(X_train, y_train)\n    score = r2_score(y_test, model.predict(X_test))\n    return score"
  },
  {
    "output": "import matplotlib.pyplot as plt\nimport pandas as pd\nimport numpy as np\nimport os\n\nnp.random.seed(42)\ndata = np.random.rand(10, 3)\ndf = pd.DataFrame(data, columns=['A', 'B', 'C'])\nfig, ax = plt.subplots()\nax.bar(range(10), df['A'], label='A')\nax.bar(range(10), df['B'], bottom=df['A'], label='B')\nax.bar(range(10), df['C'], bottom=df['A'] + df['B'], label='C')\nax.legend()\nplt.savefig('stacked_bar_chart.png')\nplt.close()\nfig, df, 'stacked_bar_chart.png'"
  },
  {
    "output": "import pandas as pd\nfrom sklearn.preprocessing import LabelEncoder\n\ndef task_func(num_samples, gender_options, age_range, location_options):\n    if not isinstance(num_samples, int):\n        raise ValueError(\"num_samples must be an integer\")\n    df = pd.DataFrame()\n    df['gender'] = pd.Series([LabelEncoder().fit(gender_options).transform([gender_options[i % len(gender_options)] for i in range(num_samples)]) for _ in range(num_samples)])\n    df['age'] = pd.Series([pd.Series(range(age_range[0], age_range[1] + 1))[i % len(pd.Series(range(age_range[0], age_range[1] + 1)))] for i in range(num_samples)])\n    df['location'] = pd.Series([LabelEncoder().fit(location_options).transform([location_options[i % len(location_options)] for i in range(num_samples)]) for _ in range(num_samples)])\n    return df"
  },
  {
    "output": "def task_func(text1, text2):\n    import re\n    from collections import Counter\n    from Levenshtein import ratio\n    text1 = re.sub(r'[^a-z0-9\\s]', '', text1).lower()\n    text2 = re.sub(r'[^a-z0-9\\s]', '', text2).lower()\n    tf1 = Counter(text1.split())\n    tf2 = Counter(text2.split())\n    vectors = [list(tf1.values()), list(tf2.values())]\n    import math\n    dot_product = sum(a * b for a, b in zip(vectors[0], vectors[1]))\n    magnitude1 = math.sqrt(sum(a**2 for a in vectors[0]))\n    magnitude2 = math.sqrt(sum(a**2 for a in vectors[1]))\n    cosine_similarity = dot_product / (magnitude1 * magnitude2) if magnitude1 * magnitude2 != 0 else 0\n    levenshtein_ratio = ratio(text1, text2)\n    return (cosine_similarity, levenshtein_ratio)"
  },
  {
    "output": "import subprocess\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\ndef run_script_and_plot(script_path):\n    subprocess.run(['python', script_path], check=True)\n    df = pd.read_csv('output.csv')\n    if df.shape[1] != 2:\n        raise ValueError(\"CSV must have exactly two columns\")\n    fig, ax = plt.subplots()\n    df.plot(kind='bar', ax=ax, x=df.columns[0], y=df.columns[1])\n    return df, ax"
  },
  {
    "output": "def task_func(matrix, tensor):\n    import numpy as np\n    import pandas as pd\n    if len(matrix.shape) != 2 or len(tensor.shape) != 3:\n        raise ValueError(\"Matrix must be 2D and tensor must be 3D\")\n    if matrix.shape[1] != tensor.shape[1]:\n        raise ValueError(\"Matrix and tensor must have the same number of columns\")\n    result = np.tensordot(matrix, tensor, axes=([1], [1]))\n    result = result / np.linalg.norm(result)\n    return pd.DataFrame(result)"
  },
  {
    "output": "def process_data(data):\n    import pandas as pd\n    import matplotlib.pyplot as plt\n    df = pd.DataFrame(data)\n    fig, ax = plt.subplots()\n    df.plot(kind='line', ax=ax)\n    return df, ax"
  },
  {
    "output": "def calculate_stats(stock_data, column):\n    if not isinstance(stock_data, list) or not all(isinstance(row, dict) for row in stock_data):\n        raise ValueError(\"Invalid stock data format\")\n    if column not in stock_data[0]:\n        raise KeyError(f\"Column '{column}' not found in stock data\")\n    values = [row[column] for row in stock_data]\n    if not values:\n        return {\"sum\": None, \"mean\": None, \"min\": None, \"max\": None}\n    sum_val = sum(values)\n    mean_val = sum_val / len(values)\n    min_val = min(values)\n    max_val = max(values)\n    return {\"sum\": sum_val, \"mean\": mean_val, \"min\": min_val, \"max\": max_val}"
  },
  {
    "output": "import pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\ndef task_func(list_of_pairs):\n    df = pd.DataFrame(list_of_pairs, columns=['Category', 'Value'])\n    plt.figure(figsize=(10, 6))\n    sns.barplot(x='Category', y='Value', data=df)\n    plt.title('Category vs Value')\n    plt.xlabel('Category')\n    plt.ylabel('Value')\n    plt.xticks(rotation=45)\n    plt.tight_layout()\n    return df, plt.gca()"
  },
  {
    "output": "import numpy as np\nimport matplotlib.pyplot as plt\nfrom mpl_toolkits.mplot3d import Axes3D\n\ndef task_func(n_points=100, random_seed=None):\n    np.random.seed(random_seed)\n    points = np.random.rand(n_points, 3)\n    fig = plt.figure()\n    plot = fig.add_subplot(111, projection='3d')\n    plot.scatter(points[:, 0], points[:, 1], points[:, 2], c='blue', marker='o')\n    plot.set_xlabel('X Axis')\n    plot.set_ylabel('Y Axis')\n    plot.set_zlabel('Z Axis')\n    return (points, plot)"
  },
  {
    "output": "import os\nimport re\nimport shutil\n\ndef task_func(source_dir, target_dir, pattern):\n    count = 0\n    for filename in os.listdir(source_dir):\n        if re.match(pattern, filename):\n            source_path = os.path.join(source_dir, filename)\n            target_path = os.path.join(target_dir, filename)\n            shutil.move(source_path, target_path)\n            count += 1\n    return count"
  },
  {
    "output": "from collections import Counter\n\ndef task_func(result):\n    urls = [item['url'] for item in result if 'url' in item]\n    url_counts = Counter(urls)\n    return dict(url_counts)"
  },
  {
    "output": "import os\nimport random\n\ndef create_random_files(n, directory):\n    os.makedirs(directory, exist_ok=True)\n    count = 0\n    for i in range(1, n+1):\n        filename = os.path.join(directory, f'file_{i}.txt')\n        with open(filename, 'w') as f:\n            f.write(str(random.randint(0, 9)))\n            f.seek(0)\n        count += 1\n    return count"
  },
  {
    "output": "def calculate_statistics_and_create_pie_chart(data, column):\n    import pandas as pd\n    import matplotlib.pyplot as plt\n    if data.empty:\n        return \"Data is empty\"\n    stats = data[column].describe()\n    plt.figure()\n    plt.pie(stats['count'], labels=data[column].unique())\n    plt.title(f'Statistics for {column}')\n    plt.show()\n    return stats"
  },
  {
    "output": "def task_func(df):\n    from sklearn.cluster import KMeans\n    import pandas as pd\n    if not pd.api.types.is_numeric_dtype(df.dtypes):\n        raise ValueError(\"DataFrame contains non-numeric values\")\n    kmeans = KMeans(n_clusters=3).fit(df)\n    return kmeans.labels_, kmeans"
  },
  {
    "output": "def generate_and_plot_scores(students):\n    import numpy as np\n    import matplotlib.pyplot as plt\n    scores = np.random.randint(0, 100, size=len(students))\n    sorted_scores = np.sort(scores)\n    plt.bar(students, sorted_scores)\n    plt.xlabel('Students')\n    plt.ylabel('Scores')\n    plt.title('Student Scores')\n    plt.show()\n    return sorted_scores, plt.gcf()"
  },
  {
    "output": "def task_func(n, pattern):\n    import re\n    import random\n    import string\n    while True:\n        s = ''.join(random.choices(string.ascii_letters, k=n))\n        if re.fullmatch(pattern, s):\n            return s"
  },
  {
    "output": "def generate_random_sine_wave():\n    import numpy as np\n    import matplotlib.pyplot as plt\n    freq = np.random.uniform(0.5, 2.5)\n    amplitude = np.random.uniform(0.5, 2.5)\n    phase_shift = np.random.uniform(0, 2*np.pi)\n    x = np.linspace(0, 2*np.pi, 1000)\n    y = amplitude * np.sin(freq * x + phase_shift)\n    fig, ax = plt.subplots()\n    ax.plot(x, y)\n    ax.set_title(\"Random Sine Wave\")\n    ax.set_xlabel(\"Time\")\n    ax.set_ylabel(\"Amplitude\")\n    return ax"
  },
  {
    "output": "def task_func(tuples_list, n_components):\n    data = np.array(tuples_list)\n    pca = PCA(n_components=n_components)\n    transformed_data = pca.fit_transform(data)\n    return transformed_data"
  },
  {
    "output": "def task_func(nested_list, menu_items):\n    from collections import Counter\n    import pandas as pd\n    flattened = [item for sublist in nested_list for item in sublist]\n    counts = Counter(flattened)\n    df = pd.DataFrame(counts.items(), columns=['Item', 'Count'])\n    return df"
  },
  {
    "output": "import requests\nfrom bs4 import BeautifulSoup\n\ndef task_func(url, tag):\n    try:\n        response = requests.get(url)\n        soup = BeautifulSoup(response.text, 'html.parser')\n        element = soup.find(tag)\n        return element.get_text() if element else None\n    except:\n        return None"
  },
  {
    "output": "def task_func(json_data):\n    utc_datetime = datetime.fromisoformat(json_data['utc_datetime'])\n    return utc_datetime.weekday() >= 5"
  },
  {
    "output": "def inspect_function(f):\n    import json\n    from inspect import signature, Parameter, islambda\n    sig = signature(f)\n    params = []\n    for name, param in sig.parameters.items():\n        default = param.default if param.default is not Parameter.empty else None\n        annotation = param.annotation if param.annotation is not Parameter.empty else ''\n        params.append({\n            \"name\": name,\n            \"default\": default,\n            \"annotation\": str(annotation)\n        })\n    return json.dumps({\n        \"name\": f.__name__,\n        \"arguments\": params,\n        \"is_lambda\": islambda(f)\n    })"
  },
  {
    "output": "import re\nfrom nltk.corpus import words\n\ndef task_func(s, n):\n    word_list = set()\n    words_corpus = set(words.words)\n    for word in re.findall(r'\\b\\w+\\b', s.lower()):\n        if word in words_corpus:\n            word_list.add(word)\n            if len(word_list) >= n:\n                return list(word_list)\n    return list(word_list)"
  },
  {
    "output": "import random\nimport json\nfrom datetime import datetime\nfrom uuid import uuid4\n\ndef task_func():\n    people = [\"Alice\", \"Bob\", \"Charlie\", \"David\", \"Eve\"]\n    selected_person = random.choice(people)\n    timestamp = datetime.utcnow().isoformat()\n    data = {\n        \"person\": selected_person,\n        \"timestamp\": timestamp\n    }\n    return json.dumps(data)"
  },
  {
    "output": "task_func(date_str, from_tz)"
  },
  {
    "output": "import csv\n\ndef find_most_common_values(csv_file_path):\n    with open(csv_file_path, 'r') as file:\n        reader = csv.reader(file)\n        headers = next(reader)\n        if not headers:\n            return {}\n        rows = list(reader)\n        if not rows:\n            return {}\n        result = {}\n        for col_index, header in enumerate(headers):\n            column_values = [row[col_index] for row in rows]\n            unique_values = {}\n            for value in column_values:\n                unique_values[value] = unique_values.get(value, 0) + 1\n            max_count = max(unique_values.values())\n            candidates = [k for k, v in unique_values.items() if v == max_count]\n            candidates.sort()\n            result[header] = candidates[0]\n        return result"
  },
  {
    "output": "def create_status_pie_chart(df):\n    import pandas as pd\n    import matplotlib.pyplot as plt\n    if not isinstance(df, pd.DataFrame) or 'Status' not in df.columns:\n        raise ValueError(\"Input must be a pandas DataFrame with a 'Status' column.\")\n    status_counts = df['Status'].value_counts()\n    fig, ax = plt.subplots()\n    ax.pie(status_counts, labels=status_counts.index, autopct='%1.1f%%')\n    ax.axis('equal')\n    return ax"
  },
  {
    "output": "def process_and_scale_data(df, plot=False):\n    if 'Date' not in df.columns or 'Value' not in df.columns:\n        raise ValueError(\"DataFrame must contain 'Date' and 'Value' columns\")\n    df['Date'] = pd.to_datetime(df['Date'])\n    df.set_index('Date', inplace=True)\n    df = df['Value'].str.split(',', expand=True).apply(pd.to_numeric, errors='coerce')\n    scaler = StandardScaler()\n    scaled = scaler.fit_transform(df)\n    scaled_df = pd.DataFrame(scaled, index=df.index, columns=df.columns)\n    if plot:\n        scaled_df.plot(kind='bar')\n    return scaled_df, plt if plot else None"
  },
  {
    "output": "import random\nfrom collections import Counter\n\n# Constants\nCARDS = ['2', '3', '4', '5', '6', '7', '8', '9', '10', 'J', 'Q', 'K', 'A']\n\ndef task_func(x=1):\n    # Create a list of all possible cards\n    all_cards = CARDS * 4  # Assuming 4 suits, though the problem states without suits\n    \n    # Shuffle the deck\n    random.shuffle(all_cards)\n    \n    # Draw x hands\n    hands = []\n    drawn_cards = []\n    for _ in range(x):\n        hand = random.sample(all_cards, 5)\n        hands.append(hand)\n        drawn_cards.extend(hand)\n    \n    # Count the drawn cards\n    card_counter = Counter(drawn_cards)\n    \n    return (hands, card_counter)"
  },
  {
    "output": "import pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nCOLUMNS = ['col1', 'col2', 'col3']\n\ndef task_func(data):\n    df = pd.DataFrame(data, columns=COLUMNS)\n    pivot_df = df.pivot_table(index='col1', columns='col2', values='col3', aggfunc='count').fillna(0)\n    ax = sns.heatmap(pivot_df, annot=True, fmt='d', cmap='viridis')\n    return df, ax"
  },
  {
    "output": "def task_func(range_limit, num_samples):\n    if range_limit <= 1:\n        raise ValueError(\"range_limit must be greater than 1\")\n    import numpy as np\n    import seaborn as sns\n    import matplotlib.pyplot as plt\n    np.random.seed(0)\n    data = np.random.uniform(low=0, high=range_limit, size=num_samples)\n    data.sort()\n    sns.histplot(data, kde=False)\n    plt.show()"
  },
  {
    "output": "from collections import Counter\nimport itertools\n\ndef task_func(d):\n    flattened = list(itertools.chain.from_iterable(d.values()))\n    counts = Counter(flattened)\n    return dict(counts)"
  },
  {
    "output": "def task_func():\n    import random\n    numbers = list(range(1, 11))\n    random.shuffle(numbers)\n    return numbers[:5]"
  },
  {
    "output": "def analyze_pattern_occurrences(strings, pattern):\n    counts = {}\n    for s in strings:\n        count = s.count(pattern)\n        if count > 0:\n            counts[s] = count\n    return counts"
  },
  {
    "output": "```python\nfrom sklearn.preprocessing import MinMaxScaler\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport numpy as np\n\ndef task_func(df):\n    # Step 1: Replace missing values with column averages\n    numeric_cols = df.select_dtypes(include=['number']).columns\n    df[numeric_cols] = df[numeric_cols].fillna(df[numeric_cols].mean())\n\n    # Step 2: Normalize numeric columns using MinMaxScaler\n    scaler = MinMaxScaler()\n    normalized_data = scaler.fit_transform(df[numeric_cols])\n\n    # Step 3: Replace original numeric columns with normalized values\n    df[numeric_cols] = normalized_data\n\n    # Step 4: Draw a box plot for each numeric column\n    fig, ax = plt.subplots()\n    df[numeric_cols].boxplot(ax=ax)\n\n    # Return the normalized"
  },
  {
    "output": "import hashlib\nfrom blake3 import blake3\n\ndef task_func(data):\n    blake3_hash = blake3(data.encode()).digest()\n    hex_hash = blake3_hash.hex()\n    md5_hash = hashlib.md5(hex_hash.encode()).hexdigest()\n    return md5_hash"
  },
  {
    "output": "import re\nimport numpy as np\nfrom scipy.stats import ttest_rel\n\ndef task_func(text1, text2):\n    words1 = text1.split()\n    words2 = text2.split()\n    if len(words1) != len(words2):\n        return (np.nan, np.nan)\n    t_stat, p_val = ttest_rel(words1, words2)\n    return (t_stat, p_val)"
  },
  {
    "output": "import os\nimport json\nimport random\n\ndef task_func(directory, n):\n    os.makedirs(directory, exist_ok=True)\n    for i in range(n):\n        file_path = os.path.join(directory, f'file_{i}.json')\n        data = {'number': random.randint(1, 100)}\n        with open(file_path, 'w') as f:\n            json.dump(data, f)\n        with open(file_path, 'r') as f:\n            content = f.read()\n        with open(file_path, 'w') as f:\n            f.write(content)"
  },
  {
    "output": "from flask import Flask, redirect, url_for, render_template, request\nfrom flask_login import LoginManager, UserMixin, login_user, logout_user, login_required, current_user\nfrom flask_wtf import FlaskForm\nfrom wtforms import StringField, PasswordField, validators\nimport os\n\napp = Flask(__name__)\napp.secret_key = os.urandom(24)\n\nlogin_manager = LoginManager()\nlogin_manager.init_app(app)\nlogin_manager.login_view = 'login'\n\nclass User(UserMixin):\n    def __init__(self, id, username, password):\n        self.id = id\n        self.username = username\n        self.password = password\n\n    def get_id(self):\n        return self.id\n\n@login_manager.user_loader\ndef load_user(user_id):\n    return User(1, 'testuser', 'password')\n\nclass LoginForm(FlaskForm):\n    username = StringField('Username', [validators.DataRequired()])\n    password = PasswordField('Password', [validators.DataRequired()])\n\n@app.route('/login', methods=['GET', 'POST'])\ndef login():\n    form = LoginForm(request.form)\n    if request.method == 'POST' and form.validate():\n        user = load_user(1)\n        if user and user.password == form.password.data:\n            login_user(user)\n            return redirect(url_for('protected'))\n    return render_template('login.html', form=form)\n\n@app.route('/logout')\ndef logout():\n    logout_user()\n    return redirect(url_for('login'))\n\n@app.route('/protected')\n@login_required\ndef protected():\n    return f'Logged in as: {current_user.username}'\n\nif __name__ == '__main__':\n    app.run(debug=True)"
  },
  {
    "output": "def task_func(text):\n    import pandas as pd\n    import regex as re\n    pattern = r'Score: (\\d+), Category: ([A-Za-z]+)'\n    matches = re.findall(pattern, text)\n    data = [{'Score': int(score), 'Category': category} for score, category in matches]\n    df = pd.DataFrame(data)\n    return df"
  },
  {
    "output": "def task_func(df, FEATURES, TARGET, dict_mapping, plot_histogram=False):\n    if not isinstance(df, pd.DataFrame) or TARGET not in df.columns or not all(f in df.columns for f in FEATURES):\n        raise ValueError(\"Invalid input DataFrame or missing columns\")\n    for col, mapping in dict_mapping.items():\n        if col in df.columns:\n            df[col] = df[col].replace(mapping)\n    scaler = StandardScaler()\n    df[FEATURES] = scaler.fit_transform(df[FEATURES])\n    if plot_histogram:\n        plt.hist(df[TARGET], bins=20)\n        plt.show()"
  },
  {
    "output": "def task_func(text):\n    text = text.lower()\n    words = [word for word in re.findall(r'\\b\\w+\\b', text) if word not in [\"a\", \"an\", \"the\", \"in\", \"is\", \"are\"]]\n    return pd.Series(words).value_counts()"
  },
  {
    "output": "import pandas as pd\nimport numpy as np\n\ndef task_func():\n    np.random.seed(42)\n    products = ['Product A', 'Product B', 'Product C', 'Product D']\n    months = ['Jan', 'Feb', 'Mar', 'Apr', 'May', 'Jun', 'Jul', 'Aug', 'Sep', 'Oct', 'Nov', 'Dec']\n    sales_data = np.random.randint(100, 1000, size=(len(products), len(months)))\n    df = pd.DataFrame(sales_data, columns=months, index=products)\n    df['Average Sales'] = df.mean(axis=1)\n    return df.reset_index().rename(columns={0: 'Product'})"
  },
  {
    "output": "def task_func():\n    try:\n        result = 10 / 0\n    except ZeroDivisionError:\n        return \"Error: Division by zero\"\n    except Exception as e:\n        return f\"An unexpected error occurred: {str(e)}\"\n    return result"
  },
  {
    "output": "def process_text_and_plot(text):\n    import re\n    from collections import Counter\n    import matplotlib.pyplot as plt\n\n    words = re.findall(r'\\b\\$\\w+\\b', text)\n    filtered_words = [word for word in words if not re.fullmatch(r'[$\\w]+', word)]\n    word_counts = Counter(filtered_words)\n    word_counts.plot(kind='bar')\n    plt.show()"
  },
  {
    "output": "def task_func(data, filename):\n    import numpy as np\n    if not data:\n        with open(filename, 'w') as f:\n            f.write('')\n        return []\n    means = []\n    for i in range(1, len(data[0])):\n        column = [float(x[i]) if isinstance(x[i], (int, float)) else np.nan for x in data]\n        mean = np.nanmean(column)\n        means.append(mean)\n        with open(filename, 'a') as f:\n            f.write(f'Position {i}: {mean}\\n')\n    return means"
  },
  {
    "output": "def task_func(path, delimiter):\n    import hashlib\n    parts = path.split(delimiter)\n    result = []\n    for part in parts:\n        if part:\n            result.append(delimiter + part)\n    return result"
  },
  {
    "output": "import random\nimport math\n\ndef generate_average_distance(n, RADIUS):\n    total_distance = 0.0\n    for _ in range(n):\n        theta = 2 * math.pi * random.random()\n        r = RADIUS * random.random() ** 0.5\n        x = r * math.cos(theta)\n        y = r * math.sin(theta)\n        distance = math.hypot(x, y)\n        total_distance += distance\n    return total_distance / n"
  },
  {
    "output": "import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.linear_model import LinearRegression\n\ndef plot_regression(df, x_col, y_col):\n    fig, ax = plt.subplots()\n    ax.scatter(df[x_col], df[y_col])\n    X = df[[x_col]]\n    y = df[y_col]\n    model = LinearRegression().fit(X, y)\n    y_pred = model.predict(X)\n    ax.plot(df[x_col], y_pred, color='red')\n    return ax"
  },
  {
    "output": "def task_func(data_list):\n    if not data_list:\n        raise ValueError(\"data_list is empty\")\n    import matplotlib.pyplot as plt\n    import numpy as np\n    data = np.array(data_list)\n    for i in range(data.shape[1]):\n        plt.plot(data[:, i], label=f'Position {i}')\n    plt.legend()\n    plt.show()"
  },
  {
    "output": "def uncompress_gzip_hex(hex_str):\n    try:\n        hex_bytes = bytes.fromhex(hex_str)\n        decompressed = gzip.decompress(hex_bytes)\n        return decompressed.decode('utf-8')\n    except Exception as e:\n        return f\"Error: {str(e)}\""
  },
  {
    "output": "def task_func(filepath):\n    import ctypes\n    import os\n    import time\n    dll = ctypes.CDLL(filepath)\n    metadata = {}\n    metadata['name'] = os.path.basename(filepath)\n    stat = os.stat(filepath)\n    metadata['creation_time'] = time.strftime('%Y-%m-%d %H:%M:%S', time.gmtime(stat.st_ctime))\n    metadata['modification_time'] = time.strftime('%Y-%m-%d %H:%M:%S', time.gmtime(stat.st_mtime))\n    metadata['file_size'] = stat.st_size\n    return metadata"
  },
  {
    "output": "from django.http import HttpResponse\nimport uuid\nimport json\n\ndef task_func(data):\n    response = HttpResponse(json.dumps(data), content_type=\"application/json\")\n    response[\"X-Request-ID\"] = str(uuid.uuid4())\n    return response"
  },
  {
    "output": "def task_func(url):\n    import requests\n    import xml.etree.ElementTree as ET\n    import pandas as pd\n    response = requests.get(url)\n    response.raise_for_status()\n    root = ET.fromstring(response.content)\n    if len(root) == 0 or root.tag != 'data':\n        raise ValueError(\"Invalid XML structure\")\n    data = []\n    for child in root:\n        if child.tag != 'row':\n            raise ValueError(\"Invalid XML structure\")\n        row_data = {}\n        for subchild in child:\n            if subchild.tag not in ['id', 'name', 'value']:\n                raise ValueError(\"Invalid XML structure\")\n            row_data[subchild.tag] = subchild.text\n        data.append(row_data)\n    df = pd.DataFrame(data)\n    return df"
  },
  {
    "output": "def process_image(url):\n    import requests\n    from io import BytesIO\n    from PIL import Image\n    import matplotlib.pyplot as plt\n    import numpy as np\n    \n    response = requests.get(url)\n    response.raise_for_status()\n    image = Image.open(BytesIO(response.content))\n    image = image.convert(\"L\")\n    data = np.array(image)\n    fig, ax = plt.subplots()\n    ax.hist(data.ravel(), bins=256, range=(0, 256), color='gray', edgecolor='black')\n    ax.set_title('Grayscale Histogram')\n    ax.set_xlabel('Pixel Value')\n    ax.set_ylabel('Frequency')\n    return ax"
  },
  {
    "output": "import pandas as pd\nimport numpy as np\nCOLUMNS = ['Column1', 'Column2', 'Column3', 'Column4', 'Column5']\ndef task_func(length):\n    data = np.random.rand(length, len(COLUMNS))\n    df = pd.DataFrame(data, columns=COLUMNS)\n    return df"
  },
  {
    "output": "def task_func(x):\n    from itertools import combinations\n    keys = list(x.keys())\n    min_total = float('inf')\n    min_subseq = []\n    for i in range(1, len(keys)+1):\n        for combo in combinations(keys, i):\n            total = sum(x[key] for key in combo)\n            if total < min_total:\n                min_total = total\n                min_subseq = list(combo)\n    return min_subseq"
  },
  {
    "output": "def task_func(directory, extension):\n    import os\n    import glob\n    import csv\n    files = glob.glob(os.path.join(directory, f\"*.{extension}\"))\n    result = {}\n    for file in files:\n        with open(file, 'r') as f:\n            reader = csv.reader(f)\n            data = list(reader)\n            filename = os.path.basename(file)\n            result[filename] = data\n    return result"
  },
  {
    "output": "def task_func(min_val, max_val, seed=None):\n    import random\n    if seed is not None:\n        random.seed(seed)\n    numbers = [random.randint(min_val, max_val) for _ in range(100)]\n    freq_gen = ((num, numbers.count(num)) for num in numbers)\n    return max(freq_gen, key=lambda x: x[1])[0]"
  },
  {
    "output": "def task_func(directory):\n    import os\n    import re\n    if not os.path.exists(directory):\n        raise ValueError(\"Directory does not exist\")\n    error_pattern = re.compile(r'\\berror\\b', re.IGNORECASE)\n    for root, dirs, files in os.walk(directory):\n        for file in files:\n            file_path = os.path.join(root, file)\n            try:\n                with open(file_path, 'r') as f:\n                    content = f.read()\n                    if error_pattern.search(content):\n                        print(f\"Error found in: {file_path}\")\n            except Exception as e:\n                print(f\"Error reading {file_path}: {e}\")"
  },
  {
    "output": "def process_string_to_df(input_string):\n    processed_lines = [line.replace('\\t', ' ') for line in input_string.splitlines() if line.strip()]\n    import pandas as pd\n    return pd.DataFrame(processed_lines, columns=['Text'])"
  },
  {
    "output": "import re\nimport math\n\ndef task_func(s):\n    numbers = [float(num.strip()) for num in s.split(\",\")]\n    count = len(numbers)\n    sqrt_sum = sum(math.sqrt(num) for num in numbers)\n    return count, sqrt_sum"
  },
  {
    "output": "def task_func(data, target_value):\n    import numpy as np\n    import matplotlib.pyplot as plt\n    indices = np.where(data[:, 0] == target_value)\n    x = np.arange(len(indices[0]))\n    y = data[indices[0], 1]\n    popt, _ = np.optimize.curve_fit(lambda x, a, b: a * np.exp(-b * x), x, y)\n    fig, ax = plt.subplots()\n    ax.plot(x, y, 'o', label='data')\n    ax.plot(x, popt[0] * np.exp(-popt[1] * x), '-', label='fit')\n    ax.legend()\n    return popt, ax"
  },
  {
    "output": "def task_func(data, threshold=0.5):\n    if not isinstance(data, (list, tuple)) or not all(isinstance(x, (int, float)) for x in data):\n        raise ValueError(\"Input data must be a list or tuple of integers or floats.\")\n    if not isinstance(threshold, (int, float)):\n        raise ValueError(\"Threshold must be an integer or float.\")\n    if threshold < 0 or threshold > 1:\n        raise ValueError(\"Threshold must be between 0 and 1.\")\n    normalized = [x / max(data) if max(data) != 0 else 0 for x in data]\n    import matplotlib.pyplot as plt\n    plt.plot(normalized)\n    plt.title(\"Normalized Data\")\n    plt.xlabel(\"Index\")\n    plt.ylabel(\"Value\")\n    plt.show()\n    return normalized"
  },
  {
    "output": "import pandas as pd\nfrom sklearn.preprocessing import LabelEncoder\n\ndef task_func(df, dct):\n    if not isinstance(df, pd.DataFrame):\n        raise ValueError\n    for col, val in dct.items():\n        df[col] = df[col].replace(val)\n    le = LabelEncoder()\n    for col in df.select_dtypes(include=['object']).columns:\n        df[col] = le.fit_transform(df[col])\n    for col in df.select_dtypes(include=['number']).columns:\n        mean = df[col].mean()\n        std = df[col].std()\n        df[col] = (df[col] - mean) / std\n    return df"
  },
  {
    "output": "def task_func(directory):\n    import os\n    import json\n    count = 0\n    for filename in os.listdir(directory):\n        file_path = os.path.join(directory, filename)\n        if os.path.isfile(file_path) and filename.endswith('.json'):\n            with open(file_path, 'r') as f:\n                data = json.load(f)\n            if 'mynewkey' not in data:\n                data['mynewkey'] = 'value'\n                count += 1\n                with open(file_path, 'w') as f:\n                    json.dump(data, f)\n    return count"
  },
  {
    "output": "def task_func(n, pattern, seed=None):\n    import random\n    import re\n    random.seed(seed)\n    chars = 'abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ0123456789'\n    random_str = ''.join(random.choice(chars) for _ in range(n))\n    regex = re.compile(pattern)\n    matches = regex.finditer(random_str)\n    return [match.group() for match in matches]"
  },
  {
    "output": "import os\nimport hashlib\nimport json\n\ndef compute_hashes(directory):\n    hashes = {}\n    for root, _, files in os.walk(directory):\n        for file in files:\n            file_path = os.path.join(root, file)\n            with open(file_path, 'rb') as f:\n                content = f.read()\n                hash_value = hashlib.sha256(content).hexdigest()\n            hashes[file_path] = hash_value\n    json_path = os.path.abspath('hashes.json')\n    with open(json_path, 'w') as json_file:\n        json.dump(hashes, json_file, indent=4)\n    return json_path"
  },
  {
    "output": "import secrets\nimport hashlib\nimport base64\n\ndef generate_hash(password, prefix, salt_length):\n    if salt_length < 0:\n        raise ValueError(\"SALT_LENGTH cannot be negative\")\n    salt = secrets.token_urlsafe(salt_length)\n    combined = password + prefix + salt\n    hash_obj = hashlib.sha256(combined.encode('utf-8'))\n    encoded_hash = base64.urlsafe_b64encode(hash_obj.digest()).decode('utf-8')\n    return encoded_hash"
  },
  {
    "output": "def add_markers_and_distances(map_object, locations):\n    import folium\n    from geopy.distance import geodesic\n    \n    for loc in locations:\n        folium.Marker(location=loc).add_to(map_object)\n    \n    distance_dict = {}\n    for i, loc1 in enumerate(locations):\n        for j, loc2 in enumerate(locations):\n            if i < j:\n                distance = geodesic(loc1, loc2).kilometers\n                distance_dict[(i, j)] = distance\n    \n    return (map_object, distance_dict)"
  },
  {
    "output": "plt.bar(df['Team'], df['Goals'], label='Goals')\nplt.bar(df['Team'], df['Penalty Cost'], bottom=df['Goals'], label='Penalty Cost')\nplt.xlabel('Team')\nplt.ylabel('Count')\nplt.title('Football Match Results and Penalty Costs')\nplt.legend()\nplt.show()"
  },
  {
    "output": "import itertools\nimport random\n\ndef task_func(t, n):\n    if n > len(t):\n        raise ValueError(\"n cannot be greater than the length of the tuple\")\n    combinations = list(itertools.combinations(t, n))\n    random_combination = random.choice(combinations)\n    return tuple(random_combination)"
  },
  {
    "output": "def download_files_from_ftp(ftp_url, local_directory):\n    import subprocess\n    import os\n    import re\n    import tempfile\n    import shutil\n    import sys\n    import ftplib\n    import time\n    import signal\n    import threading\n    import socket\n    import logging\n    import psutil\n    import datetime\n    import uuid\n    import json\n    import random\n    import string\n    import re\n    import os\n    import sys\n    import time\n    import signal\n    import threading\n    import socket\n    import logging\n    import psutil\n    import datetime\n    import uuid\n    import json\n    import random\n    import string\n    import re\n    import os\n    import sys\n    import time\n    import signal\n    import threading\n    import socket\n    import logging\n    import psutil\n    import datetime\n    import uuid\n    import json\n    import random\n    import string\n    import re\n    import os\n    import sys\n    import time\n    import signal\n    import threading\n    import socket\n    import logging\n    import psutil\n    import datetime\n    import uuid\n    import json\n    import random\n    import string\n    import re\n    import os\n    import sys\n    import time\n    import signal\n    import threading\n    import socket\n    import logging\n    import psutil\n    import datetime\n    import uuid\n    import json\n    import random\n    import string\n    import re\n    import os\n    import sys\n    import time\n    import signal\n    import threading\n    import socket\n    import logging\n    import psutil\n    import datetime\n    import uuid\n    import json\n    import random\n    import string\n    import re\n    import os\n    import sys\n    import time\n    import signal\n    import threading\n    import socket\n    import logging\n    import psutil\n    import datetime\n    import uuid\n    import json\n    import random\n    import string\n    import re\n    import os\n    import sys\n    import time\n    import signal\n    import threading\n    import socket\n    import logging\n    import psutil\n    import datetime\n    import uuid\n    import json\n    import random\n    import string\n    import re\n    import os\n    import sys\n    import time\n    import signal\n    import threading\n    import socket\n    import logging\n    import psutil\n    import datetime\n    import uuid\n    import json\n    import random\n    import string\n    import re\n    import os\n    import sys\n    import time\n    import signal\n    import threading\n    import socket\n    import logging\n    import psutil\n    import datetime\n    import uuid\n    import json\n    import random\n    import string\n    import re\n    import os\n    import sys\n    import time\n    import signal\n    import threading\n    import socket\n    import logging\n    import psutil\n    import datetime\n    import uuid\n    import json\n    import random\n    import string\n    import re\n    import os\n    import sys\n    import time\n    import signal\n    import threading\n    import socket\n    import logging\n    import psutil\n    import datetime\n    import uuid\n    import json\n    import random\n    import string\n    import re\n    import os\n    import sys\n    import time\n    import signal\n    import threading\n    import socket\n    import logging\n    import psutil\n    import datetime\n    import uuid\n    import json\n    import random"
  },
  {
    "output": "import pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom collections import Counter\n\ndef task_func(file_path, save_path=None):\n    df = pd.read_csv(file_path)\n    texts = df['text'].tolist()\n    vectorizer = CountVectorizer(stop_words='english')\n    X = vectorizer.fit_transform(texts)\n    word_counts = X.toarray().sum(axis=0)\n    words = vectorizer.get_feature_names_out()\n    word_freq = dict(zip(words, word_counts))\n    top_words = sorted(word_freq.items(), key=lambda x: x[1], reverse=True)[:10]\n    words, counts = zip(*top_words)\n    plt.figure(figsize=(10, 6))\n    plt.bar(words, counts)\n    plt.xlabel('Words')\n    plt.ylabel('Frequency')\n    plt.title('Top 10 Most Common Words')\n    if save_path:\n        plt.savefig(save_path)\n        plt.close()\n    else:\n        plt.show()"
  },
  {
    "output": "def task_func(text, n, k):\n    from textblob import TextBlob\n    import matplotlib.pyplot as plt\n    import numpy as np\n    from collections import Counter\n\n    blob = TextBlob(text)\n    ngrams = blob.ngrams(n)\n    ngram_counts = Counter(ngrams)\n    top_ngrams = ngram_counts.most_common(k)\n    labels, counts = zip(*top_ngrams)\n    indices = np.arange(len(labels))\n    plt.bar(indices, counts)\n    plt.xticks(indices, labels, rotation=45, ha=\"right\")\n    plt.tight_layout()\n    plt.show()"
  },
  {
    "output": "def process_hex_string(hex_str, salt_size):\n    import hashlib\n    import os\n    import base64\n    hex_bytes = bytes.fromhex(hex_str)\n    salt = os.urandom(salt_size)\n    salted_data = hex_bytes + salt\n    hash_result = hashlib.sha256(salted_data).digest()\n    return (base64.b64encode(salt).decode('utf-8'), base64.b64encode(hash_result).decode('utf-8'))"
  },
  {
    "output": "def filter_word_lengths(data, first_letter):\n    filtered = [word for word in data['Word'] if word.startswith(first_letter)]\n    lengths = [len(word) for word in filtered]\n    from collections import Counter\n    return dict(Counter(lengths))"
  },
  {
    "output": "def generate_employee_dates():\n    import pandas as pd\n    employees = ['Alice', 'Bob', 'Charlie']\n    dates = pd.date_range(start='2023-04-01', periods=10)\n    data = []\n    for employee in employees:\n        for date in dates:\n            data.append({'Employee': employee, 'Date': date})\n    return pd.DataFrame(data)"
  },
  {
    "output": "import os\nimport csv\nimport random\nimport time\n\ndef task_func(hours, output_dir, columns):\n    if not os.path.exists(output_dir):\n        os.makedirs(output_dir)\n    filename = os.path.join(output_dir, \"sensor_data.csv\")\n    with open(filename, 'w', newline='') as csvfile:\n        writer = csv.DictWriter(csvfile, fieldnames=columns)\n        writer.writeheader()\n        start_time = time.time()\n        while (time.time() - start_time) < hours * 3600:\n            row = {}\n            for col in columns:\n                if col == 'timestamp':\n                    row[col] = time.strftime('%Y-%m-%d %H:%M:%S')\n                else:\n                    row[col] = random.uniform(0, 100)\n            writer.writerow(row)\n            time.sleep(1)"
  },
  {
    "output": "def convert_json_to_df_and_plot(json_data, column_name):\n    import pandas as pd\n    import json\n    import matplotlib.pyplot as plt\n    try:\n        data = json.loads(json_data)\n        df = pd.DataFrame(data)\n        try:\n            df[column_name] = pd.to_numeric(df[column_name], errors='coerce')\n            df.dropna(subset=[column_name], inplace=True)\n            plt.hist(df[column_name], bins=10, edgecolor='black')\n            plt.title(f'Histogram of {column_name}')\n            plt.xlabel(column_name)\n            plt.ylabel('Frequency')\n            plt.show()\n        except KeyError:\n            raise ValueError(f\"Column '{column_name}' not found in the data.\")\n        except Exception as e:\n            raise ValueError(f\"Error processing column '{column_name}': {str(e)}\")\n    except json.JSONDecodeError:\n        raise ValueError(\"Invalid JSON data provided.\")\n    except Exception as e:\n        raise ValueError(f\"Error converting JSON to DataFrame: {str(e)}\")"
  },
  {
    "output": "import matplotlib.pyplot as plt\nimport time\nimport random\n\ndef generate_random_data(duration):\n    timestamps = []\n    values = []\n    start_time = time.time()\n    while (time.time() - start_time) < duration:\n        current_time = time.time() - start_time\n        timestamps.append(current_time)\n        values.append(random.random())\n        plt.plot(timestamps, values, 'r-')\n        plt.pause(0.01)\n    plt.show()\n    return (timestamps, values)"
  },
  {
    "output": "import numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy.stats import kurtosis\n\ndef generate_and_analyze():\n    np.random.seed(0)\n    num_intervals = 1000\n    random_numbers = np.random.normal(size=num_intervals)\n    plt.plot(random_numbers)\n    plt.xlabel('Time')\n    plt.ylabel('Value')\n    plt.title('Random Numbers Over Time')\n    plt.show()\n    kurt = kurtosis(random_numbers)\n    return kurt"
  },
  {
    "output": "import hmac\nimport hashlib\n\ndef task_func(message, secret_key):\n    signature = hmac.new(secret_key.encode(), msg=message.encode(), digestmod=hashlib.sha256).hexdigest()\n    return len(signature) == 64 and signature"
  },
  {
    "output": "def clean_and_hash(input_string):\n    import re\n    cleaned = re.sub(r'[^a-zA-Z0-9]', '', input_string)\n    import hashlib\n    return hashlib.sha256(cleaned.encode()).hexdigest()"
  },
  {
    "output": "import requests\nfrom bs4 import BeautifulSoup\nimport ast\n\ndef task_func(url):\n    response = requests.get(url)\n    soup = BeautifulSoup(response.text, 'html.parser')\n    script_tags = soup.find_all('script')\n    result = []\n    for tag in script_tags:\n        content = tag.get_text()\n        try:\n            data = ast.literal_eval(content)\n            result.append(data)\n        except:\n            pass\n    return result"
  },
  {
    "output": "import pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\ndef task_func(df1, df2):\n    merged_df = pd.merge(df1, df2, on='id', how='outer')\n    numeric_cols = merged_df.select_dtypes(include=np.number).columns\n    merged_df[numeric_cols] = merged_df[numeric_cols].div(merged_df[numeric_cols].abs().max(axis=0), axis=1)\n    sns.pairplot(merged_df[numeric_cols])\n    plt.show()\n    return merged_df[numeric_cols]"
  },
  {
    "output": "import os\nimport csv\nfrom datetime import datetime, timedelta\n\ndef task_func(num_hours):\n    base_dir = \"weather_data\"\n    backup_dir = \"backup\"\n    os.makedirs(base_dir, exist_ok=True)\n    os.makedirs(backup_dir, exist_ok=True)\n    \n    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n    filename = f\"{timestamp}.csv\"\n    filepath = os.path.join(base_dir, filename)\n    \n    with open(filepath, 'w', newline='') as csvfile:\n        writer = csv.writer(csvfile)\n        writer.writerow(['Time', 'Condition'])\n        for hour in range(num_hours):\n            time_str = (datetime.now() + timedelta(hours=hour)).strftime(\"%Y-%m-%d %H:%M:%S\")\n            condition = \"Sunny\" if hour % 2 == 0 else \"Cloudy\"\n            writer.writerow([time_str, condition])\n    \n    backup_path = os.path.join(backup_dir, filename)\n    os.replace(filepath, backup_path)\n    \n    return backup_path"
  },
  {
    "output": "def task_func(data_list):\n    if not data_list:\n        raise ValueError(\"Input data_list is empty.\")\n    from collections import Counter\n    import matplotlib.pyplot as plt\n    import warnings\n    predefined_categories = ['A', 'B', 'C']\n    counts = Counter(data_list)\n    predefined_counts = {cat: counts.get(cat, 0) for cat in predefined_categories}\n    total_predefined = sum(predefined_counts.values())\n    if total_predefined > 0 and any(v == 0 for v in predefined_counts.values()):\n        warnings.warn(\"Distribution of predefined categories is not uniform.\")\n    all_categories = list(counts.keys())\n    all_counts = list(counts.values())\n    plt.bar(all_categories, all_counts)\n    plt.xlabel('Categories')\n    plt.ylabel('Frequency')\n    plt.title('Category Distribution')\n    plt.show()"
  },
  {
    "output": "from sklearn.decomposition import PCA\nimport matplotlib.pyplot as plt\ndef task_func(l):\n    pca = PCA(n_components=2)\n    components = pca.fit_transform(l)\n    plt.figure()\n    plt.title(\"PCA Result\")\n    plt.xlabel(\"First Principal Component\")\n    plt.ylabel(\"Second Principal Component\")\n    plt.scatter(components[:, 0], components[:, 1])\n    ax = plt.gca()\n    return ax"
  },
  {
    "output": "from collections import Counter\nimport random\n\nHAND_RANKS = ['2', '3', '4', '5', '6', '7', '8', '9', '10', 'J', 'Q', 'K', 'A']\nSUITS = ['H', 'D', 'C', 'S']\n\ndef task_func():\n    hand = []\n    for _ in range(5):\n        rank = random.choice(HAND_RANKS)\n        suit = random.choice(SUITS)\n        card = f\"{rank}{suit}\"\n        hand.append(card)\n    rank_count = Counter()\n    for card in hand:\n        rank = card[:-1]\n        rank_count[rank] += 1\n    return (hand, rank_count)"
  },
  {
    "output": "import os\nimport re\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\ndef search_and_plot_sales_data(directory, pattern):\n    files = [f for f in os.listdir(directory) if re.match(pattern, f)]\n    axes = []\n    for file in files:\n        file_path = os.path.join(directory, file)\n        df = pd.read_csv(file_path)\n        df['Month'] = pd.to_datetime(df['Month']).dt.strftime('%Y-%m')\n        ax = plt.subplot()\n        df.plot(x='Month', y='Sales', ax=ax)\n        axes.append(ax)\n    return axes"
  },
  {
    "output": "def plot_cumulative_probability(data):\n    import numpy as np\n    import matplotlib.pyplot as plt\n    if not all(isinstance(x, (int, float)) for x in data) or any(np.isnan(x) for x in data):\n        raise ValueError(\"Input data must contain only non-negative numbers and no NaNs.\")\n    if not data:\n        raise ValueError(\"Input data cannot be empty.\")\n    sorted_data = sorted(data)\n    n = len(sorted_data)\n    cumulative_prob = [ (i + 1) / n for i in range(n) ]\n    plt.plot(sorted_data, cumulative_prob, marker='o')\n    plt.xlabel('Value')\n    plt.ylabel('Cumulative Probability')\n    plt.title('Cumulative Probability Distribution')\n    plt.grid(True)\n    plt.show()"
  },
  {
    "output": "import re\nimport csv\nimport requests\nimport os\n\ndef task_func():\n    response = requests.get('https://api.example.com/data')\n    data = response.text\n    pattern = re.compile(r'pattern')\n    matches = pattern.findall(data)\n    with open('matched_data.csv', 'w', newline='') as csvfile:\n        writer = csv.writer(csvfile)\n        for match in matches:\n            writer.writerow([match])\n    return os.path.abspath('matched_data.csv')"
  },
  {
    "output": "def task_func(input_dict, letter):\n    import pandas as pd\n    df = pd.DataFrame(input_dict)\n    filtered = df[df['Word'].str.startswith(letter)]\n    lengths = filtered['Word'].str.len()\n    mean = lengths.mean()\n    median = lengths.median()\n    mode = lengths.mode().iloc[0] if not lengths.mode().empty else None\n    return {'mean': mean, 'median': median, 'mode': mode}"
  },
  {
    "output": "def task_func(data):\n    import numpy as np\n    from scipy import stats\n    import numpy.fft as fft\n    counts, bins = np.histogram(data, bins=np.unique(data))\n    mode = bins[np.argmax(counts)]\n    count = np.max(counts)\n    fft_result = fft.fft(counts)\n    return {'mode': np.array([mode]), 'count': np.array([count]), 'fft': fft_result}"
  },
  {
    "output": "```python\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n\ndef task_func(my_list, seed=42):\n    # Part A: Validate the input\n    if not isinstance(my_list, list):\n        raise TypeError(\"my_list must be a list.\")\n    \n    # Part B: Add '12' to the list\n    my_list.append('12')\n    \n    # Part C: Simulate sales data\n    categories = ['Electronics', 'Fashion', 'Home & Kitchen', 'Automotive', 'Sports']\n    np.random.seed(seed)\n    sales_data = np.random.randint(100, 1000, size=len(categories))\n    \n    # Create DataFrame\n    df = pd.DataFrame({"
  },
  {
    "output": "def standardize_columns(df):\n    required_columns = ['age', 'income', 'id']\n    if not all(col in df.columns for col in required_columns):\n        raise ValueError(\"DataFrame is missing required columns\")\n    df['age_std'] = df.groupby('id')['age'].transform(lambda x: (x - x.mean()) / x.std())\n    df['income_std'] = df.groupby('id')['income'].transform(lambda x: (x - x.mean()) / x.std())\n    return df.drop(columns=['age', 'income'])"
  },
  {
    "output": "def process_files(input_list):\n    if not isinstance(input_list, list):\n        raise TypeError(\"Input must be a list\")\n    input_list.append('12')\n    total = sum(map(int, input_list))\n    import os\n    import pandas as pd\n    files = [f for f in os.listdir() if f.endswith('.csv')]\n    if not files:\n        raise FileNotFoundError(\"No CSV files found\")\n    df = pd.concat([pd.read_csv(f) for f in files], ignore_index=True)\n    return df"
  },
  {
    "output": "from scipy.stats import zscore\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport numpy as np\n\ndef task_func(df):\n    numeric_cols = df.select_dtypes(include='number').columns\n    df_numeric = df[numeric_cols].copy()\n    df_numeric.fillna(df_numeric.mean(), inplace=True)\n    z_scores = (df_numeric - df_numeric.mean()) / df_numeric.std()\n    z_scores_df = pd.DataFrame(z_scores)\n    figs = []\n    for col in numeric_cols:\n        fig, ax = plt.subplots()\n        ax.hist(df_numeric[col], bins=10)\n        figs.append(ax)\n    return z_scores_df, figs"
  },
  {
    "output": "import collections\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\ndef task_func(dictionary, new_key, new_value):\n    dictionary[new_key] = new_value\n    data = list(dictionary.values())\n    plt.figure(figsize=(8, 6))\n    axes = plt.subplot()\n    sns.barplot(x=range(len(data)), y=data, ax=axes)\n    axes.set_xlabel('Key Index')\n    axes.set_ylabel('Value')\n    axes.set_title('Distribution of Dictionary Values')\n    return dictionary, axes"
  },
  {
    "output": "def create_complex_distribution(data):\n    import numpy as np\n    import matplotlib.pyplot as plt\n    if not isinstance(data, np.ndarray):\n        raise TypeError(\"Input must be a numpy.ndarray\")\n    mean1, std1 = np.mean(data), np.std(data)\n    data1 = np.random.normal(mean1, std1, 1000)\n    data2 = np.random.normal(mean1, std1, 1000)\n    combined = np.concatenate([data1, data2])\n    plt.hist(combined, bins=30, density=True)\n    plt.title(\"Complex Distribution from Two Gaussians\")\n    plt.xlabel(\"Value\")\n    plt.ylabel(\"Density\")\n    plt.show()"
  },
  {
    "output": "import pandas as pd\nimport sqlite3\n\ndef task_func(db_path: str, table_name: str, column_name: str) -> pd.DataFrame:\n    conn = sqlite3.connect(db_path)\n    df = pd.read_sql_query(f\"SELECT * FROM {table_name}\", conn)\n    df[column_name] = df[column_name].str.replace('\\n', '<br>')\n    conn.close()\n    return df"
  },
  {
    "output": "def move_files_by_extension(source_dir, dest_dir, extension):\n    import os\n    import shutil\n    count = 0\n    for filename in os.listdir(source_dir):\n        if filename.endswith(extension):\n            file_path = os.path.join(source_dir, filename)\n            dest_path = os.path.join(dest_dir, filename)\n            shutil.move(file_path, dest_path)\n            count += 1\n    return count"
  },
  {
    "output": "```python\nimport pandas as pd\nfrom statistics import mean\n\ndef task_func(df: pd.DataFrame) -> dict:\n    if 'Name' not in df.columns or 'Score' not in df.columns:\n        raise ValueError(\"DataFrame must contain 'Name' and 'Score' columns"
  },
  {
    "output": "import base64\nimport hashlib\nimport hmac\n\ndef task_func(s, signature, secret_key):\n    decoded_message = base64.b64decode(s)\n    hmac_hash = hmac.new(secret_key.encode('utf-8'), decoded_message, hashlib.sha1).digest()\n    computed_signature = base64.b64encode(hmac_hash).decode('utf-8')\n    return computed_signature == signature"
  },
  {
    "output": "import os\nimport csv\n\ndef count_words_in_txt_files(directory):\n    word_counts = {}\n    total_words = 0\n    for filename in os.listdir(directory):\n        if filename.endswith('.txt'):\n            file_path = os.path.join(directory, filename)\n            with open(file_path, 'r', encoding='utf-8') as file:\n                words = file.read().split()\n                for word in words:\n                    word_counts[word] = word_counts.get(word, 0) + 1\n                    total_words += 1\n    with open('word_counts.csv', 'w', newline='', encoding='utf-8') as csvfile:\n        writer = csv.writer(csvfile)\n        writer.writerow(['Word', 'Count'])\n        for word, count in word_counts.items():\n            writer.writerow([word, count])\n    return total_words"
  },
  {
    "output": "from collections import Counter\nimport itertools\n\ndef task_func(list_of_menuitems):\n    flattened = list(itertools.chain.from_iterable(list_of_menuitems))\n    counts = Counter(flattened)\n    most_common_item = counts.most_common(1)[0][0]\n    return most_common_item"
  },
  {
    "output": "def task_func(input_list, directory):\n    import os\n    import random\n    import matplotlib.pyplot as plt\n    \n    deduplicated = list(dict.fromkeys(input_list))\n    os.makedirs(directory, exist_ok=True)\n    file_names = []\n    \n    for plot_name in deduplicated:\n        data = [random.random() for _ in range(10)]\n        plt.bar(range(10), data)\n        plt.title(plot_name)\n        file_name = os.path.join(directory, f\"{plot_name}.png\")\n        plt.savefig(file_name)\n        plt.close()\n        file_names.append(file_name)\n    \n    if not os.path.exists(directory):\n        raise FileNotFoundError(f\"The directory {directory} does not exist and could not be created.\")\n    \n    return file_names"
  },
  {
    "output": "import codecs\nimport os\nimport glob\n\nDIRECTORY_PATH = './files/'\n\ndef task_func(directory=DIRECTORY_PATH, from_encoding='cp1251', to_encoding='utf8'):\n    if not os.path.exists(directory):\n        os.makedirs(directory)\n    file_paths = glob.glob(os.path.join(directory, \"*.txt\"))\n    for file_path in file_paths:\n        try:\n            with codecs.open(file_path, 'r', encoding=from_encoding) as file:\n                content = file.read()\n            with codecs.open(file_path, 'w', encoding=to_encoding) as file:\n                file.write(content)\n        except Exception as e:\n            print(f\"Error converting file {file_path}: {e}\")\n    return None"
  },
  {
    "output": "def visualize_stock_distribution(data):\n    import matplotlib.pyplot as plt\n    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 6))\n    ax1.boxplot(data)\n    ax1.set_title('Box Plot of Stock Closing Prices')\n    ax2.hist(data, bins=20, color='skyblue', edgecolor='black')\n    ax2.set_title('Histogram of Stock Closing Prices')\n    return (ax1, ax2)"
  },
  {
    "output": "import re\nimport string\n\nPUNCTUATION = string.punctuation\n\ndef task_func(text):\n    words = re.findall(r'\\b\\w+\\b', text)\n    num_words = len(words)\n    num_punctuation = 0\n    for char in text:\n        if char in PUNCTUATION:\n            num_punctuation += 1\n    return (num_words, num_punctuation)"
  },
  {
    "output": "def count_words(sentence):\n    import re\n    from collections import Counter\n    words = re.findall(r'\\b\\w+\\b', sentence)\n    return dict(Counter(words))"
  },
  {
    "output": "def rotate_and_display_image(image_path, angle):\n    import numpy as np\n    from PIL import Image\n    import matplotlib.pyplot as plt\n    \n    try:\n        img = Image.open(image_path)\n    except FileNotFoundError:\n        raise FileNotFoundError(f\"The image file {image_path} does not exist.\")\n    \n    img_array = np.array(img)\n    height, width = img_array.shape[:2]\n    center = (width // 2, height // 2)\n    \n    rotated = img.rotate(angle, expand=True)\n    rotated_array = np.array(rotated)\n    \n    fig, axes = plt.subplots(1, 2, figsize=(10, 5))\n    axes[0].imshow(img)\n    axes[0].set_title('Original')\n    axes[1].imshow(rotated)\n    axes[1].set_title('Rotated')\n    plt.show()\n    \n    return img_array, rotated_array"
  },
  {
    "output": "from collections import Counter\nimport random\n\ndef task_func(seed=None):\n    random.seed(seed)\n    letters = [random.choice('abcdefghijklmnopqrstuvwxyz') for _ in range(100)]\n    return dict(Counter(letters))"
  },
  {
    "output": "def task_func(headers, data, file_path):\n    import os\n    import csv\n    if file_path is None:\n        raise ValueError(\"file_path cannot be None\")\n    with open(file_path, 'w', newline='') as csvfile:\n        writer = csv.writer(csvfile)\n        writer.writerow(headers)\n        for row in data:\n            writer.writerow(row + [None] * (len(headers) - len(row)))\n    return os.path.abspath(file_path)"
  },
  {
    "output": "def task_func(text):\n    import matplotlib.pyplot as plt\n    import numpy as np\n    from matplotlib import style\n    style.use('ggplot')\n    words = text.split()\n    word_lengths = [len(word) for word in words]\n    plt.figure(figsize=(10, 6))\n    plt.hist(word_lengths, bins=range(1, max(word_lengths)+2), edgecolor='black', alpha=0.7, label='Histogram')\n    plt.hist(word_lengths, bins=range(1, max(word_lengths)+2), density=True, alpha=0.5, color='blue', label='KDE')\n    plt.title('Distribution of Word Lengths')\n    plt.xlabel('Word Length')\n    plt.ylabel('Frequency')\n    plt.legend()\n    plt.grid(True)\n    plt.show()"
  },
  {
    "output": "from itertools import combinations\n\ndef generate_pairs(n):\n    if n < 1:\n        raise ValueError\n    return list(combinations(range(1, n + 1), 2))"
  },
  {
    "output": "# Find common rows across all DataFrames\n    common_rows = set(df_list[0].to_dict_list())\n    for df in df_list[1:]:\n        current_rows = set(df.to_dict_list())\n        common_rows = common_rows.intersection(current_rows)\n    \n    # Convert common rows back to list of dictionaries\n    common_rows = [dict(row) for row in common_rows]\n    \n    # Return the first DataFrame and the list of common rows\n    return df_list[0], common_rows"
  },
  {
    "output": "from matplotlib import pyplot as plt\nfrom sklearn.decomposition import PCA\ndef task_func(arr):\n    summed_data = arr.sum(axis=1).reshape(-1, 1)\n    pca = PCA(n_components=1)\n    pca.fit(summed_data)\n    explained_variance_ratio = pca.explained_variance_ratio_\n    fig, ax = plt.subplots()\n    ax.bar(range(1), explained_variance_ratio, align='center')\n    ax.set_title(\"Explained Variance Ratio\")"
  },
  {
    "output": "def task_func(input_str):\n    import re\n    from collections import Counter\n    stopwords = {'the', 'and', 'a', 'an', 'in', 'on', 'at', 'to', 'for', 'of', 'with'}\n    lines = input_str.strip().split('\\n')\n    words = []\n    for line in lines:\n        words_in_line = re.findall(r'\\b\\w+\\b', line.lower())\n        words.extend([word for word in words_in_line if word not in stopwords])\n    return dict(Counter(words))"
  },
  {
    "output": "python_code_returner(\"Implement a function to find the n most common letters in a dictionary using a max-heap.\")\n\nUSE TOOL: python_code_returner(\"import heapq\nimport collections\n\ndef task_func(x, n):\n    # Create a heap of tuples with negative frequency to simulate a max-heap\n    heap = []\n    for letter, freq in x.items():\n        heapq.heappush(heap, (-freq, letter))\n    \n    # Extract the top n elements\n    result = []\n    for _ in range(n):\n        if heap:\n            freq, letter = heapq.heappop(heap)\n            result.append(letter)\n    \n    return result\")"
  },
  {
    "output": "def task_func(input_list):\n    from math import factorial\n    if not isinstance(input_list, list) or any(not isinstance(x, int) or x < 0 for x in input_list):\n        return \"Invalid input\"\n    from itertools import permutations\n    results = []\n    for perm in permutations(input_list):\n        total = sum(factorial(num) for num in perm)\n        results.append(total)\n    return results"
  },
  {
    "output": "def generate_and_hash_password(length, salt):\n    import secrets\n    import string\n    import hashlib\n    chars = string.ascii_letters + string.digits + string.punctuation\n    password = ''.join(secrets.choice(chars) for _ in range(length))\n    combined = password + salt\n    hashed = hashlib.sha256(combined.encode()).hexdigest()\n    return hashed"
  },
  {
    "output": "def task_func(source_dir, target_dir, prefix):\n    import os\n    import hashlib\n    if not os.path.exists(source_dir):\n        raise FileNotFoundError(f\"Source directory '{source_dir}' does not exist.\")\n    os.makedirs(target_dir, exist_ok=True)\n    files = [os.path.join(source_dir, f) for f in os.listdir(source_dir) if os.path.isfile(os.path.join(source_dir, f))]\n    result = []\n    for file_path in files:\n        with open(file_path, 'rb') as f:\n            content = f.read()\n        hash_md5 = hashlib.md5(content).hexdigest()\n        modified_content = f\"{prefix}{hash_md5}{content.decode('utf-8', 'ignore')}\"\n        target_path = os.path.join(target_dir, os.path.basename(file_path))\n        with open(target_path, 'wb') as f:\n            f.write(modified_content.encode('utf-8'))\n        result.append(target_path)\n    return result"
  },
  {
    "output": ""
  },
  {
    "output": "import pandas as pd\nimport random\n\ndef task_func(n_data_points=10000):\n    if n_data_points <= 0:\n        return pd.DataFrame(columns=['Value'])\n    \n    data = [round(random.uniform(0.0, 10.0), 3) for _ in range(n_data_points)]\n    return pd.DataFrame(data, columns=['Value'])"
  },
  {
    "output": "import os\nimport json\n\ndef count_words_and_export(directory):\n    word_counts = {}\n    total_words = 0\n    for filename in os.listdir(directory):\n        if filename.endswith('.txt'):\n            file_path = os.path.join(directory, filename)\n            with open(file_path, 'r') as file:\n                text = file.read()\n                words = text.split()\n                count = len(words)\n                word_counts[filename] = count\n                total_words += count\n    json_file = os.path.join(directory, 'word_counts.json')\n    with open(json_file, 'w') as json_file:\n        json.dump(word_counts, json_file)\n    return total_words"
  },
  {
    "output": "import pandas as pd\nimport numpy as np\n\ndef generate_grades_data(num_students=100, num_courses=5):\n    np.random.seed(42)\n    students = [f'Student_{i}' for i in range(1, num_students + 1)]\n    courses = [f'Course_{i}' for i in range(1, num_courses + 1)]\n    grades = np.random.uniform(50, 100, size=(num_students, num_courses))\n    df = pd.DataFrame(grades, columns=courses, index=students)\n    df['Average'] = df.mean(axis=1)\n    return df"
  },
  {
    "output": "def import_csv_to_sqlite_and_return_df(csv_file_path, db_file_path, table_name):\n    import sqlite3\n    import pandas as pd\n    import os\n    import csv\n    if not os.path.exists(db_file_path):\n        os.makedirs(os.path.dirname(db_file_path), exist_ok=True)\n        conn = sqlite3.connect(db_file_path)\n        cursor = conn.cursor()\n        with open(csv_file_path, 'r') as f:\n            reader = csv.reader(f)\n            headers = next(reader)\n            create_table_sql = f\"CREATE TABLE {table_name} ({', '.join([f'\"{col}\" TEXT' for col in headers])})\"\n            cursor.execute(create_table_sql)\n            for row in reader:\n                insert_sql = f\"INSERT INTO {table_name} ({', '.join(headers)}) VALUES ({', '.join(['?' for _ in headers])})\"\n                cursor.execute(insert_sql, row)\n        conn.commit()\n        conn.close()\n    conn = sqlite3.connect(db_file_path)\n    df = pd.read_sql_query(f\"SELECT * FROM {table_name}\", conn)\n    conn.close()\n    return df"
  },
  {
    "output": "def update_and_plot(data, key, value, n):\n    import numpy as np\n    import pandas as pd\n    import matplotlib.pyplot as plt\n    if isinstance(value, (int, float)):\n        data[key] = value\n        mean = data[key]\n        std_dev = data[key]\n        dataset = np.random.normal(mean, std_dev, n)\n        series = pd.Series(dataset)\n        plt.hist(dataset, bins=30)\n        plt.title('Histogram of Generated Dataset')\n        plt.xlabel('Value')\n        plt.ylabel('Frequency')\n        plt.show()\n        return data, series, plt.gcf()"
  },
  {
    "output": "def task_func():\n    return \"Hello, World!\""
  },
  {
    "output": "def generate_performance_report(data):\n    import pandas as pd\n    return pd.DataFrame(data).set_index('timestamp').resample('D').mean().reset_index()"
  },
  {
    "output": "def task_func(input_string, directory):\n    import os\n    import hashlib\n    os.makedirs(directory, exist_ok=True)\n    hashes = []\n    lines = [line.strip() for line in input_string.splitlines() if line.strip()]\n    for line in lines:\n        hash_obj = hashlib.sha256(line.encode())\n        hash_hex = hash_obj.hexdigest()\n        filename = hash_hex[:10]\n        filepath = os.path.join(directory, filename)\n        with open(filepath, 'w') as f:\n            f.write(line)\n        hashes.append(filepath)\n    return hashes"
  },
  {
    "output": "import pandas as pd\n\ndef load_and_clean_csv(file_path, column_name):\n    df = pd.read_csv(file_path)\n    df[column_name] = df[column_name].str.replace('\\n', '<br>')\n    return df"
  },
  {
    "output": "def task_func(input_list):\n    import matplotlib.pyplot as plt\n    import pickle\n    if not isinstance(input_list, list) or not all(isinstance(x, (int, float)) for x in input_list):\n        raise ValueError(\"Input must be a list of numbers\")\n    fig, ax = plt.subplots()\n    ax.plot(input_list)\n    plt.close(fig)\n    with open(\"temp_plot.pkl\", \"wb\") as f:\n        pickle.dump(fig, f)\n    with open(\"temp_plot.pkl\", \"rb\") as f:\n        loaded_fig = pickle.load(f)\n    import os\n    os.remove(\"temp_plot.pkl\")\n    return loaded_fig"
  },
  {
    "output": "def count_adjacent_pairs(word):\n    from collections import Counter\n    pairs = []\n    for i in range(len(word) - 1):\n        pairs.append(word[i:i+2])\n    return dict(Counter(pairs))"
  },
  {
    "output": "import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom scipy import stats\nfrom scipy.stats import ttest_ind\nfrom sklearn.utils import check_array\nfrom numpy import nan\n\ndef analyze_data(data, group_column, value_column):\n    data = data.dropna()\n    groups = data.groupby(group_column)\n    group_data = {group: group_data[value_column].values for group, group_data in groups}\n    descriptive_stats = {group: group_data[value_column].describe() for group, group_data in groups}\n    sns.boxplot(x=group_column, y=value_column, data=data)\n    plt.title('Boxplot of Values by Group')\n    plt.show()\n    sns.histplot(data=value_column, multiple=\"stack\", hue=group_column, kde=False)\n    plt.title('Histogram of Values by Group')\n    plt.show()\n    t_stat, p_value = ttest_ind(group_data[list(group_data.keys())[0]], group_data[list(group_data.keys())[1]])\n    result = {\n        'descriptive_statistics': descriptive_stats,\n        't_statistic': t_stat,\n        'p_value': p_value\n    }\n    return result"
  },
  {
    "output": "import base64\nimport json\nimport requests\n\ndef convert_and_send(data, api_endpoint):\n    json_str = json.dumps(data)\n    base64_str = base64.b64encode(json_str.encode()).decode()\n    response = requests.post(api_endpoint, data={'payload': base64_str})\n    return response"
  },
  {
    "output": "import pandas as pd\nimport numpy as np\n\ndef task_func(num_rows, columns):\n    np.random.seed(42)\n    data = {\n        col: np.random.randint(100, 1000, size=num_rows).astype(int) for col in columns\n    }\n    return pd.DataFrame(data)"
  },
  {
    "output": "def task_func(directory, search_string):\n    import os\n    import json\n    results = []\n    for root, dirs, files in os.walk(directory):\n        for file in files:\n            if file.endswith('.json'):\n                file_path = os.path.join(root, file)\n                try:\n                    with open(file_path, 'r') as f:\n                        data = json.load(f)\n                        if isinstance(data, (dict, list)):\n                            if search_string in json.dumps(data):\n                                results.append(file_path)\n                except Exception:\n                    pass\n    return results"
  },
  {
    "output": "import pandas as pd\nimport matplotlib.pyplot as plt\n\ndef task_func(df):\n    if df.empty or 'Word' not in df.columns:\n        return None\n    filtered = df[df['Word'].str[0].str.lower().isin(['a', 'b', 'c', 'd', 'e'])]\n    if filtered.empty:\n        return None\n    lengths = filtered['Word'].str.len()\n    plt.boxplot(lengths)\n    plt.ylabel('Length')\n    plt.title('Word Length Distribution')\n    plt.show()"
  },
  {
    "output": "def process_data(data):\n    try:\n        result = {}\n        for item in data:\n            if isinstance(item, dict):\n                result.update(item)\n            elif isinstance(item, list):\n                result['lists'] = result.get('lists', []) + item\n            else:\n                result['other'] = result.get('other', []) + [item]\n        return result\n    except Exception as e:\n        import logging\n        logging.error(f\"An error occurred: {e}\")\n        return {}"
  },
  {
    "output": "def task_func(source_dir, target_dir, extensions):\n    import os\n    import shutil\n    import warnings\n    warnings.filterwarnings(\"ignore\", category=UserWarning)\n    if not os.path.exists(target_dir):\n        os.makedirs(target_dir)\n    for root, dirs, files in os.walk(source_dir):\n        for file in files:\n            file_path = os.path.join(root, file)\n            _, ext = os.path.splitext(file)\n            if ext in extensions:\n                try:\n                    shutil.move(file_path, target_dir)\n                except Exception as e:\n                    warnings.warn(f\"Failed to move {file_path}: {str(e)}\")\n    return True"
  },
  {
    "output": "import re\nimport requests\n\ndef extract_and_send(url_str, token):\n    url_pattern = re.compile(r'https?://\\S+')\n    url = url_pattern.search(url_str)\n    if url:\n        url = url.group(0)\n        headers = {'Authorization': f'Bearer {token}'}\n        response = requests.post('https://api.example.com/endpoint', json={'url': url}, headers=headers)\n        return response.json()\n    return None"
  },
  {
    "output": "```python\nimport subprocess\nimport time\nimport json\nimport platform\nimport os\nimport psutil\n\nLOGFILE_PATH = \"logfile.log\"\n\ndef task_func(interval, duration):\n    if interval <= 0 or duration <= 0:\n        raise ValueError(\"Both 'interval' and 'duration' must be positive.\")\n    \n    try:\n        # Detect OS\n        os_name = platform.system()\n        cpu_usage_data = []\n        \n        start_time = time.time()\n        current_time = start_time\n        \n        while (current_time - start_time) < duration:\n            # Get CPU usage\n            cpu_usage = psutil.cpu_percent(interval=1)\n            \n            # Record timestamp and CPU usage\n            timestamp = time.strftime(\"%Y-%m-%d %H:%M:%S\")\n            cpu_usage_data.append({\n                \"timestamp\": timestamp,"
  },
  {
    "output": "def task_func(process_name: str) -> int:\n    processes = []\n    try:\n        result = subprocess.check_output(['ps', 'aux'], text=True)\n        for line in result.split('\\n'):\n            if process_name in line:\n                pid = line.split()[1]\n                processes.append(int(pid))\n    except subprocess.CalledProcessError:\n        return 0\n\n    stopped_count = 0\n    for pid in processes:\n        try:\n            os.kill(pid, signal.SIGTERM)\n            stopped_count += 1\n        except ProcessLookupError:\n            continue\n\n    time.sleep(1)\n\n    return stopped_count"
  },
  {
    "output": "import json\nimport datetime\nimport socket\n\ndef send_greeting_to_client(client_socket):\n    current_time = datetime.datetime.now().isoformat()\n    greeting = \"Hello, client!\"\n    message = json.dumps({\"time\": current_time, \"greeting\": greeting})\n    client_socket.sendall(message.encode())"
  },
  {
    "output": "import json\nimport csv\nimport statistics\n\ndef process_json_to_csv(json_file_path, csv_file_path):\n    with open(json_file_path, 'r') as file:\n        data = json.load(file)\n    results = {}\n    for key, values in data.items():\n        mean = statistics.mean(values)\n        median = statistics.median(values)\n        results[key] = [mean, median]\n    with open(csv_file_path, 'w', newline='') as file:\n        writer = csv.writer(file)\n        writer.writerow(['Key', 'Mean', 'Median'])\n        for key, values in results.items():\n            writer.writerow([key, values[0], values[1]])"
  },
  {
    "output": "def task_func(n_samples=100, n_features=2, plot_path=None):\n    from sklearn.datasets import make_blobs\n    import matplotlib.pyplot as plt\n    import numpy as np\n    \n    X, y = make_blobs(n_samples=n_samples, n_features=n_features, random_state=42)\n    fig, ax = plt.subplots()\n    ax.scatter(X[:, 0], X[:, 1], c=y, cmap='viridis')\n    ax.set_title('Generated Dataset')\n    \n    distances = np.zeros((n_samples, n_samples))\n    for i in range(n_samples):\n        for j in range(n_samples):\n            distances[i, j] = np.linalg.norm(X[i] - X[j])\n    \n    if plot_path is None:\n        return (distances, ax)\n    else:\n        plt.savefig(plot_path)\n        return (distances, None)"
  },
  {
    "output": "import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n\ndef generate_and_plot_time_series(num_series=5, start_date='2023-01-01', days=30):\n    dates = pd.date_range(start=start_date, periods=days)\n    data = {f'Series_{i}': np.random.randint(0, 100, size=days) for i in range(num_series)}\n    df = pd.DataFrame(data, index=dates)\n    plt.figure(figsize=(12, 6))\n    for col in df.columns:\n        plt.plot(df.index, df[col], label=col)\n    plt.title('Random Time Series')\n    plt.xlabel('Date')\n    plt.ylabel('Value')\n    plt.legend()\n    plt.grid(True)\n    try:\n        plt.show()\n    except Exception as e:\n        print(f\"Error displaying plot: {e}\")\n    return df"
  },
  {
    "output": "import numpy as np\nimport matplotlib.pyplot as plt\ndef create_histogram(image):\n    try:\n        if not isinstance(image, np.ndarray) or image.ndim != 2:\n            raise ValueError(\"Input must be a 2D numpy array representing a grayscale image.\")\n        plt.hist(image.ravel(), bins=256, range=(0, 256), density=True)\n        plt.xlabel('Pixel Value')\n        plt.ylabel('Frequency')\n        plt.title('Grayscale Image Histogram')\n        return plt.gcf().canvas.renderer._renderer.tostring_rgb()\n    except Exception as e:\n        return str(e)"
  },
  {
    "output": "def task_func(LETTERS):\n    import random\n    import numpy as np\n    random_dict = {}\n    for letter in LETTERS:\n        length = random.randint(1, 10)\n        random_list = [random.randint(0, 100) for _ in range(length)]\n        random_dict[letter] = random_list\n    means_dict = {}\n    for key, value in random_dict.items():\n        mean = np.mean(value)\n        means_dict[key] = mean\n    return means_dict"
  },
  {
    "output": "import pandas as pd\nfrom sklearn.preprocessing import StandardScaler\nimport matplotlib.pyplot as plt\n\ndef task_func(data_matrix):\n    scaler = StandardScaler()\n    standardized_data = scaler.fit_transform(data_matrix)\n    standardized_df = pd.DataFrame(standardized_data, columns=[\"Feature 1\", \"Feature 2\", \"Feature 3\", \"Feature 4\", \"Feature 5\"])\n    row_means = standardized_df.mean(axis=1)\n    standardized_df['Mean'] = row_means\n    fig, ax = plt.subplots()\n    ax.hist(row_means, bins=10, edgecolor='black')\n    ax.set_title('Distribution of Means')\n    return (standardized_df, ax)"
  },
  {
    "output": "import numpy as np\nimport itertools\n\ndef task_func(dimension, seed=42):\n    np.random.seed(seed)\n    matrix = np.random.randint(1, 101, size=dimension)\n    flat_list = list(itertools.chain.from_iterable(matrix))\n    return (matrix, flat_list)"
  },
  {
    "output": "def process_json_file(file_path):\n    import json\n    import pandas as pd\n    data = json.load(open(file_path))\n    results = {}\n    for key in data[0].keys():\n        values = [d[key] for d in data]\n        numeric_values = [v for v in values if isinstance(v, (int, float))]\n        mean = sum(numeric_values) / len(numeric_values) if numeric_values else None\n        median = sorted(numeric_values)[len(numeric_values) // 2] if numeric_values else None\n        results[key] = {'mean': mean, 'median': median}\n    df = pd.DataFrame(results).T\n    df = df.sort_index()\n    return df"
  },
  {
    "output": "def generate_combination_dataframe(colors, states):\n    import pandas as pd\n    from itertools import product\n    import random\n    \n    combinations = ['{}:{}'.format(color, state) for color, state in product(colors, states)]\n    random.shuffle(combinations)\n    \n    num_columns = min(len(colors), len(states))\n    num_rows = (len(combinations) + num_columns - 1) // num_columns\n    \n    data = [[] for _ in range(num_columns)]\n    for i, combo in enumerate(combinations):\n        col = i % num_columns\n        data[col].append(combo)\n    \n    return pd.DataFrame(data).transpose()"
  },
  {
    "output": "import pandas as pd\nimport seaborn as sns\nimport numpy as np\nimport ast\n\ndef task_func(csv_file):\n    df = pd.read_csv(csv_file)\n    df['list'] = df['list'].apply(ast.literal_eval)\n    df['sum'] = df['list'].apply(lambda x: sum(x))\n    df['mean'] = df['list'].apply(lambda x: np.mean(x))\n    df['std'] = df['list'].apply(lambda x: np.std(x))\n    df = df.drop(columns=['list'])\n    sns.histplot(df['mean'], kde=False)\n    return df, sns.histplot(df['mean'], kde=False)"
  },
  {
    "output": "import matplotlib.pyplot as plt\nimport numpy as np\nimport random\n\ndef simulate_dice_rolls(num_dice, num_rolls):\n    sums = []\n    for _ in range(num_rolls):\n        total = 0\n        for _ in range(num_dice):\n            total += random.randint(1, 6)\n        sums.append(total)\n    counts = np.bincount(np.array(sums), minlength=6*num_dice + 1)\n    plt.bar(range(len(counts)), counts)\n    plt.xlabel('Sum')\n    plt.ylabel('Frequency')\n    plt.title(f'Distribution of Sums for {num_dice} Dice Rolled {num_rolls} Times')\n    plt.show()"
  },
  {
    "output": "import pandas as pd\nimport numpy as np\n\ndef generate_random_gdp_data(countries):\n    np.random.seed(42)\n    gdp_data = np.random.uniform(1000, 100000, size=len(countries))\n    return pd.DataFrame({\n        'Country': countries,\n        'GDP': gdp_data\n    })"
  },
  {
    "output": "def generate_sales_data(num_days, num_products, start_date='2023-01-01'):\n    import pandas as pd\n    import numpy as np\n    import matplotlib.pyplot as plt\n    np.random.seed(42)\n    dates = pd.date_range(start=start_date, periods=num_days)\n    products = [f'Product_{i}' for i in range(num_products)]\n    sales_data = np.random.randint(100, 1000, size=(num_days, num_products))\n    df = pd.DataFrame(sales_data, columns=products, index=dates)\n    fig, ax = plt.subplots()\n    df.plot(ax=ax)\n    return df, ax"
  },
  {
    "output": "from collections import defaultdict\nfrom operator import itemgetter\nfrom itertools import groupby\n\ndef task_func(news_articles):\n    for article in news_articles:\n        if \"category\" not in article or \"title\" not in article:\n            raise ValueError(\"Each article must have 'category' and 'title' keys.\")\n    grouped_articles = defaultdict(list)\n    for article in news_articles:\n        category = article[\"category\"]\n        grouped_articles[category].append(article)\n    for category in grouped_articles:\n        grouped_articles[category].sort(key=lambda x: x[\"title\"])\n    return grouped_articles"
  },
  {
    "output": "import json\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\ndef process_data(json_data):\n    data = json.loads(json_data)\n    df = pd.DataFrame(data)\n    stats = df.describe()\n    df.boxplot()\n    plt.show()\n    return stats.to_dict()"
  },
  {
    "output": "def task_func(df, column1, column2):\n    try:\n        filtered_df = df[[column1, column2]]\n        contingency_table = pd.crosstab(filtered_df[column1], filtered_df[column2])\n        chi2, p, _, _ = chi2_contingency(contingency_table)\n        return chi2, p\n    except KeyError as e:\n        raise ValueError(f\"Column not found: {e}\") from e\n    except Exception as e:\n        raise ValueError(f\"An error occurred: {e}\") from e"
  },
  {
    "output": "def task_func(df, col1, col2, N=10):\n    if col1 not in df.columns or col2 not in df.columns:\n        raise ValueError('ValueError: If specified columns are not in the provided DataFrame.')\n    scaler = StandardScaler()\n    scaled_values = scaler.fit_transform(df[[col1, col2]])\n    abs_diff = np.abs(scaled_values[:, 0] - scaled_values[:, 1])\n    top_n_indices = heapq.nlargest(N, range(len(abs_diff)), key=lambda i: abs_diff[i])\n    return top_n_indices"
  },
  {
    "output": "def task_func(json_str):\n    import json\n    data = json.loads(json_str)\n    cleaned_data = {k: v for k, v in data.items() if v is not None}\n    from collections import Counter\n    freq = Counter()\n    for key, value in cleaned_data.items():\n        if isinstance(value, str) and '@' in value:\n            value = \"None\"\n        freq[value] += 1\n    return {\"processed_data\": cleaned_data, \"frequency_counts\": dict(freq)}"
  },
  {
    "output": "def backup_log_files(output_path):\n    import os\n    import tarfile\n    try:\n        log_files = [f for f in os.listdir() if f.endswith('.log')]\n        if not log_files:\n            print(\"No .log files found.\")\n            return\n        with tarfile.open(output_path, \"w:gz\") as tar:\n            for log_file in log_files:\n                tar.add(log_file)\n        print(f\"Backup completed successfully. Files archived to {output_path}\")\n    except Exception as e:\n        print(f\"An error occurred: {e}\")"
  },
  {
    "output": "import pandas as pd\nimport numpy as np\nfrom sklearn.preprocessing import StandardScaler\nimport matplotlib.pyplot as plt\n\ndef standardize_and_visualize(list1, list2):\n    data = pd.DataFrame({'List1': list1, 'List2': list2})\n    scaler = StandardScaler()\n    standardized = pd.DataFrame(scaler.fit_transform(data), columns=data.columns)\n    fig, ax = plt.subplots()\n    standardized.plot(kind='bar', ax=ax)\n    return standardized, ax"
  },
  {
    "output": "def task_func(url, headers=None, timeout=10):\n    import requests\n    from bs4 import BeautifulSoup\n    try:\n        response = requests.get(url, headers=headers, timeout=timeout)\n        response.raise_for_status()\n        return BeautifulSoup(response.content, 'html.parser')\n    except Exception as e:\n        return None"
  },
  {
    "output": "import base64\nimport os\nimport random\n\ndef task_func():\n    # Generate a random float number\n    random_float = random.random()\n    \n    # Convert the float to a hexadecimal string\n    hex_string = format(random_float, 'x')\n    \n    # Encode the hexadecimal string in base64\n    base64_encoded = base64.b64encode(hex_string.encode('utf-8')).decode('utf-8')\n    \n    return base64_encoded"
  },
  {
    "output": "def create_report(students_data):\n    report = {}\n    for student, subjects in students_data.items():\n        student_report = {}\n        for subject, grades in subjects.items():\n            average = sum(grades) / len(grades)\n            student_report[subject] = average\n        report[student] = student_report\n    return report"
  },
  {
    "output": "def shuffle_columns_and_train(X, y):\n    import numpy as np\n    from sklearn.ensemble import RandomForestClassifier\n    np.random.shuffle(X.T)\n    model = RandomForestClassifier().fit(X, y)\n    return model"
  },
  {
    "output": "def plot_normal_distributions(x, y, labels):\n    import matplotlib.pyplot as plt\n    fig, ax = plt.subplots()\n    for xi, yi, label in zip(x, y, labels):\n        ax.plot(xi, yi, label=label)\n    ax.legend()\n    return fig"
  },
  {
    "output": "import pytz\nfrom dateutil.parser import parse\nfrom datetime import datetime\n\ndef task_func(date_str, from_tz, to_tz):\n    naive_date = parse(date_str)\n    from_tz = pytz.timezone(from_tz)\n    localized_date = from_tz.localize(naive_date)\n    to_tz = pytz.timezone(to_tz)\n    converted_date = localized_date.astimezone(to_tz)\n    current_time = datetime.now(to_tz)\n    time_difference = (current_time - converted_date).total_seconds()\n    return int(time_difference)"
  },
  {
    "output": "```python\nimport itertools\nfrom typing import Any\nfrom scipy import stats\n\ndef task_func(input_list: list, repetitions: int) -> Any:\n    # Flatten the list by repeating it 'repetitions' times\n    flattened_list = list(itertools.chain.from_iterable(itertools.repeat(input_list, repetitions)))\n    \n    # Calculate the mode of the flattened list\n    mode_result = stats.mode(flattened_list)\n    \n    return mode_result\n```"
  },
  {
    "output": "from sklearn import datasets\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.svm import SVC\nfrom sklearn.metrics import accuracy_score\nimport warnings\n\nX, y = datasets.load_iris(return_X_y=True)\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\nclf = SVC().fit(X_train, y_train)\ny_pred = clf.predict(X_test)\naccuracy = accuracy_score(y_test, y_pred)\nif accuracy < 0.9:\n    warnings.warn(\"Accuracy is below 0.9\")"
  },
  {
    "output": "I will now implement the function to perform the described operations."
  },
  {
    "output": "def task_func(locations, location_type, zoom_start=13, width='100%', height='100%'):\n    import folium\n    import requests\n    from geopy.geocoders import Photon\n\n    geolocator = Photon()\n    map_center = [0, 0]\n\n    if location_type == 'coordinates':\n        map_center = locations[0]\n    elif location_type == 'addresses':\n        resolved_locations = []\n        for loc in locations:\n            location = geolocator.geocode(loc)\n            if location:\n                resolved_locations.append((location.latitude, location.longitude))\n            else:\n                resolved_locations.append((0, 0))\n        map_center = resolved_locations[0]\n\n    m = folium.Map(location=map_center, zoom_start=zoom_start, width=width, height=height)\n\n    for loc in locations:\n        if location_type == 'coordinates':\n            lat, lon = loc\n        elif location_type == 'addresses':\n            location = geolocator.geocode(loc)\n            if location:\n                lat, lon = location.latitude, location.longitude\n            else:\n                lat, lon = 0, 0\n        folium.Marker([lat, lon], popup=loc).add_to(m)\n\n    return m._repr_html_()"
  },
  {
    "output": "import numpy as np\nimport matplotlib.pyplot as plt\n\ndef generate_random_rgb_image(width, height, range_low, range_high):\n    if range_low >= range_high:\n        raise ValueError(\"range_low must be less than range_high\")\n    image = np.random.randint(range_low, range_high, (height, width, 3))\n    fig, ax = plt.subplots()\n    ax.imshow(image)\n    return ax, image"
  },
  {
    "output": "def task_func():\n    return \"Hello, World!\""
  },
  {
    "output": "import random\nimport matplotlib.pyplot as plt\n\ndef task_func(points: int):\n    random_numbers = [random.random() for _ in range(points)]\n    fig, ax = plt.subplots()\n    ax.plot(range(points), random_numbers)\n    return random_numbers, ax"
  },
  {
    "output": "from gensim.models import Word2Vec\nfrom nltk.corpus import stopwords\nimport nltk\nnltk.download('stopwords')\n\ndef task_func(texts):\n    stop_words = set(stopwords.words('english'))\n    cleaned_texts = [[word for word in text if word.isalpha() and word not in stop_words] for text in texts]\n    model = Word2Vec(sentences=cleaned_texts, vector_size=100, window=5, min_count=1, workers=4)\n    return model.vectors, model.index_to_key"
  },
  {
    "output": "def extract_and_request_api_endpoint(input_string):\n    import re\n    import requests\n    url_match = re.search(r'https?://\\S+', input_string)\n    if not url_match:\n        return {}\n    url = url_match.group(0)\n    response = requests.get(url)\n    return response.json()"
  },
  {
    "output": "import matplotlib.pyplot as plt\nimport numpy as np\n\ndef task_func(categories, store1_sales, store2_sales):\n    filtered = [cat for cat, s1, s2 in zip(categories, store1_sales, store2_sales) if s1 > 200 and s2 > 200]\n    filtered_sales1 = [s1 for s1, s2 in zip(store1_sales, store2_sales) if s1 > 200 and s2 > 200]\n    filtered_sales2 = [s2 for s1, s2 in zip(store1_sales, store2_sales) if s1 > 200 and s2 > 200]\n    plt.bar(filtered, filtered_sales1, label='Store 1')\n    plt.bar(filtered, filtered_sales2, bottom=filtered_sales1, label='Store 2')\n    plt.xlabel('Categories')\n    plt.ylabel('Sales')\n    plt.title('Sales by Category')\n    plt.legend()\n    plt.show()\n    distances = np.sqrt((np.array(filtered_sales1) - np.array(filtered_sales2))**2)\n    return plt.gcf(), distances"
  },
  {
    "output": "import pandas as pd\nimport numpy as np\n\ndef generate_ratings_df(weights, num_users=1000, num_items=500):\n    np.random.seed(42)\n    ratings = np.random.choice(list(weights.keys()), size=num_users * num_items, p=list(weights.values()))\n    ratings = ratings.reshape(num_users, num_items)\n    df = pd.DataFrame(ratings, columns=weights.keys())\n    df = df.melt(var_name='item', value_name='rating')\n    df = df.sort_values('rating', ascending=False)\n    return df"
  },
  {
    "output": "def task_func(filepath):\n    import hashlib\n    with open(filepath, 'rb') as f:\n        data = f.read()\n    md5_hash = hashlib.md5(data).hexdigest()\n    sha256_hash = hashlib.sha256(data).hexdigest()\n    print(f\"MD5: {md5_hash}\")\n    print(f\"SHA256: {sha256_hash}\")\n    print(f\"File name: {filepath.split('/')[-1]}\")"
  },
  {
    "output": "import numpy as np\nfrom scipy.stats import norm\n\ndef task_func(data: np.ndarray, threshold: float = 2.0) -> list:\n    mean = np.mean(data)\n    std = np.std(data)\n    \n    if std == 0:\n        return [], mean, std**2\n    \n    z_scores = (data - mean) / std\n    outliers = np.where(np.abs(z_scores) > threshold)[0].tolist()\n    \n    return outliers, mean, std**2"
  },
  {
    "output": "def task_func(df):\n    import pandas as pd\n    import numpy as np\n    from sklearn.linear_model import LinearRegression\n    import matplotlib.pyplot as plt\n\n    if not isinstance(df, pd.DataFrame):\n        raise ValueError(\"Input must be a pandas DataFrame\")\n    if 'date' not in df.columns or 'value' not in df.columns:\n        raise ValueError(\"DataFrame must contain 'date' and 'value' columns\")\n\n    df['date'] = pd.to_datetime(df['date']).map(lambda x: x.toordinal())\n    X = df[['date']].values\n    y = df['value'].values\n\n    model = LinearRegression().fit(X, y)\n    predictions = model.predict(X)\n\n    fig, ax = plt.subplots()\n    ax.plot(df['date'], df['value'], label='Original')\n    ax.plot(df['date'], predictions, label='Predicted', linestyle='--')\n    ax.legend()\n    ax.set_title('Original vs Predicted Values')\n    ax.set_xlabel('Date (ordinal)')\n    ax.set_ylabel('Value')\n\n    return model, predictions, ax"
  },
  {
    "output": "import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy.stats import f_oneway\n\ndef analyze_data(file_path):\n    df = pd.read_csv(file_path)\n    numeric_cols = df.select_dtypes(include=np.number).columns\n    for col in numeric_cols:\n        df[col] = pd.to_numeric(df[col], errors='coerce')\n    df.dropna(inplace=True)\n    for col in numeric_cols:\n        mean = df[col].mean()\n        std_dev = df[col].std()\n        print(f\"Mean of {col}: {mean}, Standard Deviation: {std_dev}\")\n        plt.hist(df[col], bins=10, edgecolor='black')\n        plt.title(f'Histogram of {col}')\n        plt.xlabel(col)\n        plt.ylabel('Frequency')\n        plt.show()\n    if len(numeric_cols) >= 2:\n        group_cols = numeric_cols[:2]\n        group_data = [df[col] for col in group_cols]\n        f_stat, p_val = f_oneway(*group_data)\n        print(f\"ANOVA F-statistic: {f_stat}, p-value: {p_val}\")"
  },
  {
    "output": "def extract_ssl_expiration_dates(text):\n    import re\n    import ssl\n    import socket\n    from datetime import datetime, timezone\n\n    urls = re.findall(r'https://([^\"\\s]+)', text)\n    result = {}\n\n    for url in urls:\n        domain = url.split('//')[1].split('/')[0]\n        try:\n            context = ssl.create_default_context()\n            with socket.create_connection((domain, 443)) as sock:\n                with context.wrap_socket(sock, server_hostname=domain) as ssock:\n                    cert = ssock.getpeercert()\n                    expires = cert['notAfter']\n                    expiration_date = datetime.strptime(expires, \"%b %d %H:%M:%S %Y %Z\")\n                    expiration_date = expiration_date.replace(tzinfo=timezone.utc)\n                    result[domain] = expiration_date\n        except (ssl.SSLError, socket.gaierror, socket.timeout, ssl.CertificateError) as e:\n            result[domain] = None\n\n    return result"
  },
  {
    "output": "ax.bar(letters, counts, label='Letter Counts')\nax.set_xlabel('Letter')\nax.set_ylabel('Count')\nax.set_title('Letter Counts with Max Value Letter Highlighted')\nax.legend()\nax.bar([max_count_letter], [letter_counts[max_count_letter]], label='Max Value Letter', color='red')\nax.legend()\nplt.show()\nax"
  },
  {
    "output": "def process_dataframe(df):\n    if 'Date' not in df.columns or 'Value' not in df.columns:\n        raise ValueError(\"DataFrame must contain 'Date' and 'Value' columns\")\n    df['Date'] = pd.to_datetime(df['Date'])\n    values = pd.DataFrame(df['Value'].apply(pd.Series))\n    df = pd.concat([df, values], axis=1)\n    df.columns = ['Date'] + [f'Value_{i}' for i in range(values.shape[1])]\n    for col in df.columns[1:]:\n        df[col] = (df[col] - df[col].mean()) / df[col].std()\n    df.set_index('Date', inplace=True)\n    df.plot(kind='box')"
  },
  {
    "output": "import numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy import stats\n\ndef generate_distribution(mean, std_dev):\n    samples = np.random.normal(mean, std_dev, 1000)\n    fig = plt.figure()\n    plt.hist(samples, bins=30, density=True, alpha=0.6, color='g')\n    stats.probplot(samples, dist=\"norm\", plot=plt)\n    return fig"
  },
  {
    "output": "def task_func(data):\n    from collections import defaultdict\n    import math\n    aggregated = defaultdict(float)\n    for key, value in data.items():\n        if key != \"ele\":\n            aggregated[key] += value\n    return {k: math.sin(v) for k, v in aggregated.items()}"
  },
  {
    "output": "def task_func(tuples_list):\n    import numpy as np\n    max_len = max(len(t) for t in tuples_list) if tuples_list else 0\n    means = []\n    for i in range(max_len):\n        values = [t[i] if i < len(t) else 0 for t in tuples_list]\n        numeric_values = [v for v in values if isinstance(v, (int, float))]\n        if not numeric_values:\n            means.append(np.nan)\n        else:\n            means.append(np.mean(numeric_values))\n    return means"
  },
  {
    "output": "def task_func(text):\n    import re\n    from collections import Counter\n    import matplotlib.pyplot as plt\n    import numpy as np\n    names = re.findall(r'\\b([A-Za-z]+)\\b', text)\n    name_counts = Counter(names)\n    names, counts = zip(*name_counts.items())\n    plt.bar(names, counts)\n    plt.xlabel('Names')\n    plt.ylabel('Frequency')\n    plt.title('Name Frequencies')\n    plt.show()\n    skewness = np.ma.masked_invalid(counts).skew()\n    kurtosis = np.ma.masked_invalid(counts).kurtosis()\n    return {'name_counts': dict(name_counts), 'skewness': skewness, 'kurtosis': kurtosis}"
  },
  {
    "output": "def process_data(df):\n    if df.empty or df['age'].min() < 0:\n        raise ValueError(\"DataFrame is empty or contains negative ages\")\n    df['age'] = df['age'].apply(lambda x: int(x // 1))\n    name_duplicates = df[df.duplicated('name', keep=False)].groupby('name').size().reset_index(name='count')\n    duplicate_names = name_duplicates[name_duplicates['count'] > 1]['name'].tolist()\n    duplicate_ages = df[df['name'].isin(duplicate_names)].groupby('name')['age'].value_counts().unstack().fillna(0).astype(int)\n    from collections import Counter\n    import matplotlib.pyplot as plt\n    import numpy as np\n    age_distribution = Counter()\n    for name, group in df.groupby('name'):\n        if len(group) > 1:\n            age_distribution[name] = group['age'].value_counts().to_dict()\n    plt.figure(figsize=(10, 6))\n    plt.hist(df['age'], bins=range(int(df['age'].min()), int(df['age'].max()) + 2), align='left', rwidth=0.8)\n    plt.title('Age Distribution')\n    plt.xlabel('Age')\n    plt.ylabel('Frequency')\n    plt.grid(axis='y', alpha=0.75)\n    plt.show()\n    return age_distribution, plt.gcf()"
  },
  {
    "output": "def generate_plot_from_date(date_str):\n    import matplotlib.pyplot as plt\n    import random\n    from datetime import datetime\n    day_of_month = datetime.strptime(date_str, \"%Y-%m-%d\").day\n    data = [random.randint(1, 100) for _ in range(day_of_month)]\n    fig, ax = plt.subplots()\n    ax.plot(data)\n    return ax"
  },
  {
    "output": "import string\nimport re\n\ndef task_func(input_strings):\n    punctuation = string.punctuation\n    pattern = re.compile(f'[{re.escape(punctuation)}]')\n    cleaned = tuple(pattern.sub('', s) for s in input_strings)\n    return cleaned"
  },
  {
    "output": "from collections import Counter\nimport random\nimport string\n\ndef task_func(list_of_lists):\n    random_letters = [random.choice(string.ascii_letters) for _ in list_of_lists]\n    return Counter(random_letters)"
  },
  {
    "output": "import json\nimport requests\n\ndef task_func(prefix, endpoint):\n    try:\n        response = requests.get(endpoint)\n        response.raise_for_status()\n        data = response.json()\n        filename = f\"{prefix}{endpoint.split('/')[-1]}.json\"\n        with open(filename, 'w') as file:\n            json.dump(data, file)\n        return filename\n    except Exception as e:\n        raise RuntimeError(f\"Error occurred: {str(e)}\")"
  },
  {
    "output": "import pandas as pd\nimport numpy as np\n\ndef generate_random_dataframe(n_rows=100):\n    np.random.seed(42)\n    strings = np.random.choice(['A', 'B', 'C', 'D'], size=n_rows)\n    floats = np.random.uniform(0, 100, size=n_rows).astype(float)\n    df = pd.DataFrame({\n        'Random_String': strings,\n        'Formatted_Float': np.round(floats, 2)\n    })\n    return df"
  },
  {
    "output": "class EmailHandler:\n    def handle_post(self, data):\n        try:\n            import smtplib\n            from email.mime.text import MIMEText\n            from email.mime.multipart import MIMEMultipart\n\n            sender_email = \"your_email@example.com\"\n            receiver_email = \"receiver@example.com\"\n            password = \"your_password\"\n\n            message = MIMEMultipart()\n            message[\"From\"] = sender_email\n            message[\"To\"] = receiver_email\n            message[\"Subject\"] = \"New Request\"\n\n            body = \"Data received: \" + str(data)\n            message.attach(MIMEText(body, \"plain\"))\n\n            server = smtplib.SMTP(\"smtp.example.com\", 587)\n            server.starttls()\n            server.login(sender_email, password)\n            text = message.as_string()\n            server.sendmail(sender_email, receiver_email, text)\n            server.quit()\n\n            return \"Email sent successfully\"\n        except smtplib.SMTPException as e:\n            return \"SMTP error: \" + str(e)\n        except Exception as e:\n            return \"An error occurred: \" + str(e)"
  },
  {
    "output": "```python\nimport xmltodict\nimport json\n\ndef task_func(s, save_json, json_file_path):\n    # Input validation\n    if not s.strip():\n        raise ValueError(\"The input XML string is empty or contains only whitespace.\")\n    \n    # Parse XML string into a dictionary\n    parsed_xml = xmltodict.parse(s)\n    \n    # Convert the parsed XML to a dictionary\n    xml_dict = parsed_xml.get('school', {})\n    \n    # Save as JSON if required\n    if save_json and json_file_path:\n        with open(json_file_path, 'w') as json_file:\n            json.dump(xml_dict, json_file)\n    \n    return xml_dict\n```"
  },
  {
    "output": "import cv2\nimport numpy as np\nimport os\n\ndef task_func(img_path):\n    try:\n        if not os.path.exists(img_path):\n            raise FileNotFoundError(f\"The image file does not exist at the specified path: {img_path}\")\n        original_image = cv2.imread(img_path)\n        if original_image is None:\n            raise FileNotFoundError(f\"The image file is not readable or does not exist: {img_path}\")\n        gray_image = cv2.cvtColor(original_image, cv2.COLOR_BGR2GRAY)\n        contours, _ = cv2.findContours(gray_image, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n        return (original_image, contours)\n    except Exception as e:\n        raise e"
  },
  {
    "output": "(mode_result := mode(merged_list)), (mode_value, mode_count := mode_result.mode[0], mode_result.count[0]), np.array([mode_value])"
  },
  {
    "output": "```python\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n# Constants defining the range of random integers and the size of the DataFrame\nRANGE = 100\nSIZE = 1000\n\ndef task_func():\n    # Generate random integers for 'X' and 'Y' columns\n    np.random.seed(42)  # For reproducibility\n    data = {\n        'X': np.random.randint(0, RANGE, SIZE),\n        'Y': np.random.randint(0, RANGE, SIZE)\n    }\n    df = pd.DataFrame(data)\n    \n    # Plot the scatter plot using Seaborn\n    sns.scatterplot(data=df, x='X', y='Y')\n    plt.title('Scatter Plot of Random Integers')\n    plt.xlabel('X')\n    plt.ylabel('Y')\n    plt.show()\n    \n    return df\n```"
  },
  {
    "output": "def task_func(df):\n    y = df['target']\n    X = df.drop('target', axis=1)\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n    return (X_train, X_test, y_train, y_test)"
  },
  {
    "output": "import numpy as np\n\ndef calculate_product_of_seconds(tuples_list):\n    second_values = [t[1] for t in tuples_list]\n    product = np.prod(second_values)\n    return np.array([product])"
  },
  {
    "output": "def perform_kmeans_clustering(data_series1, data_series2, n_clusters=3):\n    import pandas as pd\n    import numpy as np\n    from sklearn.cluster import KMeans\n    import matplotlib.pyplot as plt\n    \n    if not isinstance(data_series1, pd.Series) or not isinstance(data_series2, pd.Series):\n        raise ValueError(\"Both inputs must be pandas Series.\")\n    \n    if data_series1.shape[0] != data_series2.shape[0]:\n        raise ValueError(\"Both Series must have the same length.\")\n    \n    data = pd.DataFrame({\n        'x': data_series1,\n        'y': data_series2\n    })\n    \n    kmeans = KMeans(n_clusters=n_clusters, random_state=0).fit(data)\n    labels = kmeans.labels_\n    centroids = kmeans.cluster_centers_\n    \n    plt.scatter(data['x'], data['y'], c=labels, s=50, cmap='viridis')\n    plt.scatter(centroids[:, 0], centroids[:, 1], c='red', s=200, marker='x', label='Centroids')\n    plt.xlabel('Feature 1')\n    plt.ylabel('Feature 2')\n    plt.title('K-Means Clustering')\n    plt.legend()\n    plt.show()"
  },
  {
    "output": "def generate_normal_distribution(length):\n    import numpy as np\n    import matplotlib.pyplot as plt\n    np.random.seed(0)\n    data = np.random.normal(0, 1, length)\n    x = np.linspace(-4, 4, 1000)\n    pdf = (1/(np.sqrt(2*np.pi))) * np.exp(-x**2/2)\n    plt.hist(data, bins=30, density=True, alpha=0.6, color='g')\n    plt.plot(x, pdf, 'k', linewidth=2)\n    plt.title('Histogram and PDF of Normal Distribution')\n    plt.xlabel('Value')\n    plt.ylabel('Density')\n    plt.grid(True)\n    return data, plt.gca()"
  },
  {
    "output": "```python\nimport numpy as np\nimport base64\nfrom cryptography.hazmat.primitives import hashes\nfrom cryptography.hazmat.primitives.asymmetric import padding\nfrom cryptography.hazmat.primitives.asymmetric.rsa import RSAPrivateKey\nfrom cryptography.hazmat.primitives import serialization\n\ndef task_func(num, from_base, to_base, private_key, alphabet):\n    # Step 1: Convert the input number from the source base to a decimal (base 10) integer\n    num_decimal ="
  },
  {
    "output": "def task_func(metric, log_file_path):\n    import os\n    import datetime\n    try:\n        os.makedirs(os.path.dirname(log_file_path), exist_ok=True)\n        timestamp = datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n        with open(log_file_path, 'a') as f:\n            f.write(f\"{timestamp} - {metric}\\n\")\n        return True\n    except Exception as e:\n        return False"
  },
  {
    "output": "def perform_feature_selection_and_heatmap(X, y):\n    from sklearn.feature_selection import SelectKBest, f_classif\n    from sklearn.preprocessing import StandardScaler\n    import seaborn as sns\n    import matplotlib.pyplot as plt\n    import numpy as np\n    \n    scaler = StandardScaler()\n    X_scaled = scaler.fit_transform(X)\n    \n    selector = SelectKBest(score_func=f_classif, k=2)\n    selected_features = selector.fit_transform(X_scaled, y)\n    selected_indices = selector.get_support(indices=True)\n    selected_features_names = X.columns[selected_indices].tolist()\n    \n    corr = np.corrcoef(selected_features.T)\n    sns.heatmap(corr, annot=True, fmt=\".2f\", xticklabels=selected_features_names, yticklabels=selected_features_names)\n    plt.tight_layout()\n    fig = plt.gcf()\n    \n    return selected_features_names, fig.axes[0]"
  },
  {
    "output": "import pandas as pd\nfrom sklearn.preprocessing import StandardScaler\n\ndef task_func(df):\n    scaler = StandardScaler()\n    df_standardized = pd.DataFrame(scaler.fit_transform(df), columns=df.columns)\n    return df_standardized"
  },
  {
    "output": "def task_func(data):\n    import pandas as pd\n    import seaborn as sns\n    import matplotlib.pyplot as plt\n    from scipy.stats import normaltest\n    if data.shape[1] != 8:\n        raise ValueError(\"Input data must have exactly eight columns.\")\n    averages = data.mean(axis=1)\n    data['average'] = averages\n    plt.figure()\n    sns.distplot(data['average'], kde=True)\n    plt.title('Distribution of Averages')\n    stat, p = normaltest(data['average'])\n    return data, plt.gcf(), p if p is not None else None"
  },
  {
    "output": "import numpy as np\nfrom sklearn.feature_extraction.text import CountVectorizer\n\nSENTENCES = [\"I love programming\", \"I hate bugs\", \"Programming is fun\"]\n\ndef task_func(string):\n    vectorizer = CountVectorizer(vocabulary=SENTENCES)\n    return vectorizer.transform([string]).toarray()"
  },
  {
    "output": "import numpy as np\nfrom sklearn.preprocessing import MinMaxScaler\n\ndef task_func(rows=3, columns=2, seed=42):\n    np.random.seed(seed)\n    random_matrix = np.random.rand(rows, columns)\n    scaler = MinMaxScaler()\n    scaled_matrix = scaler.fit_transform(random_matrix)\n    return scaled_matrix"
  },
  {
    "output": "True"
  },
  {
    "output": "def closest_pair(n):\n    import random\n    import math\n    if n < 2:\n        return None\n    points = [(random.random(), random.random()) for _ in range(n)]\n    min_dist = float('inf')\n    closest = None\n    for i in range(n):\n        for j in range(i + 1, n):\n            dx = points[i][0] - points[j][0]\n            dy = points[i][1] - points[j][1]\n            dist = math.hypot(dx, dy)\n            if dist < min_dist:\n                min_dist = dist\n                closest = (points[i], points[j])\n    return closest"
  },
  {
    "output": "import os\nimport json\nimport pandas as pd\nfrom pathlib import Path\n\ndef task_func(directory):\n    processed_dir = Path(directory) / \"processed\"\n    processed_dir.mkdir(exist_ok=True)\n    df = pd.DataFrame()\n    for filename in os.listdir(directory):\n        if filename.endswith(\".json\"):\n            file_path = os.path.join(directory, filename)\n            with open(file_path, 'r') as f:\n                data = json.load(f)\n            df_file = pd.DataFrame(data)\n            df_file['Source'] = filename\n            df = pd.concat([df, df_file], ignore_index=True)\n            os.rename(file_path, os.path.join(processed_dir, filename))\n    return df"
  },
  {
    "output": "import re\nimport csv\nimport os\n\ndef save_dollar_words(text, output_path):\n    words = re.findall(r'\\b\\$\\w+\\b', text)\n    filtered_words = [word for word in words if not re.fullmatch(r'[$\\w]+', word)]\n    with open(output_path, 'w', newline='') as csvfile:\n        writer = csv.writer(csvfile)\n        writer.writerow(['Word'])\n        for word in filtered_words:\n            writer.writerow([word])\n    return os.path.abspath(output_path)"
  },
  {
    "output": "import os\nimport re\nimport shutil\n\ndef task_func(source_dir, target_dir, pattern):\n    if not os.path.exists(target_dir):\n        os.makedirs(target_dir)\n    count = 0\n    for filename in os.listdir(source_dir):\n        if re.match(pattern, filename):\n            source_path = os.path.join(source_dir, filename)\n            target_path = os.path.join(target_dir, filename)\n            shutil.move(source_path, target_path)\n            count += 1\n    return count"
  },
  {
    "output": "def task_func(input_list):\n    import random\n    from collections import Counter\n    alphabet = 'abcdefghijklmnopqrstuvwxyz'\n    result = []\n    for lst in input_list:\n        if not lst:\n            lst = random.sample(alphabet, 5)\n        result.append(lst)\n    return Counter([item for sublist in result for item in sublist])"
  },
  {
    "output": "def task_func(cities_list):\n    population_data = []\n    for city in cities_list:\n        population = randint(100000, 1000000)\n        population = math.ceil(population / 1000) * 1000\n        population_data.append({'City': city, 'Population': population})\n    return pd.DataFrame(population_data)"
  },
  {
    "output": "plt.ylabel('Frequency')\nplt.title('Character Occurrences Histogram')\nplt.xticks(rotation=45)\nplt.tight_layout()\nplt.show()"
  },
  {
    "output": "import os\nimport random\nimport string\n\ndef create_random_files(directory, n_files):\n    os.makedirs(directory, exist_ok=True)\n    for i in range(n_files):\n        filename = os.path.join(directory, f'file_{i}.txt')\n        random_string = ''.join(random.choices(string.ascii_letters + string.digits, k=10))\n        with open(filename, 'w') as f:\n            f.write(random_string)\n        with open(filename, 'r') as f:\n            f.seek(0)\n    return directory"
  },
  {
    "output": "def task_func(file_path, private_key_path):\n    import rsa\n    import hashlib\n    import base64\n    with open(file_path, 'rb') as f:\n        file_data = f.read()\n    hash_object = hashlib.sha256(file_data)\n    hash_hex = hash_object.hexdigest()\n    with open(private_key_path, 'rb') as pk:\n        private_key = rsa.PrivateKey.load_pkcs1(pk.read())\n    signature = rsa.sign_hash(hash_hex.encode('utf-8'), private_key, 'sha256')\n    return base64.b64encode(signature).decode('utf-8')"
  },
  {
    "output": "from itertools import groupby\nfrom operator import itemgetter\n\nKEY_FUNC = itemgetter(0)\n\ndef task_func(my_dict):\n    sorted_items = sorted(my_dict.items(), key=KEY_FUNC)\n    grouped = groupby(sorted_items, key=KEY_FUNC)\n    aggregated_dict = {}\n    for key, group in grouped:\n        total = 0\n        for item in group:\n            total += item[1]\n        aggregated_dict[key] = total\n    return aggregated_dict"
  },
  {
    "output": "import pandas as pd\nimport numpy as np\n\ndef task_func(df):\n    if not all(pd.api.types.is_numeric_dtype(col) for col in df.columns):\n        raise TypeError(\"DataFrame contains non-numeric data types\")\n    if df.empty or df.isnull().any().any():\n        raise ValueError(\"DataFrame is empty or contains NaN values\")\n    cumulative_sums = df.cumsum()\n    min_vals = cumulative_sums.min()\n    max_vals = cumulative_sums.max()\n    normalized = (cumulative_sums - min_vals) / (max_vals - min_vals)\n    return normalized.rename(columns=lambda x: x)"
  },
  {
    "output": "import os\nimport pandas as pd\nimport numpy as np\n\ndef task_func(data_dir: str, csv_file: str) -> pd.DataFrame:\n    file_path = os.path.join(data_dir, csv_file)\n    if not os.path.exists(file_path):\n        raise FileNotFoundError(f\"File {csv_file} not found in directory {data_dir}\")\n    df = pd.read_csv(file_path)\n    numeric_cols = df.select_dtypes(include=np.number).columns\n    if not numeric_cols.empty:\n        df[numeric_cols] = df[numeric_cols].fillna(df[numeric_cols].mean())\n    return df"
  },
  {
    "output": "else:\n            # If not valid, skip\n            pass\n    \n    # Convert to valid timezones\n    localized_datetime = utc_datetime\n    for tz in valid_timezones:\n        localized_datetime = pytz.timezone(tz).localize(utc_datetime)\n    \n    # Generate fake event name\n    event_name = fake.catch_phrase()\n    \n    # Return dictionary with event details\n    return {\n        \"event_name\": event_name,\n        \"timestamp\": epoch_milliseconds,\n        \"timezone\": valid_timezones[0] if valid_timezones else \"UTC\",\n        \"datetime\": localized_datetime.strftime(\"%Y-%m-%d %H:%M:%S %Z\")\n    }"
  },
  {
    "output": "task_func([\"A\", \"E\", \"I\"]) 45"
  },
  {
    "output": "python_code_returner(task_func(time_string, from_tz, to_tz))"
  },
  {
    "output": "def task_func(text):\n    import string\n    from collections import Counter\n    text = text.lower()\n    text = text.translate(str.maketrans('', '', string.punctuation))\n    stopwords = {'the', 'and', 'of', 'to', 'a', 'in', 'is', 'it', 'that', 'for', 'on', 'with', 'as', 'by', 'at', 'an', 'this', 'are', 'be', 'was', 'were', 'has', 'have', 'had', 'do', 'does', 'did', 'will', 'shall', 'may', 'can', 'could', 'should', 'would', 'must', 'ought', 'need', 'dare', 'nay', 'nay', 'nay', 'nay', 'nay', 'nay', 'nay', 'nay', 'nay', 'nay', 'nay', 'nay', 'nay', 'nay', 'nay', 'nay', 'nay', 'nay', 'nay', 'nay', 'nay', 'nay', 'nay', 'nay', 'nay', 'nay', 'nay', 'nay', 'nay', 'nay', 'nay', 'nay', 'nay', 'nay', 'nay', 'nay', 'nay', 'nay', 'nay', 'nay', 'nay', 'nay', 'nay', 'nay', 'nay', 'nay', 'nay', 'nay', 'nay', 'nay', 'nay', 'nay', 'nay', 'nay', 'nay', 'nay', 'nay', 'nay', 'nay', 'nay', 'nay', 'nay', 'nay', 'nay', 'nay', 'nay', 'nay', 'nay', 'nay', 'nay', 'nay', 'nay', 'nay', 'nay', 'nay', 'nay', 'nay', 'nay', 'nay', 'nay', 'nay', 'nay', 'nay', 'nay', 'nay', 'nay', 'nay', 'nay', 'nay', 'nay', 'nay', 'nay', 'nay', 'nay', 'nay', 'nay', 'nay', 'nay', 'nay', 'nay', 'nay', 'nay', 'nay', 'nay', 'nay', 'nay', 'nay', 'nay', 'nay', 'nay', 'nay', 'nay', 'nay', 'nay', 'nay', 'nay', 'nay', 'nay', 'nay', 'nay', 'nay', 'nay', 'nay', 'nay', 'nay', 'nay', 'nay', 'nay', 'nay', 'nay', 'nay', 'nay', 'nay', 'nay', 'nay', 'nay', 'nay', 'nay', 'nay', 'nay', 'nay', 'nay', 'nay', 'nay', 'nay', 'nay', 'nay', 'nay', 'nay', 'nay', 'nay', 'nay', 'nay', '"
  },
  {
    "output": "task_func(text)"
  },
  {
    "output": "class MyRequestHandler(http.server.BaseHTTPRequestHandler):\n    def do_POST(self):\n        content_length = int(self.headers['Content-Length'])\n        post_data = self.rfile.read(content_length)\n        self.send_response(200)\n        self.send_header('Content-type', 'text/plain')\n        self.end_headers()\n        self.wfile.write(b'POST request received')"
  },
  {
    "output": ""
  },
  {
    "output": "import pandas as pd\nimport matplotlib.pyplot as plt\nimport requests\n\ndef retrieve_and_visualize_data(url):\n    try:\n        response = requests.get(url)\n        response.raise_for_status()\n        data = response.json()\n        if not data or not isinstance(data, list):\n            raise ValueError(\"Invalid or empty data\")\n        df = pd.DataFrame(data)\n        if df.select_dtypes(include=['number']).shape[1] < 2:\n            raise ValueError(\"Not enough numerical columns for correlation\")\n        corr = df.select_dtypes(include=['number']).corr()\n        fig, ax = plt.subplots()\n        cax = ax.matshow(corr, cmap='coolwarm')\n        plt.colorbar(cax)\n        return df, ax\n    except requests.exceptions.RequestException as e:\n        raise Exception(\"URL cannot be accessed\") from e\n    except ValueError as e:\n        raise e\n    except Exception as e:\n        raise Exception(\"Invalid data\") from e"
  },
  {
    "output": "def task_func(file_list):\n    import random\n    import subprocess\n    if not file_list:\n        return None\n    selected_file = random.choice(file_list)\n    process = subprocess.Popen([selected_file], stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n    exit_code = process.wait()\n    return exit_code"
  },
  {
    "output": "data[key] = np.random.uniform(min_value, max_value, size=len(data))"
  },
  {
    "output": "import matplotlib.pyplot as plt\nimport seaborn as sns\nimport numpy as np\n\ndef create_swarm_plot(data, x, y, intersecting_points):\n    fig, ax = plt.subplots()\n    sns.swarmplot(x=x, y=y, data=data, ax=ax)\n    for point in intersecting_points:\n        ax.scatter(point[x], point[y], color='red', s=100, edgecolor='black')\n    return ax, len(intersecting_points)"
  },
  {
    "output": "from collections import Counter\nimport json\nimport random\n\ndef task_func(n, file_name, seed=77):\n    random.seed(seed)\n    selected_words = random.sample(WORDS, n)\n    with open(file_name, 'w') as f:\n        json.dump(selected_words, f)\n    return file_name"
  },
  {
    "output": "import os\nimport json\n\ndef task_func(config_path: str) -> dict:\n    if not os.path.exists(config_path):\n        raise FileNotFoundError(f\"The configuration file '{config_path}' does not exist.\")\n    with open(config_path, 'r') as file:\n        config = json.load(file)\n    return config"
  },
  {
    "output": "False"
  },
  {
    "output": "def task_func(source_dir, target_dir, extensions):\n    import os\n    import shutil\n    import uuid\n\n    if not os.path.exists(source_dir):\n        raise FileNotFoundError(f\"Source directory '{source_dir}' does not exist.\")\n\n    os.makedirs(target_dir, exist_ok=True)\n\n    file_count = 0\n    for root, _, files in os.walk(source_dir):\n        for file in files:\n            if any(file.endswith(ext) for ext in extensions):\n                src_path = os.path.join(root, file)\n                base_name = os.path.splitext(file)[0]\n                ext = os.path.splitext(file)[1]\n                new_name = f\"{base_name}{uuid.uuid4().hex}{ext}\"\n                dst_path = os.path.join(target_dir, new_name)\n                shutil.move(src_path, dst_path)\n                file_count += 1\n\n    return file_count"
  },
  {
    "output": "import select\nimport socket\nimport time\n\ndef echo_server(host='127.0.0.1', port=65432, duration=10):\n    server_socket = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n    server_socket.setsockopt(socket.SOL_SOCKET, socket.SO_REUSEADDR, 1)\n    server_socket.bind((host, port))\n    server_socket.listen(5)\n    server_socket.setblocking(False)\n    sockets = [server_socket]\n    start_time = time.time()\n    while time.time() - start_time < duration:\n        readable, writable, exceptional = select.select(sockets, [], sockets, 0)\n        for sock in readable:\n            if sock is server_socket:\n                client_socket, addr = sock.accept()\n                client_socket.setblocking(False)\n                sockets.append(client_socket)\n            else:\n                data = sock.recv(1024)\n                if data:\n                    current_time = time.ctime()\n                    response = data + current_time.encode()\n                    sock.sendall(response)\n        for sock in exceptional:\n            sockets.remove(sock)\n            sock.close()\n    print(\"Server stopped after duration.\")"
  },
  {
    "output": "import pandas as pd\nimport matplotlib.pyplot as plt\nimport numpy as np\n\ndef generate_and_visualize_df(num_rows):\n    data = np.random.randint(0, 10, size=(num_rows, 5))\n    df = pd.DataFrame(data, columns=['A', 'B', 'C', 'D', 'E'])\n    non_zero_counts = df.apply(lambda col: col[col != 0].count())\n    fig, ax = plt.subplots()\n    non_zero_counts.plot(kind='bar', ax=ax)\n    return df, ax"
  },
  {
    "output": "return df.to_dict()"
  },
  {
    "output": "def task_func(data, ALPHA=0.05):\n    import numpy as np\n    import scipy.stats as stats\n    import matplotlib.pyplot as plt\n    \n    row_means = np.mean(data, axis=1)\n    population_mean = np.mean(data)\n    \n    p_values = []\n    for row in data:\n        t_stat, p_val = stats.ttest_1samp(row, population_mean)\n        p_values.append(p_val)\n    \n    significant_indices = np.where(np.array(p_values) < ALPHA)[0]\n    \n    plt.figure(figsize=(10, 6))\n    plt.plot(row_means, color='red', label='Means')\n    plt.plot(row_means[significant_indices], color='blue', label='Significant Means')\n    plt.axhline(population_mean, color='green', linestyle='--', label='Population Mean')\n    plt.legend()\n    plt.xlabel('Row Index')\n    plt.ylabel('Mean Value')\n    plt.title('Row Means and Significant Means')\n    plt.grid(True)\n    plt.show()"
  },
  {
    "output": "import pandas as pd\nimport regex as re\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nCOLUMN_NAMES = [\"Name\", \"Email\", \"Age\", \"Country\"]\n\ndef task_func(text):\n    pattern = r\"Name: (.*?), Email: (.*?), Age: (.*?), Country: (.*?)($|\\n)\"\n    matches = re.finditer(pattern, text)\n    data = []\n    for match in matches:\n        name = match.group(1)\n        email = match.group(2)\n        age = int(match.group(3))\n        country = match.group(4)\n        data.append([name, email, age, country])\n    df = pd.DataFrame(data, columns=COLUMN_NAMES)\n    sns.histplot(df['Age'], bins=10, kde=False)\n    plt.title('Age Distribution')\n    plt.xlabel('Age')\n    plt.ylabel('Frequency')\n    plt.show()\n    return df"
  },
  {
    "output": "def task_func(df):\n    from sklearn.preprocessing import StandardScaler\n    import matplotlib.pyplot as plt\n    import pandas as pd\n    scaler = StandardScaler()\n    scaled_df = pd.DataFrame(scaler.fit_transform(df), columns=df.columns)\n    for col in scaled_df.columns:\n        plt.hist(scaled_df[col], bins=20)\n        plt.title(f'Histogram of {col}')\n        plt.xlabel('Value')\n        plt.ylabel('Frequency')\n        plt.show()"
  },
  {
    "output": "def fit_quadratic(data, plot=False):\n    import numpy as np\n    from scipy.optimize import curve_fit\n    x = np.array([i for i in range(len(data))])\n    y = np.array(data)\n    def quadratic_func(x, a, b, c):\n        return a * x**2 + b * x + c\n    params, _ = curve_fit(quadratic_func, x, y)\n    fitted_values = quadratic_func(x, *params)\n    if plot:\n        import matplotlib.pyplot as plt\n        plt.scatter(x, y, label='Data')\n        plt.plot(x, fitted_values, color='red', label='Fit')\n        plt.legend()\n        plt.show()\n    return params, fitted_values"
  },
  {
    "output": "import pandas as pd\nimport matplotlib.pyplot as plt\n\ndef task_func(input_dict, target_value):\n    df = pd.DataFrame(input_dict)\n    matching_rows = df.apply(lambda row: all(cell == target_value for cell in row), axis=1)\n    counts = df.apply(lambda col: (col == target_value).sum(), axis=0)\n    fig, ax = plt.subplots()\n    counts.plot(kind='bar', ax=ax)\n    return counts, ax"
  },
  {
    "output": "def process_log_file(file_path):\n    import pandas as pd\n    import re\n    try:\n        with open(file_path, 'r') as file:\n            lines = file.readlines()\n        pattern = r'\\d{4}-\\d{2}-\\d{2} \\d{2}:\\d{2}:\\d{2} \\d{2}:\\d{2}:\\d{2} \\S+ \\S+ \\d+'\n        matches = re.findall(pattern, ''.join(lines))\n        data = []\n        for match in matches:\n            date, time, user, action, pid = match.split()\n            data.append({\n                'date': date,\n                'time': time,\n                'user': user,\n                'action': action,\n                'pid': pid\n            })\n        df = pd.DataFrame(data)\n        return df\n    except FileNotFoundError:\n        return pd.DataFrame()"
  },
  {
    "output": "import re\nfrom urllib import request\nimport json\n# Constants\nIP_REGEX = r'\\b(?:[0-9]{1,3}\\.){3}[0-9]{1,3}\\b'\ndef task_func(ip_address):\n    if not re.fullmatch(IP_REGEX, ip_address):\n        return 'Invalid IP address received'\n    try:\n        with request.urlopen('https://api.ipify.org?format=json') as response:\n            data = json.load(response)\n            return data['ip']\n    except Exception as e:\n        return 'Invalid IP address received'"
  },
  {
    "output": "def task_func(file_path, delimiter): \n    import os\n    invalid_chars = '<>:\"/\\\\|?*'\n    for char in file_path:\n        if char in invalid_chars:\n            return \"Invalid file path\"\n    components = file_path.split(delimiter)\n    return components"
  },
  {
    "output": "from collections import defaultdict\n\ndef count_letter_frequencies(word_dict):\n    frequency = defaultdict(int)\n    for word in word_dict:\n        for letter in word:\n            frequency[letter] += 1\n    return frequency"
  },
  {
    "output": "import numpy as np\nimport math\n\ndef task_func(L):\n    flat_list = []\n    for sublist in L:\n        for item in sublist:\n            flat_list.append(item)\n    median = np.median(flat_list)\n    return median"
  },
  {
    "output": "def task_func(file_path, goals, penalties):\n    import csv\n    from collections import Counter\n\n    with open(file_path, 'r') as file:\n        reader = csv.DictReader(file)\n        data = [row for row in reader]\n\n    total_goals = int(data[0]['goals']) + goals\n    total_penalties = int(data[0]['penalties']) + penalties\n\n    return Counter({'total_goals': total_goals, 'total_penalties': total_penalties})"
  },
  {
    "output": "def task_func(file_path):\n    import os\n    import csv\n    import random\n    if not os.path.exists(file_path) or not file_path.endswith('.csv'):\n        return []\n    with open(file_path, 'r') as f:\n        lines = f.readlines()\n    random.shuffle(lines)\n    split_files = []\n    for i in range(0, len(lines), 1000):\n        chunk = lines[i:i+1000]\n        split_file = f'split_{i}.csv'\n        with open(split_file, 'w') as f:\n            f.writelines(chunk)\n        split_files.append(split_file)\n    return split_files"
  },
  {
    "output": "from collections import Counter\nimport random\n\ndef generate_and_analyze_letter_pairs():\n    predefined_list = ['AA', 'AB', 'AC', 'AD', 'AE', 'AF', 'AG', 'AH', 'AI', 'AJ', 'AK', 'AL', 'AM', 'AN', 'AO', 'AP', 'AQ', 'AR', 'AS', 'AT', 'AU', 'AV', 'AW', 'AX', 'AY', 'AZ', 'BA', 'BB', 'BC', 'BD', 'BE', 'BF', 'BG', 'BH', 'BI', 'BJ', 'BK', 'BL', 'BM', 'BN', 'BO', 'BP', 'BQ', 'BR', 'BS', 'BT', 'BU', 'BV', 'BW', 'BX', 'BY', 'BZ']\n    sample_size = 1000\n    random_pairs = [random.choice(predefined_list) for _ in range(sample_size)]\n    frequency = Counter(random_pairs)\n    return frequency"
  },
  {
    "output": "def plot_histogram_with_normal(df, column):\n    import matplotlib.pyplot as plt\n    import numpy as np\n    import pandas as pd\n    fig, ax = plt.subplots()\n    data = df[column]\n    n, bins, patches = ax.hist(data, bins=30, density=True, edgecolor='black')\n    mu, std = np.mean(data), np.std(data)\n    pdf = np.exp(-((bins - mu)**2)/(2*std**2)) / (std*np.sqrt(2*np.pi))\n    ax.plot(bins, pdf, 'r-', lw=2)\n    return ax"
  },
  {
    "output": "def task_func(file_path, delimiter):\n    import os\n    import shutil\n    parts = file_path.split(delimiter)\n    for part in parts:\n        if not os.path.exists(part):\n            raise FileNotFoundError(f\"Directory {part} does not exist\")\n    total_size = 0\n    for part in parts:\n        total_size += shutil.disk_usage(part).used\n    return total_size"
  },
  {
    "output": "import pandas as pd\nimport matplotlib.pyplot as plt\nimport os\n\ndef analyze_csv_data(csv_path, plot_path):\n    if not os.path.exists(csv_path):\n        raise FileNotFoundError(f\"The file {csv_path} does not exist.\")\n    df = pd.read_csv(csv_path)\n    df = df.dropna()\n    df = df.apply(pd.to_numeric, errors='coerce')\n    df = df.dropna()\n    numeric_data = df.values.flatten()\n    mean = numeric_data.mean()\n    median = numeric_data.median()\n    plt.plot(numeric_data)\n    plt.title(\"Data Distribution\")\n    plt.xlabel(\"Index\")\n    plt.ylabel(\"Value\")\n    plt.savefig(plot_path)\n    plt.close()\n    return (mean, median, plot_path)"
  },
  {
    "output": "def task_func(l=None):\n    if l is None:\n        l = random.sample(ELEMENTS, 10)\n    arr = np.array(l)\n    if len(arr) < 3:\n        return arr\n    return np.concatenate((arr[3:], arr[:3]))"
  },
  {
    "output": "def task_func(python_version=PYTHON_VERSION, path_to_append=PATH_TO_APPEND):\n    current_version = sys.version.split()[0]\n    if current_version != python_version:\n        try:\n            subprocess.check_output(['hash', '-r'], stderr=subprocess.STDOUT, shell=True)\n            subprocess.check_output([f'hash -d python'], stderr=subprocess.STDOUT, shell=True)\n            subprocess.check_output([f'hash -d python3'], stderr=subprocess.STDOUT, shell=True)\n            subprocess.check_output([f'hash -d python{python_version[0]}'], stderr=subprocess.STDOUT, shell=True)\n            subprocess.check_output([f'hash -d python{python_version[0]}'], stderr=subprocess.STDOUT, shell=True)\n            subprocess.check_output([f'hash -d python{python_version[0]}'], stderr=subprocess.STDOUT, shell=True)\n            subprocess.check_output([f'hash -d python{python_version[0]}'], stderr=subprocess.STDOUT, shell=True)"
  },
  {
    "output": "def calculate_letter_frequencies(s):\n    if not isinstance(s, str):\n        raise ValueError(\"Input must be a string\")\n    from collections import Counter\n    import matplotlib.pyplot as plt\n    counts = Counter()\n    for char in s:\n        if char.isalpha():\n            counts[char.lower()] += 1\n    letters = list(counts.keys())\n    frequencies = list(counts.values())\n    plt.bar(letters, frequencies)\n    plt.xlabel('Letters')\n    plt.ylabel('Frequency')\n    plt.title('Letter Frequency')\n    plt.show()"
  },
  {
    "output": "import time\nimport threading\n\ndef task_func(delay_time: float = 1.0, num_threads: int = 5):\n    completion_messages = []\n    lock = threading.Lock()\n    \n    def thread_target(thread_id):\n        time.sleep(delay_time)\n        with lock:\n            completion_messages.append(f'Delay in thread {thread_id} completed')\n    \n    threads = []\n    for i in range(num_threads):\n        thread = threading.Thread(target=thread_target, args=(i,))\n        threads.append(thread)\n        thread.start()\n    \n    for thread in threads:\n        thread.join()\n    \n    return completion_messages"
  },
  {
    "output": "return ax, empirical_mean, empirical_std"
  },
  {
    "output": "import pandas as pd\nimport collections\n\ndef task_func(df):\n    if not isinstance(df, pd.DataFrame):\n        raise ValueError(\"Input must be a DataFrame.\")\n    total_sales = df['Sales'].sum()\n    category_sales = df.groupby('Category')['Sales'].sum().reset_index()\n    category_sales.sort_values(by=['Sales', 'Category'], ascending=[False, True], inplace=True)\n    most_popular_category = category_sales.iloc[0]['Category']\n    return {\n        'Total Sales': total_sales,\n        'Most Popular Category': most_popular_category\n    }"
  },
  {
    "output": "def create_series_list(list_of_lists):\n    import pandas as pd\n    import numpy as np\n    result = []\n    for sublist in list_of_lists:\n        indices = sublist\n        values = np.random.permutation(range(1, len(indices) + 1))\n        series = pd.Series(values, index=indices)\n        result.append(series)\n    return result"
  },
  {
    "output": "def execute_script_in_temp_dir(script_content):\n    import tempfile\n    import os\n    import subprocess\n    import uuid\n    temp_dir = tempfile.mkdtemp()\n    script_path = os.path.join(temp_dir, f\"script_{uuid.uuid4().hex}.py\")\n    with open(script_path, 'w') as f:\n        f.write(script_content)\n    try:\n        result = subprocess.run(['python', script_path], capture_output=True, text=True, check=True)\n        return 0\n    except subprocess.CalledProcessError as e:\n        return 1\n    finally:\n        os.rmdir(temp_dir)"
  },
  {
    "output": "def convert_dict_files(directory):\n    import os\n    import json\n    dicts = []\n    for filename in os.listdir(directory):\n        file_path = os.path.join(directory, filename)\n        if os.path.isfile(file_path) and filename.endswith('.txt'):\n            try:\n                with open(file_path, 'r', encoding='utf-8') as f:\n                    content = f.read()\n                try:\n                    dicts.append(json.loads(content))\n                except json.JSONDecodeError:\n                    raise ValueError(f\"Invalid JSON in file: {file_path}\")\n            except Exception as e:\n                raise ValueError(f\"Error reading file {file_path}: {str(e)}\")\n    return dicts"
  },
  {
    "output": "def generate_sales_report():\n    import pandas as pd\n    import random\n    data = {\n        'Product': [f'Product {i}' for i in range(1, 11)],\n        'Units Sold': [random.randint(50, 500) for _ in range(10)],\n        'Price': [round(random.uniform(10, 100), 2) for _ in range(10)],\n        'Cost': [round(random.uniform(5, 50), 2) for _ in range(10)]\n    }\n    df = pd.DataFrame(data)\n    df['Revenue'] = df['Units Sold'] * df['Price']\n    df['Cost'] = df['Units Sold'] * df['Cost']\n    df['Profit'] = df['Revenue'] - df['Cost']\n    return df.to_string(index=False)"
  },
  {
    "output": "import pandas as pd\nimport numpy as np\n\ndef task_func(rows=100, columns=3):\n    data = np.random.choice(list('abcdefghijklmnopqrstuvwxyz'), size=(rows, columns))\n    columns_names = [chr(ord('a') + i) for i in range(columns)]\n    df = pd.DataFrame(data, columns=columns_names)\n    return df"
  },
  {
    "output": "import matplotlib.pyplot as plt\nimport pandas as pd\n\ndef process_dataframe(df, target_values):\n    for col in df.columns:\n        df[col] = df[col].apply(lambda x: x if x in target_values else 0)\n    axes = []\n    for col in df.columns:\n        ax = plt.subplot(len(df.columns), 1, df.columns.get_loc(col) + 1)\n        df[col].plot(kind='hist', ax=ax)\n        ax.set_title(col)\n        axes.append(ax)\n    return axes"
  },
  {
    "output": "def task_func(file_path: str, column_name: str) -> pd.DataFrame:\n    df = pd.read_csv(file_path)\n    df[column_name] = df[column_name].str.replace('\\n', '<br>')\n    le = LabelEncoder()\n    df[column_name] = le.fit_transform(df[column_name])\n    return df"
  },
  {
    "output": "import pandas as pd\nimport numpy as np\n\ndef generate_random_student_data(num_students):\n    np.random.seed(42)\n    data = {\n        'StudentID': np.random.randint(1000, 9999, size=num_students),\n        'Name': [f\"Student {i}\" for i in range(1, num_students + 1)],\n        'Age': np.random.randint(18, 30, size=num_students),\n        'GPA': np.round(np.random.uniform(2.0, 4.0, size=num_students), 2),\n        'Major': np.random.choice(['Computer Science', 'Mathematics', 'Physics', 'Engineering'], size=num_students)\n    }\n    return pd.DataFrame(data)"
  },
  {
    "output": "def task_func(api_url):\n    import pandas as pd\n    import matplotlib.pyplot as plt\n    import requests\n    if not isinstance(api_url, str):\n        raise ValueError(\"api_url must be a string\")\n    try:\n        response = requests.get(api_url)\n        response.raise_for_status()\n    except requests.exceptions.RequestException as e:\n        raise Exception(\"API request failed\") from e\n    try:\n        data = response.json()\n    except Exception as e:\n        raise Exception(\"Failed to parse JSON response\") from e\n    if not data:\n        raise ValueError(\"API returned empty data\")\n    df = pd.DataFrame(data)\n    if not df.empty:\n        df.plot()\n        plt.show()\n    else:\n        raise ValueError(\"DataFrame is empty after processing\")"
  },
  {
    "output": "linestyle='-', color='r')\n    plt.title('Transformed Signal')\n    plt.xlabel('Frequency Bin')\n    plt.ylabel('Magnitude')\n    plt.tight_layout()\n    plt.show()\n    \n    return transformed_signal, rounded_transformed_signal"
  },
  {
    "output": "import os\nimport re\nfrom collections import defaultdict\n\ndef organize_files(directory):\n    subdirs = defaultdict(list)\n    pattern = re.compile(r'(?:(?<!\\])[^[\\]]+)(?=\\[|$)')\n    for filename in os.listdir(directory):\n        match = pattern.search(filename)\n        if match:\n            subdir = match.group(1)\n            filepath = os.path.join(directory, filename)\n            subdirs[subdir].append(filepath)\n    for subdir, files in subdirs.items():\n        subdir_path = os.path.join(directory, subdir)\n        os.makedirs(subdir_path, exist_ok=True)\n        for file in files:\n            os.rename(file, os.path.join(subdir_path, os.path.basename(file)))\n    return (directory, dict(subdirs))"
  },
  {
    "output": "def create_value_distribution_chart(df):\n    import pandas as pd\n    import matplotlib.pyplot as plt\n    if not isinstance(df, pd.DataFrame):\n        raise ValueError(\"Input must be a pandas DataFrame\")\n    counts = df['value'].value_counts().sort_index()\n    fig, ax = plt.subplots()\n    counts.plot(kind='bar', ax=ax)\n    ax.set_title('Value Distribution')\n    ax.set_xlabel('Value')\n    ax.set_ylabel('Count')\n    return ax"
  },
  {
    "output": "import numpy as np\nimport pandas as pd\nimport csv\nimport os\n\ndef generate_and_write_matrix(filename, rows, cols):\n    letters = [chr(i) for i in range(ord('a'), ord('z') + 1)]\n    matrix = np.random.choice(letters, size=(rows, cols))\n    df = pd.DataFrame(matrix)\n    df.to_csv(filename, index=False, header=False)"
  },
  {
    "output": "import json\nimport csv\nimport os\nimport base64\n\ndef task_func(raw_string, filename, output_dir):\n    decoded_data = base64.b64decode(raw_string).decode('utf-8')\n    json_data = json.loads(decoded_data)\n    os.makedirs(output_dir, exist_ok=True)\n    file_path = os.path.join(output_dir, filename)\n    with open(file_path, 'w', newline='') as csvfile:\n        csv_writer = csv.writer(csvfile)\n        csv_writer.writerow(json_data.keys())\n        csv_writer.writerow(json_data.values())"
  },
  {
    "output": "def analyze_fitness_data(data):\n    import numpy as np\n    import matplotlib.pyplot as plt\n    if not data or not all(isinstance(x, (int, float)) for x in data):\n        raise ValueError(\"Invalid data format\")\n    mean = np.mean(data)\n    std_dev = np.std(data)\n    plt.plot(data)\n    plt.title(\"Fitness Data\")\n    plt.xlabel(\"Sample\")\n    plt.ylabel(\"Value\")\n    plt.show()\n    return mean, std_dev"
  },
  {
    "output": "def generate_random_csv(file_path, num_rows, num_cols, min_val, max_val):\n    import pandas as pd\n    import numpy as np\n    data = np.random.uniform(min_val, max_val, (num_rows, num_cols))\n    df = pd.DataFrame(data, columns=[f'col_{i}' for i in range(num_cols)])\n    df.to_csv(file_path, index=False)"
  },
  {
    "output": "```python\nimport pandas as pd\nfrom sklearn.preprocessing import MinMaxScaler\n\ndef task_func(d):\n    if not d:\n        return pd.DataFrame()\n    df = pd.DataFrame(d)\n    scaler = MinMaxScaler()\n    scaled_data = scaler.fit_transform(df[['x', 'y', 'z']])\n    scaled_df = pd.DataFrame(scaled_data, columns=['x', 'y', 'z'])\n    return scaled_df\n```"
  },
  {
    "output": "def create_app(templates_folder, external_api_url):\n    from flask import Flask, jsonify\n    app = Flask(__name__, template_folder=templates_folder)\n    \n    @app.route('/data')\n    def get_data():\n        import requests\n        response = requests.get(external_api_url)\n        return jsonify(response.json())\n    \n    return app"
  },
  {
    "output": "def normalize_and_visualize(data):\n    import numpy as np\n    import pandas as pd\n    import matplotlib.pyplot as plt\n    \n    df = pd.DataFrame(data)\n    min_vals = df.min()\n    max_vals = df.max()\n    normalized = (df - min_vals) / (max_vals - min_vals)\n    row_averages = normalized.mean(axis=1)\n    normalized['average'] = row_averages\n    normalized.plot(kind='line')\n    plt.show()"
  },
  {
    "output": "import pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\ndef task_func(data):\n    corr = data.corr()\n    plt.figure(figsize=(10, 8))\n    ax = sns.heatmap(corr, annot=True, fmt=\".2f\", cmap='coolwarm', square=True)\n    ax.set_title('Correlation Matrix')\n    return ax"
  },
  {
    "output": "def task_func(data=None):\n    if not isinstance(data, pd.DataFrame):\n        data = pd.DataFrame(data)\n    data = data.applymap(lambda x: 0 if x < 0.5 else x)\n    scaler = StandardScaler()\n    data_scaled = scaler.fit_transform(data)\n    data_preprocessed = pd.DataFrame(data_scaled, columns=data.columns)\n    return data_preprocessed"
  },
  {
    "output": "def find_target_in_csv(directory, target, move_to=None):\n    import os\n    import csv\n    import shutil\n    files = [f for f in os.listdir(directory) if f.endswith('.csv')]\n    results = []\n    for file in files:\n        file_path = os.path.join(directory, file)\n        with open(file_path, 'r') as f:\n            reader = csv.reader(f)\n            for idx, row in enumerate(reader):\n                if row and row[0] == target:\n                    results.append((file, idx))\n                    break\n        if move_to and os.path.exists(file_path):\n            shutil.move(file_path, os.path.join(move_to, file))\n    return results"
  },
  {
    "output": "(train_set, test_set)"
  },
  {
    "output": "def extract_and_calculate_tfidf(text):\n    import re\n    from sklearn.feature_extraction.text import TfidfVectorizer\n    import numpy as np\n    cleaned_text = re.sub(r'\\[.*?\\]', ' ', text)\n    vectorizer = TfidfVectorizer()\n    tfidf_matrix = vectorizer.fit_transform([cleaned_text])\n    feature_names = vectorizer.get_feature_names_out()\n    tfidf_dict = {word: tfidf_matrix[0, i] for i, word in enumerate(feature_names)}\n    return tfidf_dict"
  },
  {
    "output": "from collections import defaultdict\nimport re\n\ndef task_func(text):\n    words = re.findall(r'\\b\\w+\\b', text)\n    if not words:\n        return {}\n    stemmed_words = [word[:-1] if i != len(words)-1 else word for i, word in enumerate(words)]\n    freq = defaultdict(int)\n    for word in stemmed_words:\n        freq[word] += 1\n    return dict(freq)"
  },
  {
    "output": "```python\nimport pandas as pd\nimport numpy as np\n\ndef task_func(test_scores, student):\n    # Convert the dictionary to a DataFrame\n    df = pd.DataFrame(test_scores)\n    \n    # Check if the student exists in the DataFrame\n    if student not in df.columns:\n        raise ValueError(\"student is not present in the"
  },
  {
    "output": "import matplotlib.pyplot as plt\nimport numpy as np\ndef draw_histogram_with_gaussian_fit(data, title):\n    plt.hist(data, bins=30, density=True, alpha=0.6, color='g')\n    mu, sigma = np.mean(data), np.std(data)\n    x = np.linspace(mu - 3*sigma, mu + 3*sigma, 100)\n    plt.plot(x, 1/(sigma*np.sqrt(2*np.pi)) * np.exp(- (x - mu)**2 / (2*sigma**2)), 'r--')\n    plt.title(title)\n    plt.show()"
  },
  {
    "output": "import numpy as np\nimport matplotlib.pyplot as plt\n\ndef process_fft(data):\n    if not data:\n        return np.array([]), np.array([]), None\n    arr = np.array(data)\n    fft_data = np.fft.fft(arr)\n    plt.hist(np.abs(fft_data), bins=20)\n    fig = plt.gcf()\n    return arr, fft_data, fig.axes[0] if fig.axes else None"
  },
  {
    "output": "def task_func(directory):\n    import os\n    import json\n    processed_files = []\n    for filename in os.listdir(directory):\n        file_path = os.path.join(directory, filename)\n        try:\n            with open(file_path, 'r') as f:\n                data = json.load(f)\n            with open(file_path, 'w') as f:\n                json.dump(data, f, ensure_ascii=False)\n            processed_files.append(filename)\n        except FileNotFoundError:\n            pass\n    return processed_files"
  },
  {
    "output": "import matplotlib.pyplot as plt\nimport numpy as np\n\ndef plot_sales_trends_with_std_dev(sales_data):\n    fig, ax = plt.subplots()\n    products = list(sales_data.keys())\n    dates = np.arange(1, 13)\n    for i, product in enumerate(products):\n        sales = sales_data[product]\n        mean = np.mean(sales)\n        std_dev = np.std(sales)\n        ax.plot(dates, sales, label=product)\n        ax.fill_between(dates, mean - std_dev, mean + std_dev, alpha=0.3)\n    ax.set_xlabel('Month')\n    ax.set_ylabel('Sales')\n    ax.set_title('Sales Trends with Standard Deviation')\n    ax.legend()\n    return ax"
  },
  {
    "output": "import numpy as np\nimport matplotlib.pyplot as plt\n\ndef generate_complex_wave_with_hann_window(amplitude, frequency, time_points):\n    wave = amplitude * np.exp(1j * 2 * np.pi * frequency * time_points)\n    hann_window = 0.5 * (1 - np.cos(2 * np.pi * np.arange(len(time_points)) / (len(time_points) - 1)))\n    wave *= hann_window\n    fig, ax = plt.subplots(2, 1, figsize=(10, 6))\n    ax[0].plot(time_points, np.real(wave), label='Real Part')\n    ax[0].set_title('Real Part of Complex Wave')\n    ax[0].grid(True)\n    ax[1].plot(time_points, np.imag(wave), label='Imaginary Part')\n    ax[1].set_title('Imaginary Part of Complex Wave')\n    ax[1].grid(True)\n    plt.tight_layout()\n    return wave, fig, ax"
  },
  {
    "output": "def task_func(url):\n    import subprocess\n    import platform\n    import time\n    if platform.system() == \"Windows\":\n        command = [\"start\", \"\", url]\n    elif platform.system() == \"Darwin\":\n        command = [\"open\", url]\n    elif platform.system() == \"Linux\":\n        command = [\"xdg-open\", url]\n    else:\n        raise OSError(\"Unsupported operating system\")\n    process = subprocess.Popen(command, stdout=subprocess.PIPE, stderr=subprocess.PIPE, stdin=subprocess.PIPE, start_new_session=True)\n    time.sleep(1)\n    return process.returncode"
  },
  {
    "output": "import random\nimport statistics\n\ndef create_population_std_dict(data):\n    return {key: statistics.pstdev(values) for key, values in data.items()}"
  },
  {
    "output": "def decode_unicode_strings(df):\n    import pandas as pd\n    import re\n    import unicodedata\n    def decode_unicode(s):\n        try:\n            return s.encode('utf-8').decode('unicode_escape').encode('latin1').decode('utf-8')\n        except Exception as e:\n            return str(e)\n    df['UnicodeString'] = df['UnicodeString'].apply(decode_unicode)\n    return df"
  },
  {
    "output": "def process_data(d):\n    if not isinstance(d, list) or any(not isinstance(item, dict) for item in d):\n        raise ValueError(\"Input must be a list of dictionaries\")\n    stats = {}\n    for key in ['x', 'y', 'z']:\n        values = [item[key] for item in d]\n        mean = sum(values) / len(values)\n        sum_val = sum(values)\n        max_val = max(values)\n        min_val = min(values)\n        variance = sum((x - mean)**2 for x in values) / len(values)\n        std_dev = variance**0.5\n        stats[key] = {\n            'mean': mean,\n            'sum': sum_val,\n            'max': max_val,\n            'min': min_val,\n            'std_dev': std_dev\n        }\n    return stats"
  },
  {
    "output": "True"
  },
  {
    "output": "def task_func(X, y):\n    from sklearn.model_selection import train_test_split\n    from tensorflow.keras.models import Sequential\n    from tensorflow.keras.layers import Dense\n    from tensorflow.keras.optimizers import SGD\n    from sklearn.metrics import precision_recall_curve\n    import matplotlib.pyplot as plt\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n    model = Sequential()\n    model.add(Dense(10, activation='sigmoid', input_shape=(X.shape[1],)))\n    model.compile(loss='binary_crossentropy', optimizer=SGD(), metrics=['accuracy'])\n    model.fit(X_train, y_train, epochs=10, batch_size=32, validation_split=0.1, verbose=0)\n    y_scores = model.predict(X_test)\n    precision, recall, _ = precision_recall_curve(y_test, y_scores)\n    fig, ax = plt.subplots()\n    ax.plot(recall, precision)\n    ax.set_xlabel('Recall')\n    ax.set_ylabel('Precision')\n    return model, ax"
  },
  {
    "output": "def task_func(**kwargs):\n    import os\n    import shutil\n    target_dir = kwargs.get('target_dir', '.')\n    copied_files = []\n    for file_path, source_path in kwargs.items():\n        if os.path.exists(source_path):\n            if os.path.isfile(source_path):\n                shutil.copy2(source_path, target_dir)\n                copied_files.append(os.path.join(target_dir, os.path.basename(source_path)))\n    return copied_files"
  },
  {
    "output": "import random\n\nLETTERS = ['a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j']\n\ndef task_func(n_keys, n_values):\n    n_keys = min(n_keys, len(LETTERS))\n    selected_keys = random.sample(LETTERS, n_keys)\n    values = list(range(1, n_values + 1))\n    result_dict = {key: values for key in selected_keys}\n    return result_dict"
  },
  {
    "output": "import os\nimport requests\nimport zipfile\n\ndef task_func(url, extract_to):\n    zip_path = os.path.join(extract_to, os.path.basename(url))\n    response = requests.get(url)\n    with open(zip_path, 'wb') as f:\n        f.write(response.content)\n    with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n        zip_ref.extractall(extract_to)\n    os.remove(zip_path)\n    return extract_to"
  },
  {
    "output": "import matplotlib.pyplot as plt\nimport seaborn as sns\ndef task_func(dataframe, target_value='332'):\n    # Create a Boolean DataFrame indicating the presence of the target value\n    boolean_df = dataframe == target_value\n    \n    # Create a heatmap using seaborn\n    plt.figure(figsize=(10, 8))\n    ax = sns.heatmap(boolean_df, annot=True, cmap='viridis', cbar=False)\n    \n    # Return the tuple\n    return boolean_df, ax"
  },
  {
    "output": "import pandas as pd\nfrom sklearn.preprocessing import MinMaxScaler\n\ndef normalize_csv_data(file_path):\n    df = pd.read_csv(file_path)\n    numeric_cols = df.select_dtypes(include=['number']).columns\n    if len(numeric_cols) == 0:\n        raise ValueError(\"No numeric columns found\")\n    scaler = MinMaxScaler()\n    df[numeric_cols] = scaler.fit_transform(df[numeric_cols])\n    return df.to_csv(index=False)"
  },
  {
    "output": "from pathlib import Path\nimport zipfile\n\ndef task_func(directory_path):\n    directory = Path(directory_path)\n    if not directory.exists():\n        raise FileNotFoundError\n    files = list(directory.iterdir())\n    if not any(file.is_file() for file in files):\n        return None\n    zip_path = directory / f\"{directory.name}.zip\"\n    with zipfile.ZipFile(zip_path, 'w') as zipf:\n        for file in files:\n            if file.is_file():\n                zipf.write(file, arcname=file.name)\n    return str(zip_path)"
  },
  {
    "output": "import os\nimport sqlite3\nimport pandas as pd\nimport re\n\ndef task_func(database_path, table_name, column_name, regex_pattern):\n    if not os.path.exists(database_path):\n        return pd.DataFrame()\n    conn = sqlite3.connect(database_path)\n    query = f\"SELECT {column_name} FROM {table_name}\"\n    df = pd.read_sql_query(query, conn)\n    conn.close()\n    matches = df[column_name].apply(lambda x: re.findall(regex_pattern, str(x)))\n    matches_df = pd.DataFrame(matches)\n    return matches_df"
  },
  {
    "output": "import binascii\nimport urllib.parse\n\ndef task_func(url):\n    parsed_url = urllib.parse.urlparse(url)\n    query_params = urllib.parse.parse_qs(parsed_url.query)\n    if 'q' in query_params:\n        hex_string = query_params['q'][0]\n        try:\n            decoded_bytes = binascii.unhexlify(hex_string)\n            decoded_str = decoded_bytes.decode('utf-8')\n            return decoded_str\n        except (binascii.Error, UnicodeDecodeError):\n            return None\n    return None"
  },
  {
    "output": "def task_func(param1, param2):\n    return param1 + param2"
  },
  {
    "output": "import numpy as np\nimport matplotlib.pyplot as plt\n\ndef fit_and_plot_quadratic(X, Y):\n    X = np.array(X).reshape(-1, 1)\n    Y = np.array(Y)\n    A = np.c_[np.ones(X.shape[0]), X, X**2]\n    params, _ = np.linalg.lstsq(A, Y, rcond=None)\n    x_fit = np.linspace(min(X), max(X), 100)\n    y_fit = params[0] + params[1] * x_fit + params[2] * x_fit**2\n    fig, ax = plt.subplots()\n    ax.scatter(X, Y, color='blue')\n    ax.plot(x_fit, y_fit, color='red')\n    return params, ax"
  },
  {
    "output": "def download_and_extract_zip(url, extract_to):\n    import os\n    import requests\n    import zipfile\n    response = requests.get(url)\n    response.raise_for_status()\n    zip_path = os.path.join(extract_to, \"downloaded_file.zip\")\n    with open(zip_path, 'wb') as f:\n        f.write(response.content)\n    with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n        zip_ref.extractall(extract_to)\n    extracted_files = [os.path.join(extract_to, f) for f in zip_ref.namelist()]\n    os.remove(zip_path)\n    return extracted_files"
  },
  {
    "output": "def find_latest_log_file(directory, pattern):\n    import re\n    import os\n    files = [f for f in os.listdir(directory) if os.path.isfile(os.path.join(directory, f))]\n    regex = re.compile(pattern)\n    matches = [f for f in files if regex.search(f)]\n    if not matches:\n        return None\n    latest = max(matches, key=lambda x: os.path.getmtime(os.path.join(directory, x)))\n    return os.path.join(directory, latest)"
  },
  {
    "output": "import subprocess\nimport random\n\nSCRIPTS = ['script1.sh', 'script2.sh', 'script3.sh']\nSCRIPTS_DIR = '/path/to/scripts'  \n\ndef task_func():\n    selected_script = random.choice(SCRIPTS)\n    full_path = f\"{SCRIPTS_DIR}/{selected_script}\"\n    subprocess.run([full_path], check=True)\n    return full_path"
  },
  {
    "output": "from collections import Counter\nimport re\nfrom nltk.corpus import stopwords\nfrom nltk import ngrams\n\ndef task_func(text, n=2):\n    text = re.sub(r'\\W+', ' ', text).lower()\n    stop_words = set(stopwords.words('english'))\n    words = [word for word in text.split() if word not in stop_words]\n    grams = ngrams(words, n)\n    return dict(Counter(grams))"
  },
  {
    "output": "import numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import mean_squared_error\n\ndef task_func(X, y):\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n    model = LinearRegression()\n    model.fit(X_train, y_train)\n    y_pred = model.predict(X_test)\n    mse = mean_squared_error(y_test, y_pred)\n    return mse"
  },
  {
    "output": "def task_func(db_path, users, salt):\n    import sqlite3\n    import hashlib\n    conn = sqlite3.connect(db_path)\n    cursor = conn.cursor()\n    updated_count = 0\n    for user in users:\n        if not isinstance(salt, str):\n            raise TypeError(\"Salt must be a string\")\n        hashed_password = hashlib.sha256(salt.encode() + user['password'].encode()).hexdigest()\n        cursor.execute(\"UPDATE user SET password = ? WHERE id = ?\", (hashed_password, user['id']))\n        updated_count += 1\n    conn.commit()\n    conn.close()\n    return updated_count"
  },
  {
    "output": "def calculate_pearson_and_plot(data):\n    import pandas as pd\n    import numpy as np\n    import matplotlib.pyplot as plt\n    df = pd.DataFrame(data)\n    x = df.iloc[:, 0]\n    y = df.iloc[:, 1]\n    mean_x = np.mean(x)\n    mean_y = np.mean(y)\n    numerator = np.sum((x - mean_x) * (y - mean_y))\n    denominator = np.sqrt(np.sum((x - mean_x)**2) * np.sum((y - mean_y)**2))\n    pearson = numerator / denominator\n    plt.scatter(x, y)\n    m, b = np.polyfit(x, y, 1)\n    plt.plot(x, m*x + b, color='red')\n    plt.xlabel('X')\n    plt.ylabel('Y')\n    plt.title('Scatter plot with regression line')\n    plt.show()\n    return pearson"
  },
  {
    "output": "def task_func(df):\n    pca = PCA(n_components=2)\n    df_pca = pca.fit_transform(df)\n    df_pca = pd.DataFrame(df_pca, columns=['PC1', 'PC2'])\n    return df_pca"
  },
  {
    "output": "import pandas as pd\nimport matplotlib.pyplot as plt\n\ndef process_dataframe(df):\n    df['row_average'] = df.mean(axis=1)\n    df.plot(x=range(len(df)), y='row_average', kind='line')\n    plt.show()"
  },
  {
    "output": "import pandas as pd\nimport matplotlib.pyplot as plt\n\ndef task_func(df, bins=20):\n    if not isinstance(df, pd.DataFrame):\n        raise ValueError(\"Input is not a DataFrame\")\n    if df.empty:\n        raise ValueError(\"DataFrame is empty\")\n    \n    last_column = df.columns[-1]\n    ax = df[last_column].plot(kind='hist', bins=bins)\n    ax.set_title(f'Histogram of {last_column}')\n    ax.set_xlabel('Value')\n    ax.set_ylabel('Frequency')\n    return ax"
  },
  {
    "output": "```python\nimport pandas as pd\nimport os\nimport numpy as np\nimport ast\n\ndef task_func(directory):\n    # Check if directory exists\n    if not os.path.exists(directory):\n        return pd.DataFrame(columns=['email', 'list', 'sum', 'mean', 'median']), None\n    \n    # Traverse directory for CSV files\n    csv_files = [f for f in os.listdir(directory) if f.endswith('.csv')]\n    \n    # If no CSV files, return empty DataFrame and None\n    if not csv_files:\n        return pd.DataFrame(columns=['email', 'list', 'sum', 'mean', 'median']), None\n    \n    # Find CSV file with the longest filename\n    longest_file = max(csv_files, key=lambda x: len(x))\n    file_path = os.path.join(directory, longest_file)\n    \n    #"
  },
  {
    "output": "import matplotlib.pyplot as plt\nfrom collections import Counter\n\nFRUITS = ['Apple', 'Banana', 'Cherry', 'Date', 'Elderberry', 'Fig', 'Grape', 'Honeydew', 'Indian Prune', 'Jackfruit']\n\ndef task_func(fruit_dict):\n    fruit_counts = Counter(fruit_dict.values())\n    fruits = list(fruit_counts.keys())\n    counts = list(fruit_counts.values())\n    fig, ax = plt.subplots()\n    ax.bar(fruits, counts)\n    ax.set_xlabel('Fruit')\n    ax.set_ylabel('Frequency')\n    ax.set_title('Fruit Frequency')\n    return fruit_counts, ax"
  },
  {
    "output": "import pandas as pd\nimport os\n\nOUTPUT_DIR = './output'\n\ndef task_func(df, filename, output_dir=OUTPUT_DIR):\n    os.makedirs(output_dir, exist_ok=True)\n    file_path = os.path.join(output_dir, filename)\n    df.to_json(file_path, orient='records')\n    return file_path"
  },
  {
    "output": "import numpy as np\nimport matplotlib.pyplot as plt\n\ndef generate_exponential_values(start, end, step):\n    for x in np.arange(start, end, step):\n        yield (x, np.exp(x))\n\ndef plot_exponential_function(values):\n    xs, es = zip(*values)\n    plt.plot(xs, es)\n    plt.xlabel('x')\n    plt.ylabel('e^x')\n    plt.title('Exponential Function')\n    plt.grid(True)\n    plt.show()"
  },
  {
    "output": "import pandas as pd\nimport random\n\ndef task_func(product_list, categories, min_value=10, max_value=100):\n    data = []\n    for product in product_list:\n        category = random.choice(categories)\n        quantity_sold = random.randint(min_value, max_value)\n        revenue = quantity_sold * random.randint(min_value, max_value)\n        data.append({\n            'Product': product,\n            'Category': category,\n            'Quantity Sold': quantity_sold,\n            'Revenue': revenue,\n            'Total Revenue': revenue\n        })\n    df = pd.DataFrame(data)\n    return df"
  },
  {
    "output": "def task_func(text):\n    from textblob import TextBlob\n    from nltk.corpus import stopwords\n    from nltk.tokenize import word_tokenize\n    import re\n\n    stop_words = set(stopwords.words('english'))\n    words = word_tokenize(text.lower())\n    filtered = [word for word in words if word.isalpha() and word not in stop_words]\n    cleaned_text = ' '.join(filtered)\n    return TextBlob(cleaned_text).sentiment.polarity"
  },
  {
    "output": "from scipy import fftpack\nfrom matplotlib import pyplot as plt\nimport numpy as np\n\ndef task_func(arr):\n    row_sums = np.sum(arr, axis=1)\n    fft_coefficients = fftpack.fft(row_sums)\n    abs_fft = np.abs(fft_coefficients)\n    fig, ax = plt.subplots()\n    ax.plot(abs_fft)\n    ax.set_title(\"Absolute Values of FFT Coefficients\")\n    ax.set_xlabel(\"Frequency\")\n    ax.set_ylabel(\"Magnitude\")\n    return ax"
  },
  {
    "output": "```python\nimport pandas as pd\nfrom scipy.stats import skew\n\ndef task_func(df):\n    if not isinstance(df, pd.DataFrame) or df.shape[1] == 0:\n        raise ValueError(\"Input is not a DataFrame or has no columns.\")\n    last_column = df.iloc[:, -1]\n    return float(skew(last_column))\n```"
  },
  {
    "output": "import pandas as pd\n\ndef process_json_data(input_file, output_file, key_to_remove):\n    df = pd.read_json(input_file, orient='records')\n    df = df.drop(columns=[key_to_remove], errors='ignore')\n    df.to_json(output_file, orient='records', indent=4)"
  },
  {
    "output": "```python\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Constants\nCOLORS = ['r', 'g', 'b']\n\ndef task_func(df, group_col, value_col):\n    # Check if the value column contains non-numeric values\n    if not pd.api.types.is_numeric_dtype(df[value_col]):\n        raise TypeError(\""
  },
  {
    "output": "def task_func(url):\n    urllib.request.urlretrieve(url, 'downloaded_file.txt')\n    with open('downloaded_file.txt', 'r') as file:\n        content = file.read()\n    occurrences = len(re.findall(r'\\bERROR\\b', content))\n    print(occurrences)\n    os.remove('downloaded_file.txt')\n    return occurrences"
  },
  {
    "output": "import pandas as pd\nimport matplotlib.pyplot as plt\n\ndef create_dataframe_and_scatter_plot(data):\n    df = pd.DataFrame(data)\n    plt.scatter(df.iloc[:, 0], df.iloc[:, 1])\n    plt.show()"
  },
  {
    "output": "def task_func(data, letter):\n    df = pd.DataFrame(data)\n    filtered_names = df[df['Name'].str.startswith(letter)]['Name']\n    return filtered_names"
  },
  {
    "output": "def task_func(T1, row_num, seed):\n    import pandas as pd\n    import numpy as np\n    np.random.seed(seed)\n    col_num = sum(int(x) for x in T1)\n    data = np.random.rand(row_num, col_num)\n    return pd.DataFrame(data)"
  },
  {
    "output": "```python\nimport pandas as pd\nfrom datetime import datetime, timedelta\nfrom random import randint, seed as random_seed\nfrom dateutil.relativedelta import relativedelta\n\ndef task_func(start_date=datetime(2020, 1, 1), end_date=datetime(2020, 12, 31), seed=42):\n    # Input validation\n    if not isinstance(start_date, datetime) or not isinstance(end_date, datetime):\n        raise ValueError(\"start_date and end_date must be datetime.datetime objects\")\n    if start_date > end_date:\n        raise ValueError(\"start_date cannot be later than end_date\")\n    \n    # Calculate the number of days in the range\n    num_days = relativedelta(end_date, start_date).days + 1\n    \n    # Set seed for reproducibility\n    random_seed(seed)\n    \n    # Generate random dates\n    random_dates = []\n    for _ in range(num_days):\n        # Generate a random number of days between 0 and num_days - 1"
  },
  {
    "output": "def task_func():\n    return \"Hello, world!\""
  },
  {
    "output": "def task_func(df):\n    if df.empty or not all(col in df.columns for col in ['A', 'B', 'C']) or not all(df[col].dtype.kind in 'fci' for col in ['A', 'B', 'C']):\n        return None\n    filtered = df[(df['B'] > 50) & (df['C'] == 900)]\n    if filtered.empty:\n        return None\n    from sklearn.model_selection import train_test_split\n    from sklearn.linear_model import LinearRegression\n    X, y = filtered[['A']], filtered['B']\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n    model = LinearRegression().fit(X_train, y_train)\n    predictions = model.predict(X_test)\n    return predictions, model"
  },
  {
    "output": "```python\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\ndef task_func(df, letters=list('ABCDEFGHIJKLMNOPQRSTUVWXYZ')):\n    # Check if the input is a DataFrame\n    if not isinstance(df, pd.DataFrame):\n        raise ValueError(\"If 'df' is not a DataFrame or lacks the 'Letters' column.\")\n    # Check if the 'Letters' column exists\n    if 'Letters' not in df.columns:\n        raise ValueError(\"If 'df' is not a DataFrame or lacks the 'Letters' column.\")\n    \n    # Count the frequency of each letter\n    letter_counts = df['Letters'].value_counts()\n    \n    # Create a bar chart\n    fig, ax = plt.subplots()\n    letter_counts.plot(kind='bar', ax=ax)\n    \n    # Set labels and title\n    ax.set_xlabel('Letters')\n    ax.set_ylabel('Frequency')\n    ax.set_title('Letter Frequency')\n    \n    #"
  },
  {
    "output": "from itertools import chain\nimport numpy as np\nfrom sklearn.cluster import KMeans\nimport matplotlib.pyplot as plt\n\ndef task_func(L):\n    data = list(chain.from_iterable(L))\n    data_array = np.array(data).reshape(-1, 1)\n    kmeans = KMeans(n_clusters=3, random_state=0).fit(data_array)\n    labels = kmeans.labels_\n    fig, ax = plt.subplots()\n    ax.scatter(data_array, np.zeros_like(data_array), c=labels, cmap='viridis', s=10)\n    return ax"
  },
  {
    "output": "import numpy as np\nimport matplotlib.pyplot as plt\n\ndef task_func():\n    x = np.linspace(0, 2*np.pi, 1000)\n    sin_x = np.sin(x)\n    cos_x = np.cos(x)\n    abs_diff = np.abs(sin_x - cos_x)\n    sequence = list(zip(x, sin_x, cos_x, abs_diff))\n    plt.plot(x, sin_x, label='sin(x)')\n    plt.plot(x, cos_x, label='cos(x)')\n    plt.legend()\n    fft_abs_diff = np.fft.fft(abs_diff)\n    mean_fft = np.mean(np.abs(fft_abs_diff))\n    median_fft = np.median(np.abs(fft_abs_diff))\n    return mean_fft, median_fft, sequence"
  },
  {
    "output": "import pandas as pd\nimport matplotlib.pyplot as plt\nfrom random import randint\n\ndef task_func(num_types=5, integer_range=(0, 100)):\n    data = {f'Type {i+1}': [randint(*integer_range) for _ in range(num_types)] for i in range(num_types)}\n    df = pd.DataFrame(data)\n    fig, ax = plt.subplots()\n    df.plot(kind='barh', stacked=True, ax=ax)\n    ax.set_xlabel('Values')\n    ax.set_ylabel('Categories')\n    ax.set_title('Horizontal Stacked Bar Chart')\n    return (fig, ax)"
  },
  {
    "output": "def remove_urls_and_count(text, n):\n    import re\n    from collections import Counter\n    text = re.sub(r'http\\S+', '', text)\n    words = re.findall(r'\\b\\w+\\b', text.lower())\n    counts = Counter(words)\n    return counts.most_common(n)"
  },
  {
    "output": "import xlwt\nimport os\n\ndef task_func(values, filename):\n    workbook = xlwt.Workbook()\n    worksheet = workbook.add_sheet('Sheet1')\n    for i, field in enumerate(['ID', 'Name', 'Age']):\n        worksheet.write(0, i, field)\n    for i, row in enumerate(values):\n        for j, key in enumerate(['ID', 'Name', 'Age']):\n            worksheet.write(i + 1, j, row.get(key, ''))\n    path = os.path.abspath(filename)\n    workbook.save(path)\n    return path"
  },
  {
    "output": "import numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\ndef task_func(num_samples=100, num_features=5):\n    np.random.seed(42)\n    data = np.random.rand(num_samples, num_features)\n    df = pd.DataFrame(data, columns=[f'Feature_{i+1}' for i in range(num_features)])\n    corr = df.corr()\n    plt.figure(figsize=(10, 8))\n    sns.heatmap(corr, annot=True, cmap='coolwarm', fmt=\".2f\")\n    plt.title('Correlation Matrix Heatmap')\n    plt.show()\n    return df, plt.gca()"
  },
  {
    "output": "def task_func(L):\n    import numpy as np\n    from collections import Counter\n    import matplotlib.pyplot as plt\n\n    mean = np.mean(L)\n    median = np.median(L)\n    counter = Counter(L)\n    mode = counter.most_common(1)[0][0]\n    std_dev = np.std(L)\n    fig, ax = plt.subplots()\n    ax.hist(L, bins=10, edgecolor='black')\n    return {\n        'mean': mean,\n        'median': median,\n        'mode': mode,\n        'std_dev': std_dev,\n        'plot': ax\n    }"
  },
  {
    "output": "def inspect_function(f):\n    import inspect\n    import types\n    result = {}\n    result['name'] = f.__name__\n    result['is_lambda'] = isinstance(f, types.LambdaType)\n    signature = inspect.signature(f)\n    result['arguments'] = []\n    for name, param in signature.parameters.items():\n        arg_info = {\n            'name': name,\n            'default': param.default if param.default is not inspect.Parameter.empty else None,\n            'annotation': param.annotation if param.annotation is not inspect.Parameter.empty else None\n        }\n        result['arguments'].append(arg_info)\n    return result"
  },
  {
    "output": "def process_log_file(log_file_path, output_csv_path):\n    import csv\n    import re\n    from datetime import datetime\n    \n    valid_entries = []\n    with open(log_file_path, 'r') as file:\n        for line in file:\n            match = re.match(r'(\\d{4}-\\d{2}-\\d{2} \\d{2}:\\d{2}:\\d{2}),(.*)', line.strip())\n            if match:\n                timestamp_str, message = match.groups()\n                try:\n                    timestamp = datetime.strptime(timestamp_str, '%Y-%m-%d %H:%M:%S')\n                    valid_entries.append((timestamp, message))\n                except ValueError:\n                    pass\n    if not valid_entries:\n        raise ValueError(\"No valid log entries found\")\n    with open(output_csv_path, 'w', newline='') as csvfile:\n        writer = csv.writer(csvfile)\n        writer.writerow(['Timestamp', 'Message'])\n        for timestamp, message in valid_entries:\n            writer.writerow([timestamp, message])"
  },
  {
    "output": "```python\nimport pandas as pd\nfrom sklearn.cluster import KMeans\nimport matplotlib.pyplot as plt\n\ndef task_func(df, n_clusters=3, random_state=0):\n    # Check if the DataFrame is empty or lacks required columns\n    if df.empty or 'date' not in df.columns or 'value' not in df.columns:\n        raise ValueError(\"DataFrame is empty or lacks required columns.\")\n    \n    # Convert 'date' to datetime\n    df['date'] = pd.to_datetime(df['date'])\n    \n    # Convert to ordinal (number of days since 0001-01-01)\n    df['date_ordinal'] = df['date'].dt.toordinal\n    \n    # Prepare the data for clustering\n    X"
  },
  {
    "output": "import mechanize\nfrom bs4 import BeautifulSoup\n\ndef task_func(url, form_id, data):\n    br = mechanize.Browser()\n    br.open(url)\n    br.select_form(nr=0)\n    br.form['form_id'] = form_id\n    br.form.set_all_readonly(False)\n    br.form['data'] = data\n    response = br.submit()\n    soup = BeautifulSoup(response.read(), 'html.parser')\n    title = soup.find('title')\n    return title.get_text() if title else 'No Title'"
  },
  {
    "output": "plt.figure(figsize=(10, 6))\nplt.plot(df['Date'], df['Sales'], marker='o', linestyle='-')\nplt.title('Daily Turnover Line Chart')\nplt.xlabel('Date')\nplt.ylabel('Sales')\nplt.grid(True)\nreturn plt.gca()"
  },
  {
    "output": "plt.plot(history.history['val_loss'])\nplt.title('Training and Validation Loss')\nplt.ylabel('Loss')\nplt.xlabel('Epoch')\nplt.legend(['Train', 'Validation'], loc='upper right')\nplt.show()"
  },
  {
    "output": "from sklearn.preprocessing import MinMaxScaler\n\ndef task_func(my_dict):\n    if 'array' not in my_dict or not isinstance(my_dict['array'], (list, tuple, np.ndarray)):\n        raise TypeError(\"The 'array' key is missing or not a valid array type.\")\n    arr = my_dict['array']\n    if isinstance(arr, (list, tuple)):\n        arr = np.array(arr)\n    scaler = MinMaxScaler()\n    normalized = scaler.fit_transform(arr.reshape(-1, 1)).flatten()\n    my_dict['normalized_array'] = normalized\n    return my_dict"
  },
  {
    "output": "import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\ndef task_func(data_str, separator=\",\", bins=20):\n    try:\n        data_list = list(map(int, data_str.split(separator)))\n        if not data_list:\n            raise ValueError(\"Data is empty.\")\n        series = pd.Series(data_list, dtype=np.int64)\n        fig, ax = plt.subplots()\n        ax.hist(series, bins=bins, color='#607c8e', rwidth=0.9, grid=True)\n        return (series, ax)\n    except ValueError as e:\n        raise ValueError(\"Failed to convert data or data is empty.\") from e"
  },
  {
    "output": "from scipy import stats\nimport matplotlib.pyplot as plt\n\ndef task_func(data_dict, data_keys):\n    x = data_dict[data_keys[0]]\n    y = data_dict[data_keys[1]]\n    correlation_coefficient = stats.pearsonr(x, y)[0]\n    plt.figure(figsize=(8, 6))\n    plt.scatter(x, y, alpha=0.6)\n    plt.xlabel(data_keys[0])\n    plt.ylabel(data_keys[1])\n    plt.title(f'Scatter Plot of {data_keys[0]} vs {data_keys[1]}')\n    return (correlation_coefficient, plt.gca())"
  },
  {
    "output": "import random\n\ndef generate_palindrome_sentence(word_pool, MIN_WORDS, MAX_WORDS):\n    length = random.randint(MIN_WORDS, MAX_WORDS)\n    if length % 2 == 0:\n        first_half = [random.choice(word_pool) for _ in range(length // 2)]\n        second_half = first_half[::-1]\n    else:\n        first_half = [random.choice(word_pool) for _ in range((length - 1) // 2)]\n        middle = [random.choice(word_pool)]\n        second_half = first_half[::-1]\n    sentence = ' '.join(first_half + middle + second_half)\n    return sentence"
  },
  {
    "output": "def task_func(my_obj):\n    return json.dumps(my_obj, cls=CustomJSONEncoder)"
  },
  {
    "output": "def scale_and_plot(data_dict):\n    import pandas as pd\n    import numpy as np\n    import matplotlib.pyplot as plt\n    from sklearn.preprocessing import MinMaxScaler\n\n    df = pd.DataFrame(data_dict)\n    scaler = MinMaxScaler()\n    scaled_data = scaler.fit_transform(df)\n    scaled_df = pd.DataFrame(scaled_data, columns=df.columns)\n    fig, ax = plt.subplots()\n    scaled_df.plot(ax=ax)\n    return scaled_df, ax"
  },
  {
    "output": "import pandas as pd\nimport random\n\ndef process_strings(strings):\n    modified = []\n    for s in strings:\n        if ',' in s:\n            parts = s.split(',')\n            random_index = random.randint(0, len(parts)-1)\n            modified_str = ','.join(parts[:random_index] + parts[random_index+1:])\n            modified.append(modified_str)\n        else:\n            modified.append(s)\n    return pd.DataFrame({'Original': strings, 'Modified': modified})"
  },
  {
    "output": "```python\nfrom sklearn.preprocessing import MinMaxScaler\nimport pandas as pd\n\ndef task_func(l):\n    scaler = MinMaxScaler()\n    scaled_array = scaler.fit_transform(l)\n    df = pd.DataFrame(scaled_array, columns=['Scaled Values'])\n    return df\n```"
  },
  {
    "output": "def scan_logs_for_ip_counts(directory):\n    import os\n    import re\n    ip_count = {}\n    ip_pattern = re.compile(r'\\b(?:[0-9]{1,3}\\.){3}[0-9]{1,3}\\b')\n    for filename in os.listdir(directory):\n        file_path = os.path.join(directory, filename)\n        if os.path.isfile(file_path):\n            with open(file_path, 'r') as file:\n                for line in file:\n                    ips = ip_pattern.findall(line)\n                    for ip in ips:\n                        ip_count[ip] = ip_count.get(ip, 0) + 1\n    return ip_count"
  },
  {
    "output": "def archive_processed_files(source_dir, target_dir, archive_name):\n    import os\n    import shutil\n    processed_files = [f for f in os.listdir(source_dir) if f.endswith('_processed')]\n    if not processed_files:\n        return\n    archive_path = os.path.join(target_dir, archive_name)\n    with shutil.make_archive(archive_path, 'zip', source_dir, processed_files) as _:\n        pass\n    for file in processed_files:\n        os.remove(os.path.join(source_dir, file))"
  },
  {
    "output": "import numpy as np\nfrom itertools import chain\n\ndef task_func(L):\n    flat_list = list(chain.from_iterable(L))\n    arr = np.array(flat_list)\n    mean = np.mean(arr)\n    variance = np.var(arr)\n    return {'mean': mean, 'variance': variance}"
  },
  {
    "output": "def analyze_weather_data(data_column):\n    import matplotlib.pyplot as plt\n    import numpy as np\n    import pandas as pd\n    \n    if not data_column or not isinstance(data_column, pd.Series):\n        return {\n            \"mean\": None,\n            \"median\": None,\n            \"std_dev\": None,\n            \"min\": None,\n            \"max\": None,\n            \"plot\": None\n        }\n    \n    data = data_column.dropna()\n    if data.empty:\n        return {\n            \"mean\": None,\n            \"median\": None,\n            \"std_dev\": None,\n            \"min\": None,\n            \"max\": None,\n            \"plot\": None\n        }\n    \n    mean = np.mean(data)\n    median = np.median(data)\n    std_dev = np.std(data)\n    min_val = data.min()\n    max_val = data.max()\n    \n    plt.figure(figsize=(10, 6))\n    plt.hist(data, bins=20, color='skyblue', edgecolor='black')\n    plt.title('Distribution of Weather Data')\n    plt.xlabel('Value')\n    plt.ylabel('Frequency')\n    plot = plt.gcf()\n    \n    plt.close()\n    \n    return {\n        \"mean\": mean,\n        \"median\": median,\n        \"std_dev\": std_dev,\n        \"min\": min_val,\n        \"max\": max_val,\n        \"plot\": plot\n    }"
  },
  {
    "output": "import pandas as pd\nfrom nltk.corpus import stopwords\nfrom nltk.tokenize import sent_tokenize, word_tokenize\nimport string\n\ndef task_func(text):\n    sentences = sent_tokenize(text)\n    stop_words = set(stopwords.words('english'))\n    word_counts = {}\n    for sentence in sentences:\n        words = word_tokenize(sentence)\n        words = [word.lower() for word in words if word.isalpha()]\n        for word in words:\n            if word not in stop_words:\n                word_counts[word] = word_counts.get(word, 0) + 1\n    return pd.Series(word_counts)"
  },
  {
    "output": "import os\nimport requests\nimport gzip\nimport tempfile\n\ndef fetch_and_compress_json(url):\n    response = requests.get(url)\n    data = response.json()\n    with tempfile.NamedTemporaryFile(mode='w', delete=False, suffix='.json.gz') as tmpfile:\n        with gzip.open(tmpfile.name, 'wb') as gzip_file:\n            gzip_file.write(data.encode('utf-8'))\n        return tmpfile.name"
  },
  {
    "output": "def task_func(filepath, destination_dir):\n    import os\n    import shutil\n    import ctypes\n    dll_name = os.path.basename(filepath)\n    dll_path = os.path.abspath(filepath)\n    dll_dir = os.path.dirname(dll_path)\n    dll_files = [f for f in os.listdir(dll_dir) if f.endswith('.dll')]\n    for dll_file in dll_files:\n        src = os.path.join(dll_dir, dll_file)\n        dst = os.path.join(destination_dir, dll_file)\n        shutil.move(src, dst)\n    dll_handle = ctypes.windll.LoadLibrary(dll_path)\n    return dll_name"
  },
  {
    "output": "def task_func(source_file, backup_dir):\n    import os\n    import shutil\n    import subprocess\n    if not os.path.exists(source_file):\n        return -1\n    try:\n        backup_path = os.path.join(backup_dir, os.path.basename(source_file))\n        shutil.copy2(source_file, backup_path)\n        result = subprocess.run([source_file], capture_output=True, text=True)\n        return result.returncode\n    except Exception:\n        return -1"
  },
  {
    "output": "import pandas as pd\nfrom sklearn.preprocessing import StandardScaler\n\ndef task_func(df, age, weight):\n    required_columns = ['Age', 'Weight']\n    if not all(col in df.columns for col in required_columns):\n        return pd.DataFrame()\n    filtered_df = df[(df['Age'] == age) & (df['Weight'] == weight)]\n    if filtered_df.empty:\n        return pd.DataFrame()\n    scaler = StandardScaler()\n    scaled_values = scaler.fit_transform(filtered_df)\n    return pd.DataFrame(scaled_values, columns=filtered_df.columns)"
  },
  {
    "output": "ax.set_yticks([0, 1])\nplt.setp(ax.get_xticklabels(), rotation=45, ha=\"right\")\nplt.setp(ax.get_yticklabels(), rotation=45, ha=\"right\")\nax.set_xticklabels(['', ''], fontsize=12)\nax.set_yticklabels(['', ''], fontsize=12)\nax.set_title(\"Confusion matrix\", fontsize=12)\nfig.tight_layout()\nreturn cm, ax"
  },
  {
    "output": "def task_func():\n    return \"Hello, World!\""
  },
  {
    "output": "import ipaddress\nimport requests\nfrom requests.exceptions import ConnectionError, Timeout\n\ndef task_func(ip_range, timeout):\n    try:\n        network = ipaddress.ip_network(ip_range, strict=False)\n    except ValueError:\n        raise ValueError(\"Invalid IP range\")\n    \n    active_servers = []\n    \n    for ip in network.hosts():\n        ip_str = str(ip)\n        try:\n            response = requests.get(f\"http://{ip_str}\", timeout=timeout)\n            if response.status_code == 200:\n                active_servers.append(ip_str)\n        except (ConnectionError, Timeout):\n            continue\n    \n    return active_servers"
  },
  {
    "output": "def task_func(hex_keys=KEYS):\n    hex_str = random.choice(hex_keys)\n    ascii_bytes = bytes.fromhex(hex_str)\n    ascii_str = ascii_bytes.decode('ascii')\n    try:\n        float_num = float(ascii_str)\n    except ValueError:\n        float_num = random.uniform(0.0, 100.0)\n    float_str = str(float_num)\n    utf8_bytes = float_str.encode('utf-8')\n    return utf8_bytes"
  },
  {
    "output": "import pandas as pd\nfrom collections import Counter\n\ndef task_func(data):\n    required_keys = {'Name', 'Age', 'Scores'}\n    if not required_keys.issubset(data.keys()):\n        raise ValueError(\"The dictionary does not have the required keys.\")\n    df = pd.DataFrame(data)\n    df_sorted = df.sort_values(by=['Name', 'Age'])\n    avg_scores = df.groupby('Name')['Scores'].mean()\n    most_common_age = df_sorted['Age'].mode()[0] if not df_sorted.empty else None\n    return df_sorted, avg_scores, most_common_age"
  },
  {
    "output": "color='purple', linestyle='--', label=f'Std Dev: {std_dev:.2f}')\n    ax.legend()\n    \n    # Save the plot as a PNG file\n    plt.savefig('random_histogram.png')\n    plt.close()"
  },
  {
    "output": "def task_func(directory, zip_filename):\n    import os\n    import zipfile\n    from Crypto.PublicKey import RSA\n    from Crypto.Cipher import PKCS1_OAEP\n    from Crypto.Random import get_random_bytes\n    import base64\n    \n    key = RSA.generate(2048)\n    public_key = key.publickey()\n    \n    encrypted_files = []\n    \n    for filename in os.listdir(directory):\n        file_path = os.path.join(directory, filename)\n        if os.path.isfile(file_path):\n            with open(file_path, 'rb') as f:\n                data = f.read()\n            cipher = PKCS1_OAEP.new(public_key)\n            encrypted_data = cipher.encrypt(data)\n            encrypted_filename = f\"encrypted_{filename}\"\n            with open(encrypted_filename, 'wb') as f:\n                f.write(encrypted_data)\n            encrypted_files.append(encrypted_filename)\n    \n    with zipfile.ZipFile(zip_filename, 'w') as zipf:\n        for file in encrypted_files:\n            zipf.write(file)\n    \n    os.makedirs('encrypted_files', exist_ok=True)\n    for file in encrypted_files:\n        os.rename(file, os.path.join('encrypted_files', file))\n    \n    return public_key, zip_filename"
  },
  {
    "output": "def merge_and_encode(lists):\n    import numpy as np\n    from sklearn.preprocessing import OneHotEncoder\n    merged = [item for sublist in lists for item in sublist]\n    encoder = OneHotEncoder(sparse_output=False)\n    encoded = encoder.fit_transform(np.array(merged).reshape(-1, 1))\n    return encoded.flatten().tolist()"
  },
  {
    "output": "import pandas as pd\nimport requests\nfrom io import StringIO\n\ndef task_func(csv_url, sort_by_column=\"title\"):\n    response = requests.get(csv_url)\n    if response.status_code != 200:\n        raise Exception(\"Failed to fetch data\")\n    df = pd.read_csv(StringIO(response.text))\n    df_sorted = df.sort_values(by=sort_by_column)\n    return df_sorted"
  },
  {
    "output": "def make_https_get_request(server, path):\n    import socket\n    import ssl\n    import urllib.parse\n    import http.client\n\n    url = urllib.parse.urlunsplit(['https', server, path, '', ''])\n    try:\n        conn = http.client.HTTPSConnection(server)\n        conn.request(\"GET\", path)\n        response = conn.getresponse()\n        return response.read().decode('utf-8')\n    except (socket.error, ssl.SSLError, http.client.HTTPException) as e:\n        return str(e)"
  },
  {
    "output": "def task_func(latin_names, other_names, start_date, end_date, email_domain):\n    import pandas as pd\n    import numpy as np\n    np.random.seed(42)\n    ids = np.random.randint(1, 101, size=100)\n    names = np.random.choice(latin_names + other_names, size=100)\n    names = [name.replace('ç', 'c').replace('ñ', 'n').replace('ö', 'o') for name in names]\n    dob = pd.date_range(start=start_date, end=end_date, freq='D').tolist()\n    dob = np.random.choice(dob, size=100)\n    years = dob.apply(lambda x: x.year).values\n    emails = np.array([f\"{name}.{year}@{email_domain}\" for name, year in zip(names, years)])\n    df = pd.DataFrame({\n        'ID': ids,\n        'Name': names,\n        'Date of Birth': dob,\n        'Email': emails\n    })\n    return df"
  },
  {
    "output": ", alpha=0.6)\n    plt.title('KMeans Clustering Result')\n    plt.xlabel('Index')\n    plt.ylabel('Value')\n    plt.colorbar()\n    return flattened_data, plt.gca()"
  },
  {
    "output": "```python\nfrom random import randint, seed\nimport matplotlib.pyplot as plt\nimport pandas as pd\n\n# Constants (they can be overridden with default parameters)\nTEAMS = ['Team A', 'Team B', 'Team C', 'Team D', 'Team E']\nPENALTY_COST = 1000  # in dollars\n\ndef task_func(goals, penalties, teams=TEAMS, penalty_cost=PENALTY_COST, rng_seed=None):\n    # Set random seed for reproducibility\n    if rng_seed is not None:\n        seed(rng_seed)\n    \n    # Generate random data for teams\n    if teams is None:\n        teams = TEAMS\n    if goals is None:\n        goals = [randint(0, 10) for _ in range(len(teams))]\n    if penalties is None:\n        penalties = [randint(0, 5) for _ in range(len(teams))]"
  },
  {
    "output": "def task_func(df):\n    if len(df) < 2:\n        return float('nan')\n    scores = df['scores'].astype(float)\n    grades = df['grades'].str.replace('A', '4').str.replace('B', '3').str.replace('C', '2').str.replace('D', '1').astype(float)\n    return (scores - scores.mean()) * (grades - grades.mean()).sum() / ( (scores - scores.mean()).pow(2).sum() * (grades - grades.mean()).pow(2).sum() )**0.5"
  },
  {
    "output": "import json\nimport random\nimport os\n\ndef task_func(file_path, seed):\n    random.seed(seed)\n    activity_data = [\n        {\"user\": \"user1\", \"activity\": \"login\", \"timestamp\": random.randint(1000, 9999)},\n        {\"user\": \"user2\", \"activity\": \"logout\", \"timestamp\": random.randint(1000, 9999)},\n        {\"user\": \"user3\", \"activity\": \"click\", \"timestamp\": random.randint(1000, 9999)}\n    ]\n    with open(file_path, 'w') as f:\n        json.dump(activity_data, f)\n    return file_path"
  },
  {
    "output": "def task_func(df):\n    if not isinstance(df, pd.DataFrame) or 'Name' not in df.columns:\n        return \"Invalid input\"\n    unique_names = df['Name'].unique()\n    scores = df[df['Name'].isin(unique_names)]['Score']\n    countries = df[df['Name'].isin(unique_names)]['Country']\n    plt.figure(figsize=(12, 6))\n    plt.hist(scores, bins=10, color='blue', alpha=0.7)\n    plt.title('Histogram of Scores')\n    plt.xlabel('Score')\n    plt.ylabel('Frequency')\n    plt.show()\n    plt.figure(figsize=(12, 6))\n    countries = countries.dropna()\n    scores = scores.loc[countries.index]\n    plt.boxplot([scores[countries == country] for country in countries.unique()], labels=countries.unique())\n    plt.title('Boxplot of Scores by Country')\n    plt.xlabel('Country')\n    plt.ylabel('Score')\n    plt.show()"
  },
  {
    "output": "def task_func(data, columns, target_column):\n    X = data[columns]\n    y = data[target_column]\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n    model = LogisticRegression()\n    model.fit(X_train, y_train)\n    y_pred = model.predict(X_test)\n    accuracy = accuracy_score(y_test, y_pred)\n    return accuracy"
  },
  {
    "output": "True"
  },
  {
    "output": "def task_func(days_in_past=7):\n    if days_in_past < 0:\n        raise ValueError(\"days_in_past cannot be negative\")\n    today_utc = datetime.now(pytz.utc)\n    past_date = today_utc - timedelta(days=days_in_past)\n    weekday_name = calendar.day_name[past_date.weekday()]\n    return weekday_name"
  },
  {
    "output": "import pandas as pd\nfrom sklearn.decomposition import PCA\nimport matplotlib.pyplot as plt\n\ndef task_func(df):\n    if not isinstance(df, pd.DataFrame):\n        raise ValueError(\"Input is not a DataFrame\")\n    if df.empty:\n        raise ValueError(\"DataFrame is empty\")\n    \n    pca = PCA(n_components=2)\n    principal_components = pca.fit_transform(df)\n    \n    pc_df = pd.DataFrame(data=principal_components, columns=['Principal Component 1', 'Principal Component 2'])\n    \n    plt.figure(figsize=(8, 6))\n    plt.scatter(pc_df['Principal Component 1'], pc_df['Principal Component 2'])\n    plt.title('2 Component PCA')\n    plt.xlabel('Principal Component 1')\n    plt.ylabel('Principal Component 2')\n    axes = plt.gca()\n    \n    return pc_df, axes"
  },
  {
    "output": "import sys\nimport sqlite3\n\ndef task_func(path):\n    sys.path.append(path)\n    conn = sqlite3.connect('path.db')\n    c = conn.cursor()\n    c.execute('''CREATE TABLE IF NOT EXISTS paths (path TEXT PRIMARY KEY)''')\n    c.execute(\"SELECT * FROM paths WHERE path = ?\", (path,))\n    if not c.fetchone():\n        c.execute(\"INSERT INTO paths (path) VALUES (?)\", (path,))\n        conn.commit()\n    conn.close()\n    return path"
  },
  {
    "output": "import random\nimport math\n\ndef generate_random_float_from_hex(hex_list):\n    hex_str = random.choice(hex_list)\n    float_val = float.fromhex(hex_str)\n    return round(float_val, 2)"
  },
  {
    "output": "import os\nimport zipfile\n\ndef zip_files_with_extensions(source_dir, target_dir, zip_name, valid_extensions):\n    if not os.path.exists(source_dir):\n        raise OSError(f\"Source directory '{source_dir}' does not exist.\")\n    files = [f for f in os.listdir(source_dir) if os.path.isfile(os.path.join(source_dir, f)) and os.path.splitext(f)[1].lower() in valid_extensions]\n    zip_path = os.path.join(target_dir, zip_name)\n    with zipfile.ZipFile(zip_path, 'w') as zipf:\n        for file in files:\n            file_path = os.path.join(source_dir, file)\n            zipf.write(file_path, os.path.basename(file_path))\n    return zip_path"
  },
  {
    "output": "def generate_random_dataframe(rows, columns):\n    import pandas as pd\n    import numpy as np\n    data = {}\n    for col in columns:\n        if np.random.rand() > 0.5:\n            data[col] = np.random.randint(0, 100, size=rows)\n        else:\n            data[col] = np.random.choice(['low', 'medium', 'high'], size=rows)\n    return pd.DataFrame(data)"
  },
  {
    "output": "import pandas as pd\nfrom sklearn.linear_model import LinearRegression\n\ndef task_func(d, target='y'):\n    X = [[item['x'], item['z']] for item in d]\n    y = [item[target] for item in d]\n    model = LinearRegression()\n    model.fit(X, y)\n    return model"
  },
  {
    "output": "import numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy.stats import gaussian_kde\n\ndef generate_and_plot_gaussian_kde(mean, std_dev, sample_size):\n    np.random.seed(42)\n    data = np.random.normal(mean, std_dev, sample_size)\n    kde = gaussian_kde(data)\n    x = np.linspace(data.min(), data.max(), 1000)\n    y = kde(x)\n    fig, ax = plt.subplots()\n    ax.plot(x, y)\n    return ax"
  },
  {
    "output": "def task_func(text):\n    import nltk\n    from nltk.corpus import stopwords\n    import numpy as np\n    import pandas as pd\n    import matplotlib.pyplot as plt\n    nltk.download('stopwords')\n    stop_words = set(stopwords.words('english'))\n    words = text.split()\n    words = [word for word in words if word not in stop_words]\n    unique_words = []\n    prev = None\n    for word in words:\n        if word != prev:\n            unique_words.append(word)\n        prev = word\n    word_to_index = {word: i for i, word in enumerate(unique_words)}\n    n = len(unique_words)\n    co_occurrence = np.zeros((n, n), dtype=int)\n    for i in range(len(unique_words) - 1):\n        current = unique_words[i]\n        next_word = unique_words[i + 1]\n        co_occurrence[word_to_index[current], word_to_index[next_word]] += 1\n    df = pd.DataFrame(co_occurrence, index=unique_words, columns=unique_words)\n    plt.figure(figsize=(10, 10))\n    plt.imshow(df, cmap='viridis', interpolation='nearest')\n    plt.colorbar()\n    plt.title('Co-occurrence Matrix')\n    plt.xlabel('Words')\n    plt.ylabel('Words')\n    plot_obj = plt.gcf()\n    return (df, plot_obj)"
  },
  {
    "output": "feast object e from object d"
  },
  {
    "output": "def next_action(): return 'pick up the red block'"
  },
  {
    "output": "Please provide the complete initial conditions to proceed."
  },
  {
    "output": "False"
  },
  {
    "output": "Use hoist1 to lift crate0 from pallet1 at depot1."
  },
  {
    "output": "feast object d from object"
  },
  {
    "output": ""
  },
  {
    "output": "The first step is valid."
  },
  {
    "output": "False"
  },
  {
    "output": "Feast object b from object a"
  },
  {
    "output": "def next_step():\n    return \"Pick up the blue block\""
  },
  {
    "output": "load package_0 into airplane_0 at location_2_0"
  },
  {
    "output": ""
  },
  {
    "output": "pick up the white"
  },
  {
    "output": "def next_step(): return 'unstack the orange block from the table'"
  },
  {
    "output": "unstack the red block from on top of the yellow block"
  },
  {
    "output": "unstack the yellow block from on top of the red block"
  },
  {
    "output": ""
  },
  {
    "output": "def next_action():\n    return \"Pick up the orange block\""
  },
  {
    "output": "Use hoist1 to lift crate2 from pallet2 at depot1."
  },
  {
    "output": "Drive truck0 from depot1 to depot2"
  },
  {
    "output": ""
  },
  {
    "output": "def simulate_plan(initial_conditions, plan, rules):\n    state = initial_conditions.copy()\n    for action in plan:\n        if action in rules:\n            state = rules[action](state)\n        else:\n            raise ValueError(f\"Unknown action: {action}\")\n    return state == desired_outcome(state)"
  },
  {
    "output": ""
  },
  {
    "output": ""
  },
  {
    "output": "def next_action(): return 'pick up the blue block'"
  },
  {
    "output": ""
  },
  {
    "output": "unstack the orange block from on top of the red block"
  },
  {
    "output": ""
  },
  {
    "output": ""
  },
  {
    "output": ""
  },
  {
    "output": "Feast object d from object a."
  },
  {
    "output": ""
  },
  {
    "output": ""
  },
  {
    "output": "\"Use hoist1 to lift crate0 from pallet1 at depot1\""
  },
  {
    "output": ""
  },
  {
    "output": "def execute_action():\n    return \"Feast object c from object b\""
  },
  {
    "output": ""
  },
  {
    "output": "def feast_object_a_from_object_c(): return 'Feast object a from object c'"
  },
  {
    "output": "Use hoist3 to lift crate0 from pallet3 at distributor0."
  },
  {
    "output": ""
  },
  {
    "output": ""
  },
  {
    "output": "I will evaluate the plan step-by-step to determine if it can achieve the desired outcome."
  },
  {
    "output": ""
  },
  {
    "output": "unstack the yellow block from on top of the blue block"
  },
  {
    "output": "Insufficient information to proceed."
  },
  {
    "output": ""
  },
  {
    "output": "Use hoist1 to lift crate1 from pallet1 at depot1."
  },
  {
    "output": "Please provide the complete plan to evaluate its feasibility."
  },
  {
    "output": ""
  },
  {
    "output": "The current state is not fully described, and the next action cannot be determined without further information."
  },
  {
    "output": "Attack object"
  },
  {
    "output": "pick up the blue block"
  },
  {
    "output": ""
  },
  {
    "output": ""
  },
  {
    "output": ""
  },
  {
    "output": "feast object a from object d"
  },
  {
    "output": "True"
  },
  {
    "output": "pick up the yellow block"
  },
  {
    "output": "def feast_action(): return 'Feast action performed'"
  },
  {
    "output": ""
  },
  {
    "output": ""
  },
  {
    "output": ""
  },
  {
    "output": "False"
  },
  {
    "output": "\"Pick up the red block.\""
  },
  {
    "output": "def next_action(): return 'Feast object b from object a'"
  },
  {
    "output": "The plan is not provided, so I cannot determine the next action."
  },
  {
    "output": "Adjust the plan to include the movement of package_0 using truck_1 and ensure that the movement of package_1 is correctly executed using airplane_1 and truck_2."
  },
  {
    "output": "unstack the red block from on top of the orange block"
  },
  {
    "output": ""
  },
  {
    "output": "20"
  },
  {
    "output": ""
  },
  {
    "output": "Please provide the complete list of initial conditions."
  },
  {
    "output": "True"
  },
  {
    "output": "pick up the red block"
  },
  {
    "output": ""
  },
  {
    "output": "unstack the orange block from on top of the blue block"
  },
  {
    "output": ""
  },
  {
    "output": ""
  },
  {
    "output": "Please provide the complete current state of the system to proceed with evaluating the plan."
  },
  {
    "output": ""
  },
  {
    "output": "def solve():\n    return \"plan\""
  },
  {
    "output": "def next_action():\n    return \"Pick up the red block and stack it under the yellow block.\""
  },
  {
    "output": ""
  },
  {
    "output": ""
  },
  {
    "output": ""
  },
  {
    "output": "Please provide the plan or the next action you want to take."
  },
  {
    "output": "feast object b from object d"
  },
  {
    "output": "The plan cannot be evaluated for validity without the initial conditions, rules, and the actual plan."
  },
  {
    "output": "Please provide the complete plan for evaluation."
  },
  {
    "output": "Feast object a from object"
  },
  {
    "output": ""
  },
  {
    "output": "Insufficient information to determine next step."
  },
  {
    "output": ""
  },
  {
    "output": ""
  },
  {
    "output": "unstack the blue block from on top of the orange block"
  },
  {
    "output": ""
  },
  {
    "output": "def simulate_actions(state):\n    if not state:\n        return False\n    actions = [\n        ('move', 'A', 'B'),\n        ('rotate', 'B', 'C'),\n        ('check', 'C', 'D'),\n        ('activate', 'D', 'E')\n    ]\n    for action in actions:\n        if action[0] == 'move':\n            if state[action[1]] != 'free':\n                return False\n            state[action[2]] = state[action[1]]\n            state[action[1]] = 'free'\n        elif action[0] == 'rotate':\n            if state[action[1]] != 'rotatable':\n                return False\n            state[action[2]] = state[action[1]]\n            state[action[1]] = 'free'\n        elif action[0] == 'check':\n            if state[action[1]] != 'checked':\n                return False\n            state[action[2]] = state[action[1]]\n            state[action[1]] = 'free'\n        elif action[0] == 'activate':\n            if state[action[1]] != 'active':\n                return False\n            state[action[2]] = state[action[1]]\n            state[action[1]] = 'free'\n    return True"
  },
  {
    "output": ""
  },
  {
    "output": ""
  },
  {
    "output": ""
  },
  {
    "output": "Attack object b"
  },
  {
    "output": "Step 1: Evaluate the initial conditions and the plan to determine the next action."
  },
  {
    "output": ""
  }
]